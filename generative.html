<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Generative Models | Basics of Statistical Learning</title>
  <meta name="description" content="Chapter 10 Generative Models | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.19.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Generative Models | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://statisticallearning.org/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Generative Models | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="binary-classification.html"/>
<link rel="next" href="supervised-i.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i><b>0.1</b> Who?</a><ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#readers"><i class="fa fa-check"></i><b>0.1.1</b> Readers</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#author"><i class="fa fa-check"></i><b>0.1.2</b> Author</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.1.3</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i><b>0.2</b> What?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i><b>0.3</b> Why?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#where"><i class="fa fa-check"></i><b>0.4</b> Where?</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#when"><i class="fa fa-check"></i><b>0.5</b> When?</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#how"><i class="fa fa-check"></i><b>0.6</b> How?</a><ul>
<li class="chapter" data-level="0.6.1" data-path="index.html"><a href="index.html#build-tools"><i class="fa fa-check"></i><b>0.6.1</b> Build Tools</a></li>
<li class="chapter" data-level="0.6.2" data-path="index.html"><a href="index.html#active-development"><i class="fa fa-check"></i><b>0.6.2</b> Active Development</a></li>
<li class="chapter" data-level="0.6.3" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>0.6.3</b> License</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#reading"><i class="fa fa-check"></i><b>1.1</b> Reading</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.2</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-tasks"><i class="fa fa-check"></i><b>1.3</b> Machine Learning Tasks</a><ul>
<li class="chapter" data-level="1.3.1" data-path="ml-overview.html"><a href="ml-overview.html#supervised-learning"><i class="fa fa-check"></i><b>1.3.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="1.3.2" data-path="ml-overview.html"><a href="ml-overview.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.3.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ml-overview.html"><a href="ml-overview.html#open-questions"><i class="fa fa-check"></i><b>1.4</b> Open Questions</a></li>
<li class="chapter" data-level="1.5" data-path="ml-overview.html"><a href="ml-overview.html#source"><i class="fa fa-check"></i><b>1.5</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#reading-1"><i class="fa fa-check"></i><b>2.1</b> Reading</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>2.2</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#setup"><i class="fa fa-check"></i><b>2.3</b> Setup</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#mathematical-setup"><i class="fa fa-check"></i><b>2.4</b> Mathematical Setup</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>2.5</b> Linear Regression Models</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#using-lm"><i class="fa fa-check"></i><b>2.6</b> Using <code>lm()</code></a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#the-predict-function"><i class="fa fa-check"></i><b>2.7</b> The <code>predict()</code> Function</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#data-splitting"><i class="fa fa-check"></i><b>2.8</b> Data Splitting</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#regression-metrics"><i class="fa fa-check"></i><b>2.9</b> Regression Metrics</a><ul>
<li class="chapter" data-level="2.9.1" data-path="linear-regression.html"><a href="linear-regression.html#graphical-evaluation"><i class="fa fa-check"></i><b>2.9.1</b> Graphical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#example-simple-simulated-data"><i class="fa fa-check"></i><b>2.10</b> Example: “Simple” Simulated Data</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#example-diamonds-data"><i class="fa fa-check"></i><b>2.11</b> Example: Diamonds Data</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#example-credit-card-data"><i class="fa fa-check"></i><b>2.12</b> Example: Credit Card Data</a></li>
<li class="chapter" data-level="2.13" data-path="linear-regression.html"><a href="linear-regression.html#source-1"><i class="fa fa-check"></i><b>2.13</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>3</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#reading-2"><i class="fa fa-check"></i><b>3.1</b> Reading</a></li>
<li class="chapter" data-level="3.2" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#mathematical-setup-1"><i class="fa fa-check"></i><b>3.2</b> Mathematical Setup</a></li>
<li class="chapter" data-level="3.3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="3.4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#decision-trees"><i class="fa fa-check"></i><b>3.4</b> Decision Trees</a></li>
<li class="chapter" data-level="3.5" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#example-credit-card-data-1"><i class="fa fa-check"></i><b>3.5</b> Example: Credit Card Data</a></li>
<li class="chapter" data-level="3.6" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#source-2"><i class="fa fa-check"></i><b>3.6</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bvt.html"><a href="bvt.html"><i class="fa fa-check"></i><b>4</b> Bias–Variance Tradeoff</a><ul>
<li class="chapter" data-level="4.1" data-path="bvt.html"><a href="bvt.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>4.1</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="4.2" data-path="bvt.html"><a href="bvt.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>4.2</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="4.3" data-path="bvt.html"><a href="bvt.html#simulation"><i class="fa fa-check"></i><b>4.3</b> Simulation</a></li>
<li class="chapter" data-level="4.4" data-path="bvt.html"><a href="bvt.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>4.4</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="4.5" data-path="bvt.html"><a href="bvt.html#source-3"><i class="fa fa-check"></i><b>4.5</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-overview.html"><a href="regression-overview.html"><i class="fa fa-check"></i><b>5</b> Regression Overview</a><ul>
<li class="chapter" data-level="5.1" data-path="regression-overview.html"><a href="regression-overview.html#goal"><i class="fa fa-check"></i><b>5.1</b> Goal</a></li>
<li class="chapter" data-level="5.2" data-path="regression-overview.html"><a href="regression-overview.html#strategy"><i class="fa fa-check"></i><b>5.2</b> Strategy</a></li>
<li class="chapter" data-level="5.3" data-path="regression-overview.html"><a href="regression-overview.html#models"><i class="fa fa-check"></i><b>5.3</b> Models</a></li>
<li class="chapter" data-level="5.4" data-path="regression-overview.html"><a href="regression-overview.html#model-flexibility"><i class="fa fa-check"></i><b>5.4</b> Model Flexibility</a></li>
<li class="chapter" data-level="5.5" data-path="regression-overview.html"><a href="regression-overview.html#overfitting"><i class="fa fa-check"></i><b>5.5</b> Overfitting</a></li>
<li class="chapter" data-level="5.6" data-path="regression-overview.html"><a href="regression-overview.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.6</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="5.7" data-path="regression-overview.html"><a href="regression-overview.html#source-4"><i class="fa fa-check"></i><b>5.7</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#reading-3"><i class="fa fa-check"></i><b>6.1</b> Reading</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#classification-metrics"><i class="fa fa-check"></i><b>6.2</b> Classification Metrics</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#source-5"><i class="fa fa-check"></i><b>6.3</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html"><i class="fa fa-check"></i><b>7</b> Nonparametric Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#reading-4"><i class="fa fa-check"></i><b>7.1</b> Reading</a></li>
<li class="chapter" data-level="7.2" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#source-6"><i class="fa fa-check"></i><b>7.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#reading-5"><i class="fa fa-check"></i><b>8.1</b> Reading</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#source-7"><i class="fa fa-check"></i><b>8.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="binary-classification.html"><a href="binary-classification.html"><i class="fa fa-check"></i><b>9</b> Binary Classification</a><ul>
<li class="chapter" data-level="9.1" data-path="binary-classification.html"><a href="binary-classification.html#reading-6"><i class="fa fa-check"></i><b>9.1</b> Reading</a></li>
<li class="chapter" data-level="9.2" data-path="binary-classification.html"><a href="binary-classification.html#source-8"><i class="fa fa-check"></i><b>9.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generative.html"><a href="generative.html"><i class="fa fa-check"></i><b>10</b> Generative Models</a><ul>
<li class="chapter" data-level="10.1" data-path="generative.html"><a href="generative.html#reading-7"><i class="fa fa-check"></i><b>10.1</b> Reading</a></li>
<li class="chapter" data-level="10.2" data-path="generative.html"><a href="generative.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="10.3" data-path="generative.html"><a href="generative.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>10.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="10.4" data-path="generative.html"><a href="generative.html#naive-bayes"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.5" data-path="generative.html"><a href="generative.html#discrete-inputs"><i class="fa fa-check"></i><b>10.5</b> Discrete Inputs</a></li>
<li class="chapter" data-level="10.6" data-path="generative.html"><a href="generative.html#source-9"><i class="fa fa-check"></i><b>10.6</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="supervised-i.html"><a href="supervised-i.html"><i class="fa fa-check"></i><b>11</b> Supervised Learning Overview I</a><ul>
<li class="chapter" data-level="11.1" data-path="supervised-i.html"><a href="supervised-i.html#reading-8"><i class="fa fa-check"></i><b>11.1</b> Reading</a></li>
<li class="chapter" data-level="11.2" data-path="supervised-i.html"><a href="supervised-i.html#source-10"><i class="fa fa-check"></i><b>11.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="simulation.html"><a href="simulation.html"><i class="fa fa-check"></i><b>12</b> Simulation</a><ul>
<li class="chapter" data-level="12.1" data-path="simulation.html"><a href="simulation.html#reading-9"><i class="fa fa-check"></i><b>12.1</b> Reading</a></li>
<li class="chapter" data-level="12.2" data-path="simulation.html"><a href="simulation.html#source-11"><i class="fa fa-check"></i><b>12.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>13</b> Bootstrap Resampling</a><ul>
<li class="chapter" data-level="13.1" data-path="bootstrap.html"><a href="bootstrap.html#reading-10"><i class="fa fa-check"></i><b>13.1</b> Reading</a></li>
<li class="chapter" data-level="13.2" data-path="bootstrap.html"><a href="bootstrap.html#source-12"><i class="fa fa-check"></i><b>13.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>14</b> Cross-Validation</a><ul>
<li class="chapter" data-level="14.1" data-path="cross-validation.html"><a href="cross-validation.html#reading-11"><i class="fa fa-check"></i><b>14.1</b> Reading</a></li>
<li class="chapter" data-level="14.2" data-path="cross-validation.html"><a href="cross-validation.html#source-13"><i class="fa fa-check"></i><b>14.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="supervised-ii.html"><a href="supervised-ii.html"><i class="fa fa-check"></i><b>15</b> Supervised Learning Overview II</a><ul>
<li class="chapter" data-level="15.1" data-path="supervised-ii.html"><a href="supervised-ii.html#classification-2"><i class="fa fa-check"></i><b>15.1</b> Classification</a><ul>
<li class="chapter" data-level="15.1.1" data-path="supervised-ii.html"><a href="supervised-ii.html#tuning"><i class="fa fa-check"></i><b>15.1.1</b> Tuning</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="supervised-ii.html"><a href="supervised-ii.html#regression-1"><i class="fa fa-check"></i><b>15.2</b> Regression</a><ul>
<li class="chapter" data-level="15.2.1" data-path="supervised-ii.html"><a href="supervised-ii.html#methods"><i class="fa fa-check"></i><b>15.2.1</b> Methods</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="supervised-ii.html"><a href="supervised-ii.html#external-links"><i class="fa fa-check"></i><b>15.3</b> External Links</a></li>
<li class="chapter" data-level="15.4" data-path="supervised-ii.html"><a href="supervised-ii.html#rmarkdown"><i class="fa fa-check"></i><b>15.4</b> <code>rmarkdown</code></a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>16</b> Regularization</a><ul>
<li class="chapter" data-level="16.1" data-path="regularization.html"><a href="regularization.html#reading-12"><i class="fa fa-check"></i><b>16.1</b> Reading</a></li>
<li class="chapter" data-level="16.2" data-path="regularization.html"><a href="regularization.html#source-14"><i class="fa fa-check"></i><b>16.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="dimension-reduction.html"><a href="dimension-reduction.html"><i class="fa fa-check"></i><b>17</b> Dimension Reduction</a><ul>
<li class="chapter" data-level="17.1" data-path="dimension-reduction.html"><a href="dimension-reduction.html#reading-13"><i class="fa fa-check"></i><b>17.1</b> Reading</a></li>
<li class="chapter" data-level="17.2" data-path="dimension-reduction.html"><a href="dimension-reduction.html#source-15"><i class="fa fa-check"></i><b>17.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>18</b> Ensemble Methods</a><ul>
<li class="chapter" data-level="18.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading-14"><i class="fa fa-check"></i><b>18.1</b> Reading</a></li>
<li class="chapter" data-level="18.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#source-16"><i class="fa fa-check"></i><b>18.2</b> Source</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="additional-reading.html"><a href="additional-reading.html"><i class="fa fa-check"></i><b>A</b> Additional Reading</a><ul>
<li class="chapter" data-level="A.1" data-path="additional-reading.html"><a href="additional-reading.html#books"><i class="fa fa-check"></i><b>A.1</b> Books</a></li>
<li class="chapter" data-level="A.2" data-path="additional-reading.html"><a href="additional-reading.html#papers"><i class="fa fa-check"></i><b>A.2</b> Papers</a></li>
<li class="chapter" data-level="A.3" data-path="additional-reading.html"><a href="additional-reading.html#blog-posts"><i class="fa fa-check"></i><b>A.3</b> Blog Posts</a></li>
<li class="chapter" data-level="A.4" data-path="additional-reading.html"><a href="additional-reading.html#miscellaneous"><i class="fa fa-check"></i><b>A.4</b> Miscellaneous</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="ten-rules.html"><a href="ten-rules.html"><i class="fa fa-check"></i><b>B</b> Ten Simple Rules for Success in STAT 432</a><ul>
<li class="chapter" data-level="B.1" data-path="ten-rules.html"><a href="ten-rules.html#rule-1-there-are-no-rules"><i class="fa fa-check"></i><b>B.1</b> Rule 1: There Are No Rules</a></li>
<li class="chapter" data-level="B.2" data-path="ten-rules.html"><a href="ten-rules.html#rule-2-read-the-syllabus"><i class="fa fa-check"></i><b>B.2</b> Rule 2: Read the Syllabus</a></li>
<li class="chapter" data-level="B.3" data-path="ten-rules.html"><a href="ten-rules.html#rule-3-previous-learning-is-not-gospel"><i class="fa fa-check"></i><b>B.3</b> Rule 3: Previous Learning is Not Gospel</a></li>
<li class="chapter" data-level="B.4" data-path="ten-rules.html"><a href="ten-rules.html#rule-4-all-statements-are-true"><i class="fa fa-check"></i><b>B.4</b> Rule 4: All Statements Are True</a></li>
<li class="chapter" data-level="B.5" data-path="ten-rules.html"><a href="ten-rules.html#rule-5-dont-miss-the-forest-for-the-trees"><i class="fa fa-check"></i><b>B.5</b> Rule 5: Don’t Miss The Forest For The Trees</a></li>
<li class="chapter" data-level="B.6" data-path="ten-rules.html"><a href="ten-rules.html#rule-6-you-will-struggle"><i class="fa fa-check"></i><b>B.6</b> Rule 6: You Will Struggle</a></li>
<li class="chapter" data-level="B.7" data-path="ten-rules.html"><a href="ten-rules.html#rule-7-keep-it-simple"><i class="fa fa-check"></i><b>B.7</b> Rule 7: Keep It Simple</a></li>
<li class="chapter" data-level="B.8" data-path="ten-rules.html"><a href="ten-rules.html#rule-8-rtfm"><i class="fa fa-check"></i><b>B.8</b> Rule 8: RTFM</a></li>
<li class="chapter" data-level="B.9" data-path="ten-rules.html"><a href="ten-rules.html#rule-9-there-are-no-stupid-questions"><i class="fa fa-check"></i><b>B.9</b> Rule 9: There Are No Stupid Questions</a></li>
<li class="chapter" data-level="B.10" data-path="ten-rules.html"><a href="ten-rules.html#rule-10-learn-by-doing"><i class="fa fa-check"></i><b>B.10</b> Rule 10: Learn By Doing</a></li>
<li class="chapter" data-level="B.11" data-path="ten-rules.html"><a href="ten-rules.html#conclusion"><i class="fa fa-check"></i><b>B.11</b> Conclusion</a></li>
<li class="chapter" data-level="B.12" data-path="ten-rules.html"><a href="ten-rules.html#source-17"><i class="fa fa-check"></i><b>B.12</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="computing.html"><a href="computing.html"><i class="fa fa-check"></i><b>C</b> Computing</a><ul>
<li class="chapter" data-level="C.1" data-path="computing.html"><a href="computing.html#reading-15"><i class="fa fa-check"></i><b>C.1</b> Reading</a></li>
<li class="chapter" data-level="C.2" data-path="computing.html"><a href="computing.html#additional-resources"><i class="fa fa-check"></i><b>C.2</b> Additional Resources</a><ul>
<li class="chapter" data-level="C.2.1" data-path="computing.html"><a href="computing.html#r"><i class="fa fa-check"></i><b>C.2.1</b> R</a></li>
<li class="chapter" data-level="C.2.2" data-path="computing.html"><a href="computing.html#rstudio"><i class="fa fa-check"></i><b>C.2.2</b> RStudio</a></li>
<li class="chapter" data-level="C.2.3" data-path="computing.html"><a href="computing.html#r-markdown"><i class="fa fa-check"></i><b>C.2.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="computing.html"><a href="computing.html#stat-432-idioms"><i class="fa fa-check"></i><b>C.3</b> STAT 432 Idioms</a><ul>
<li class="chapter" data-level="C.3.1" data-path="computing.html"><a href="computing.html#dont-restore-old-workspaces"><i class="fa fa-check"></i><b>C.3.1</b> Don’t Restore Old Workspaces</a></li>
<li class="chapter" data-level="C.3.2" data-path="computing.html"><a href="computing.html#r-versions"><i class="fa fa-check"></i><b>C.3.2</b> R Versions</a></li>
<li class="chapter" data-level="C.3.3" data-path="computing.html"><a href="computing.html#code-style"><i class="fa fa-check"></i><b>C.3.3</b> Code Style</a></li>
<li class="chapter" data-level="C.3.4" data-path="computing.html"><a href="computing.html#reference-style"><i class="fa fa-check"></i><b>C.3.4</b> Reference Style</a></li>
<li class="chapter" data-level="C.3.5" data-path="computing.html"><a href="computing.html#stat-432-r-style-overrides"><i class="fa fa-check"></i><b>C.3.5</b> STAT 432 R Style Overrides</a></li>
<li class="chapter" data-level="C.3.6" data-path="computing.html"><a href="computing.html#stat-432-r-markdown-style"><i class="fa fa-check"></i><b>C.3.6</b> STAT 432 R Markdown Style</a></li>
<li class="chapter" data-level="C.3.7" data-path="computing.html"><a href="computing.html#style-heuristics"><i class="fa fa-check"></i><b>C.3.7</b> Style Heuristics</a></li>
<li class="chapter" data-level="C.3.8" data-path="computing.html"><a href="computing.html#objects-and-functions"><i class="fa fa-check"></i><b>C.3.8</b> Objects and Functions</a></li>
<li class="chapter" data-level="C.3.9" data-path="computing.html"><a href="computing.html#print-versus-return"><i class="fa fa-check"></i><b>C.3.9</b> Print versus Return</a></li>
<li class="chapter" data-level="C.3.10" data-path="computing.html"><a href="computing.html#help"><i class="fa fa-check"></i><b>C.3.10</b> Help</a></li>
<li class="chapter" data-level="C.3.11" data-path="computing.html"><a href="computing.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>C.3.11</b> Keyboard Shortcuts</a></li>
<li class="chapter" data-level="C.3.12" data-path="computing.html"><a href="computing.html#common-issues"><i class="fa fa-check"></i><b>C.3.12</b> Common Issues</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="computing.html"><a href="computing.html#source-18"><i class="fa fa-check"></i><b>C.4</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>D</b> Probability</a><ul>
<li class="chapter" data-level="D.1" data-path="probability.html"><a href="probability.html#reading-16"><i class="fa fa-check"></i><b>D.1</b> Reading</a></li>
<li class="chapter" data-level="D.2" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>D.2</b> Probability Models</a></li>
<li class="chapter" data-level="D.3" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>D.3</b> Probability Axioms</a></li>
<li class="chapter" data-level="D.4" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>D.4</b> Probability Rules</a></li>
<li class="chapter" data-level="D.5" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>D.5</b> Random Variables</a><ul>
<li class="chapter" data-level="D.5.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>D.5.1</b> Distributions</a></li>
<li class="chapter" data-level="D.5.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>D.5.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="D.5.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>D.5.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="D.5.4" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>D.5.4</b> Distributions in R</a></li>
<li class="chapter" data-level="D.5.5" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>D.5.5</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="D.6" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>D.6</b> Expectations</a></li>
<li class="chapter" data-level="D.7" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>D.7</b> Likelihood</a></li>
<li class="chapter" data-level="D.8" data-path="probability.html"><a href="probability.html#references"><i class="fa fa-check"></i><b>D.8</b> References</a><ul>
<li class="chapter" data-level="D.8.1" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>D.8.1</b> Videos</a></li>
</ul></li>
<li class="chapter" data-level="D.9" data-path="probability.html"><a href="probability.html#source-19"><i class="fa fa-check"></i><b>D.9</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>E</b> Statistics</a><ul>
<li class="chapter" data-level="E.1" data-path="statistics.html"><a href="statistics.html#reading-17"><i class="fa fa-check"></i><b>E.1</b> Reading</a></li>
<li class="chapter" data-level="E.2" data-path="statistics.html"><a href="statistics.html#statistics-1"><i class="fa fa-check"></i><b>E.2</b> Statistics</a></li>
<li class="chapter" data-level="E.3" data-path="statistics.html"><a href="statistics.html#estimators"><i class="fa fa-check"></i><b>E.3</b> Estimators</a><ul>
<li class="chapter" data-level="E.3.1" data-path="statistics.html"><a href="statistics.html#properties"><i class="fa fa-check"></i><b>E.3.1</b> Properties</a></li>
<li class="chapter" data-level="E.3.2" data-path="statistics.html"><a href="statistics.html#example-mse-of-an-estimator"><i class="fa fa-check"></i><b>E.3.2</b> Example: MSE of an Estimator</a></li>
<li class="chapter" data-level="E.3.3" data-path="statistics.html"><a href="statistics.html#estimation-methods"><i class="fa fa-check"></i><b>E.3.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="E.3.4" data-path="statistics.html"><a href="statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>E.3.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="E.3.5" data-path="statistics.html"><a href="statistics.html#method-of-moments"><i class="fa fa-check"></i><b>E.3.5</b> Method of Moments</a></li>
<li class="chapter" data-level="E.3.6" data-path="statistics.html"><a href="statistics.html#empirical-distribution-function"><i class="fa fa-check"></i><b>E.3.6</b> Empirical Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="statistics.html"><a href="statistics.html#source-20"><i class="fa fa-check"></i><b>E.4</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://daviddalpiaz.org" target="blank">&copy; 2020 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generative-models" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Generative Models</h1>
<hr />
<p>In this chapter, we continue our discussion of classification methods. We introduce three new methods, each a <strong>generative</strong> method. This in comparison to logistic regression, which is a <strong>discriminative</strong> method.</p>
<p>Generative methods model the joint probability, <span class="math inline">\(p(\boldsymbol{x}, y)\)</span>, often by assuming some distribution for the conditional distribution of <span class="math inline">\(\boldsymbol{X}\)</span> given <span class="math inline">\(Y\)</span>, <span class="math inline">\(f(\boldsymbol{x} \mid y)\)</span>. Bayes theorem is then applied to classify according to <span class="math inline">\(p(y \mid \boldsymbol{x})\)</span>. Discriminative methods such as logistic regression directly model this conditional, <span class="math inline">\(p(y \mid \boldsymbol{x})\)</span>. A detailed discussion and analysis can be found in <a href="https://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf">Ng and Jordan, 2002</a>.</p>
<hr />
<div id="reading-7" class="section level2">
<h2><span class="header-section-number">10.1</span> Reading</h2>
<ul>
<li><strong>Required:</strong> <a href="https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf">ISL Chapter 4, Sections 4</a></li>
</ul>
<hr />
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;MASS&quot;</span>)</a>
<a class="sourceLine" id="cb168-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)</a>
<a class="sourceLine" id="cb168-3" data-line-number="3"><span class="kw">library</span>(<span class="st">&quot;knitr&quot;</span>)</a>
<a class="sourceLine" id="cb168-4" data-line-number="4"><span class="kw">library</span>(<span class="st">&quot;kableExtra&quot;</span>)</a>
<a class="sourceLine" id="cb168-5" data-line-number="5"><span class="kw">library</span>(<span class="st">&quot;klaR&quot;</span>)</a></code></pre></div>
<hr />
<p>Each of the methods in this chapter will use Bayes theorem to build a classifier.</p>
<p><span class="math display">\[
p_k(\boldsymbol{x}) = P(Y = k \mid \boldsymbol{X} = \boldsymbol{x}) = \frac{\pi_k \cdot f_k(\boldsymbol{x})}{\sum_{g = 1}^{G} \pi_g \cdot f_g(\boldsymbol{x})}
\]</span></p>
<p>We call <span class="math inline">\(p_k(\boldsymbol{x})\)</span> the <strong>posterior</strong> probability, which we will estimate then use to create classifications. The <span class="math inline">\(\pi_g\)</span> are called the <strong>prior</strong> probabilities for each possible class <span class="math inline">\(g\)</span>. That is, <span class="math inline">\(\pi_g = P(Y = g)\)</span>, unconditioned on <span class="math inline">\(\boldsymbol X\)</span>. (Here, there are <span class="math inline">\(G\)</span> possible classes, denoted <span class="math inline">\(1, 2, \ldots G\)</span>. We use <span class="math inline">\(k\)</span> to refer to a particular class.) The <span class="math inline">\(f_g(x)\)</span> are called the <strong>likelihoods</strong>, which are indexed by <span class="math inline">\(g\)</span> to denote that they are conditional on the classes. The denominator is often referred to as a <strong>normalizing constant</strong>.</p>
<p>The methods will differ by placing different modeling assumptions on the likelihoods, <span class="math inline">\(f_g(\boldsymbol x)\)</span>. For each method, the priors could be learned from data or pre-specified.</p>
<p>For each method, classifications are made to the class with the highest estimated posterior probability, which is equivalent to the class with the largest</p>
<p><span class="math display">\[
\log(\hat{\pi}_k \cdot \hat{f}_k(\boldsymbol{x})).
\]</span></p>
<p>By substituting the corresponding likelihoods, simplifying, and eliminating unnecessary terms, we could derive the discriminant function for each.</p>
<p>To illustrate these new methods, we return to the iris data, which you may remember has three classes. After a test-train split, we create a number of plots to refresh our memory.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">9</span>)</a>
<a class="sourceLine" id="cb169-2" data-line-number="2">iris_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(iris), <span class="dt">size =</span> <span class="kw">trunc</span>(<span class="fl">0.50</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(iris)))</a>
<a class="sourceLine" id="cb169-3" data-line-number="3"><span class="co"># iris_idx = sample(nrow(iris), size = trunc(0.10 * nrow(iris)))</span></a>
<a class="sourceLine" id="cb169-4" data-line-number="4">iris_trn =<span class="st"> </span>iris[iris_idx, ]</a>
<a class="sourceLine" id="cb169-5" data-line-number="5">iris_tst =<span class="st"> </span>iris[<span class="op">-</span>iris_idx, ]</a></code></pre></div>
<p>Note that we have only performed a test-train split, so we are not validating any of these models. Also note that using 50% of the data for training is an arbitrary choice here.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1">caret<span class="op">::</span><span class="kw">featurePlot</span>(<span class="dt">x =</span> iris_trn[, <span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, </a>
<a class="sourceLine" id="cb170-2" data-line-number="2">                                    <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)], </a>
<a class="sourceLine" id="cb170-3" data-line-number="3">                   <span class="dt">y =</span> iris_trn<span class="op">$</span>Species,</a>
<a class="sourceLine" id="cb170-4" data-line-number="4">                   <span class="dt">plot =</span> <span class="st">&quot;density&quot;</span>, </a>
<a class="sourceLine" id="cb170-5" data-line-number="5">                   <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">relation =</span> <span class="st">&quot;free&quot;</span>), </a>
<a class="sourceLine" id="cb170-6" data-line-number="6">                                 <span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">relation =</span> <span class="st">&quot;free&quot;</span>)), </a>
<a class="sourceLine" id="cb170-7" data-line-number="7">                   <span class="dt">adjust =</span> <span class="fl">1.5</span>, </a>
<a class="sourceLine" id="cb170-8" data-line-number="8">                   <span class="dt">pch =</span> <span class="st">&quot;|&quot;</span>, </a>
<a class="sourceLine" id="cb170-9" data-line-number="9">                   <span class="dt">layout =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), </a>
<a class="sourceLine" id="cb170-10" data-line-number="10">                   <span class="dt">auto.key =</span> <span class="kw">list</span>(<span class="dt">columns =</span> <span class="dv">3</span>))</a></code></pre></div>
<p><img src="generative_files/figure-html/unnamed-chunk-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" data-line-number="1">caret<span class="op">::</span><span class="kw">featurePlot</span>(<span class="dt">x =</span> iris_trn[, <span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, </a>
<a class="sourceLine" id="cb171-2" data-line-number="2">                                    <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)], </a>
<a class="sourceLine" id="cb171-3" data-line-number="3">                   <span class="dt">y =</span> iris_trn<span class="op">$</span>Species,</a>
<a class="sourceLine" id="cb171-4" data-line-number="4">                   <span class="dt">plot =</span> <span class="st">&quot;ellipse&quot;</span>,</a>
<a class="sourceLine" id="cb171-5" data-line-number="5">                   <span class="dt">auto.key =</span> <span class="kw">list</span>(<span class="dt">columns =</span> <span class="dv">3</span>))</a></code></pre></div>
<p><img src="generative_files/figure-html/unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1">caret<span class="op">::</span><span class="kw">featurePlot</span>(<span class="dt">x =</span> iris_trn[, <span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, </a>
<a class="sourceLine" id="cb172-2" data-line-number="2">                                    <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)], </a>
<a class="sourceLine" id="cb172-3" data-line-number="3">                   <span class="dt">y =</span> iris_trn<span class="op">$</span>Species,</a>
<a class="sourceLine" id="cb172-4" data-line-number="4">                   <span class="dt">plot =</span> <span class="st">&quot;box&quot;</span>,</a>
<a class="sourceLine" id="cb172-5" data-line-number="5">                   <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">relation =</span> <span class="st">&quot;free&quot;</span>),</a>
<a class="sourceLine" id="cb172-6" data-line-number="6">                                 <span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">rot =</span> <span class="dv">90</span>)),</a>
<a class="sourceLine" id="cb172-7" data-line-number="7">                   <span class="dt">layout =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">1</span>))</a></code></pre></div>
<p><img src="generative_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Especially based on the pairs plot, we see that it should not be too difficult to find a good classifier.</p>
<p>Notice that we use <code>caret::featurePlot</code> to access the <code>featurePlot()</code> function without loading the entire <code>caret</code> package.</p>
<hr />
</div>
<div id="linear-discriminant-analysis" class="section level2">
<h2><span class="header-section-number">10.2</span> Linear Discriminant Analysis</h2>
<p>Linear Discriminant Analysis, <strong>LDA</strong>, assumes that the features are multivariate normal conditioned on the classes.</p>
<p><span class="math display">\[
\boldsymbol{X} \mid Y = k \sim N(\boldsymbol{\mu}_k, \boldsymbol\Sigma)
\]</span></p>
<p><span class="math display">\[
f_k(\boldsymbol{x}) = \frac{1}{(2\pi)^{p/2}|\boldsymbol\Sigma|^{1/2}}\exp\left[-\frac{1}{2}(\boldsymbol x - \boldsymbol\mu_k)^{\prime}\boldsymbol\Sigma^{-1}(\boldsymbol x - \boldsymbol\mu_k)\right]
\]</span></p>
<p>Notice that <span class="math inline">\(\boldsymbol\Sigma\)</span> does <strong>not</strong> depend on <span class="math inline">\(k\)</span>, that is, we are assuming the same <span class="math inline">\(\Sigma\)</span> for each class. We then use information from all the classes to estimate <span class="math inline">\(\boldsymbol\Sigma\)</span>.</p>
<p>To fit an LDA model, we use the <code>lda()</code> function from the <code>MASS</code> package.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" data-line-number="1">iris_lda =<span class="st"> </span><span class="kw">lda</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris_trn)</a>
<a class="sourceLine" id="cb173-2" data-line-number="2">iris_lda</a></code></pre></div>
<pre><code>## Call:
## lda(Species ~ ., data = iris_trn)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##  0.4000000  0.2933333  0.3066667 
## 
## Group means:
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa         5.033333    3.490000     1.413333   0.2466667
## versicolor     6.068182    2.850000     4.372727   1.3636364
## virginica      6.534783    2.986957     5.452174   2.0217391
## 
## Coefficients of linear discriminants:
##                     LD1        LD2
## Sepal.Length  0.2941746 -0.4021398
## Sepal.Width   1.5879728  1.6962530
## Petal.Length -2.7473811 -0.9385391
## Petal.Width  -2.3223845  3.1408028
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9959 0.0041</code></pre>
<p>Here we see the estimated <span class="math inline">\(\hat{\pi}_k\)</span> and <span class="math inline">\(\hat{\boldsymbol\mu}_k\)</span> for each class.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" data-line-number="1"><span class="kw">is.list</span>(<span class="kw">predict</span>(iris_lda, iris_trn))</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" data-line-number="1"><span class="kw">names</span>(<span class="kw">predict</span>(iris_lda, iris_trn))</a></code></pre></div>
<pre><code>## [1] &quot;class&quot;     &quot;posterior&quot; &quot;x&quot;</code></pre>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">predict</span>(iris_lda, iris_trn)<span class="op">$</span>class, <span class="dt">n =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] versicolor setosa     versicolor versicolor setosa     virginica 
##  [7] setosa     setosa     setosa     setosa    
## Levels: setosa versicolor virginica</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">predict</span>(iris_lda, iris_trn)<span class="op">$</span>posterior, <span class="dt">n =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>##           setosa   versicolor    virginica
## 53  1.061809e-34 9.884510e-01 1.154896e-02
## 6   1.000000e+00 9.796157e-28 7.784092e-51
## 59  1.685242e-30 9.998375e-01 1.624818e-04
## 83  1.971739e-22 9.999997e-01 3.057713e-07
## 3   1.000000e+00 1.437420e-28 1.155778e-52
## 140 1.106749e-51 1.917744e-04 9.998082e-01
## 48  1.000000e+00 4.855435e-27 1.296275e-50
## 30  1.000000e+00 2.216075e-24 3.964649e-47
## 44  1.000000e+00 4.020843e-23 1.979000e-44
## 37  1.000000e+00 3.629736e-32 1.883675e-57</code></pre>
<p>As we should come to expect, the <code>predict()</code> function operates in a new way when called on an <code>lda</code> object. By default, it returns an entire list. Within that list <code>class</code> stores the classifications and <code>posterior</code> contains the estimated probability for each class.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1">iris_lda_trn_pred =<span class="st"> </span><span class="kw">predict</span>(iris_lda, iris_trn)<span class="op">$</span>class</a>
<a class="sourceLine" id="cb183-2" data-line-number="2">iris_lda_tst_pred =<span class="st"> </span><span class="kw">predict</span>(iris_lda, iris_tst)<span class="op">$</span>class</a></code></pre></div>
<p>We store the predictions made on the train and test sets.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1">calc_misclass =<span class="st"> </span><span class="cf">function</span>(actual, predicted) {</a>
<a class="sourceLine" id="cb184-2" data-line-number="2">  <span class="kw">mean</span>(actual <span class="op">!=</span><span class="st"> </span>predicted)</a>
<a class="sourceLine" id="cb184-3" data-line-number="3">}</a></code></pre></div>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_lda_trn_pred, <span class="dt">actual =</span> iris_trn<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_lda_tst_pred, <span class="dt">actual =</span> iris_tst<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0.02666667</code></pre>
<p>As expected, LDA performs well on both the train and test data. (Note that the “training error” here is calculated on the training data since there is no validation data. This is not how we have usually defined training error.)</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1"><span class="kw">table</span>(<span class="dt">predicted =</span> iris_lda_tst_pred, <span class="dt">actual =</span> iris_tst<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>##             actual
## predicted    setosa versicolor virginica
##   setosa         20          0         0
##   versicolor      0         26         0
##   virginica       0          2        27</code></pre>
<p>Looking at the test set, we see that we are perfectly predicting both setosa and virginica. The only error is labeling a couple versicolors as a virginica.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1">iris_lda_flat =<span class="st"> </span><span class="kw">lda</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris_trn, <span class="dt">prior =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb191-2" data-line-number="2">iris_lda_flat</a></code></pre></div>
<pre><code>## Call:
## lda(Species ~ ., data = iris_trn, prior = c(1, 1, 1)/3)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##  0.3333333  0.3333333  0.3333333 
## 
## Group means:
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa         5.033333    3.490000     1.413333   0.2466667
## versicolor     6.068182    2.850000     4.372727   1.3636364
## virginica      6.534783    2.986957     5.452174   2.0217391
## 
## Coefficients of linear discriminants:
##                     LD1        LD2
## Sepal.Length  0.2950319 -0.4015112
## Sepal.Width   1.5843501  1.6996372
## Petal.Length -2.7453724 -0.9443988
## Petal.Width  -2.3290804  3.1358407
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9951 0.0049</code></pre>
<p>Instead of learning (estimating) the proportion of the three species from the data, we could instead specify them ourselves. Here we choose a uniform distributions over the possible species. We would call this a “flat” prior.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1">iris_lda_flat_trn_pred =<span class="st"> </span><span class="kw">predict</span>(iris_lda_flat, iris_trn)<span class="op">$</span>class</a>
<a class="sourceLine" id="cb193-2" data-line-number="2">iris_lda_flat_tst_pred =<span class="st"> </span><span class="kw">predict</span>(iris_lda_flat, iris_tst)<span class="op">$</span>class</a></code></pre></div>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_lda_flat_trn_pred, <span class="dt">actual =</span> iris_trn<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0.01333333</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_lda_flat_tst_pred, <span class="dt">actual =</span> iris_tst<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0.01333333</code></pre>
<p>This actually gives a better test accuracy! In practice, this could be useful if you have prior knowledge about the future proportions of the response variable.</p>
<hr />
</div>
<div id="quadratic-discriminant-analysis" class="section level2">
<h2><span class="header-section-number">10.3</span> Quadratic Discriminant Analysis</h2>
<p>Quadratic Discriminant Analysis, <strong>QDA</strong>, also assumes that the features are multivariate normal conditioned on the classes.</p>
<p><span class="math display">\[
\boldsymbol X \mid Y = k \sim N(\boldsymbol\mu_k, \boldsymbol\Sigma_k)
\]</span></p>
<p><span class="math display">\[
f_k(\boldsymbol x) = \frac{1}{(2\pi)^{p/2}|\boldsymbol\Sigma_k|^{1/2}}\exp\left[-\frac{1}{2}(\boldsymbol x - \boldsymbol\mu_k)^{\prime}\boldsymbol\Sigma_{k}^{-1}(\boldsymbol x - \boldsymbol\mu_k)\right]
\]</span></p>
<p>Notice that now <span class="math inline">\(\boldsymbol\Sigma_k\)</span> <strong>does</strong> depend on <span class="math inline">\(k\)</span>, that is, we are allowing a different <span class="math inline">\(\boldsymbol\Sigma_k\)</span> for each class. We only use information from class <span class="math inline">\(k\)</span> to estimate <span class="math inline">\(\Sigma_k\)</span>.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb198-1" data-line-number="1">iris_qda =<span class="st"> </span><span class="kw">qda</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris_trn)</a>
<a class="sourceLine" id="cb198-2" data-line-number="2">iris_qda</a></code></pre></div>
<pre><code>## Call:
## qda(Species ~ ., data = iris_trn)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##  0.4000000  0.2933333  0.3066667 
## 
## Group means:
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa         5.033333    3.490000     1.413333   0.2466667
## versicolor     6.068182    2.850000     4.372727   1.3636364
## virginica      6.534783    2.986957     5.452174   2.0217391</code></pre>
<p>Here the output is similar to LDA, again giving the estimated <span class="math inline">\(\hat{\pi}_k\)</span> and <span class="math inline">\(\hat{\boldsymbol\mu}_k\)</span> for each class. Like <code>lda()</code>, the <code>qda()</code> function is found in the <code>MASS</code> package.</p>
<p>Consider trying to fit QDA again, but this time with a smaller training set. (Use the commented line above to obtain a smaller test set.) This will cause an error because there are not enough observations within each class to estimate the large number of parameters in the <span class="math inline">\(\boldsymbol\Sigma_k\)</span> matrices. This is less of a problem with LDA, since all observations, no matter the class, are being use to estimate the shared <span class="math inline">\(\boldsymbol\Sigma\)</span> matrix.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" data-line-number="1">iris_qda_trn_pred =<span class="st"> </span><span class="kw">predict</span>(iris_qda, iris_trn)<span class="op">$</span>class</a>
<a class="sourceLine" id="cb200-2" data-line-number="2">iris_qda_tst_pred =<span class="st"> </span><span class="kw">predict</span>(iris_qda, iris_tst)<span class="op">$</span>class</a></code></pre></div>
<p>The <code>predict()</code> function operates the same as the <code>predict()</code> function for LDA.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb201-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_qda_trn_pred, <span class="dt">actual =</span> iris_trn<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0.01333333</code></pre>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb203-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_qda_tst_pred, <span class="dt">actual =</span> iris_tst<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0.02666667</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb205-1" data-line-number="1"><span class="kw">table</span>(<span class="dt">predicted =</span> iris_qda_tst_pred, <span class="dt">actual =</span> iris_tst<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>##             actual
## predicted    setosa versicolor virginica
##   setosa         20          0         0
##   versicolor      0         26         0
##   virginica       0          2        27</code></pre>
<p>Here we see that QDA has similar performance to LDA. This is somewhat surprising, as we can see from the plots above that the covariance matrix for each class is likely not the same. In this case, we would sort of assume that LDA is too restrictive a model as it assumes the same covariance within each class. (It is likely due to the high seperation of the data that this doesn’t make much of a difference. Also, random chance as we’re not dealing with a lot of data.)</p>
<p>Since QDA is a more complex model than LDA (it has many more parameters), QDA is more likely to overfit than LDA. (Although, that does not seem to have happened here.)</p>
<p>Also note that, QDA creates quadratic decision boundaries, while LDA creates linear decision boundaries. We could also add quadratic terms to LDA to allow it to create quadratic decision boundaries.</p>
<hr />
</div>
<div id="naive-bayes" class="section level2">
<h2><span class="header-section-number">10.4</span> Naive Bayes</h2>
<p>Naive Bayes comes in many forms. With only numeric features, it often assumes a multivariate normal conditioned on the classes, but a very specific multivariate normal.</p>
<p><span class="math display">\[
{\boldsymbol X} \mid Y = k \sim N(\boldsymbol\mu_k, \boldsymbol\Sigma_k)
\]</span></p>
<p>Naive Bayes assumes that the features <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span> are independent. This is the “naive” part of naive Bayes. The Bayes part is nothing new. Since <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span> are assumed independent, each <span class="math inline">\(\boldsymbol\Sigma_k\)</span> is diagonal, that is, we assume no correlation between features. Independence implies zero correlation.</p>
<p>This will allow us to write the (joint) likelihood as a product of univariate distributions. In this case, the product of univariate normal distributions instead of a (joint) multivariate distribution.</p>
<p><span class="math display">\[
f_k(\boldsymbol x) = \prod_{j = 1}^{p} f_{kj}(\boldsymbol x_j)
\]</span></p>
<p>Here, <span class="math inline">\(f_{kj}(\boldsymbol x_j)\)</span> is the density for the <span class="math inline">\(j\)</span>-th feature conditioned on the <span class="math inline">\(k\)</span>-th class. Notice that there is a <span class="math inline">\(\sigma_{kj}\)</span> for each feature for each class.</p>
<p><span class="math display">\[
f_{kj}(\boldsymbol x_j) = \frac{1}{\sigma_{kj}\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{x_j - \mu_{kj}}{\sigma_{kj}}\right)^2\right]
\]</span></p>
<p>When <span class="math inline">\(p = 1\)</span>, this version of naive Bayes is equivalent to QDA.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb207-1" data-line-number="1">iris_nb =<span class="st"> </span><span class="kw">NaiveBayes</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris_trn)</a></code></pre></div>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb208-1" data-line-number="1">iris_nb<span class="op">$</span>apriori</a></code></pre></div>
<pre><code>## grouping
##     setosa versicolor  virginica 
##  0.4000000  0.2933333  0.3066667</code></pre>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb210-1" data-line-number="1">iris_nb<span class="op">$</span>tables</a></code></pre></div>
<pre><code>## $Sepal.Length
##                [,1]      [,2]
## setosa     5.033333 0.4113337
## versicolor 6.068182 0.5454455
## virginica  6.534783 0.4744458
## 
## $Sepal.Width
##                [,1]      [,2]
## setosa     3.490000 0.4293941
## versicolor 2.850000 0.3661251
## virginica  2.986957 0.2881041
## 
## $Petal.Length
##                [,1]      [,2]
## setosa     1.413333 0.1696514
## versicolor 4.372727 0.4014045
## virginica  5.452174 0.4176444
## 
## $Petal.Width
##                 [,1]      [,2]
## setosa     0.2466667 0.1074255
## versicolor 1.3636364 0.1890967
## virginica  2.0217391 0.3302293</code></pre>
<p>Many packages implement naive Bayes. Here we choose to use <code>NaiveBayes()</code> from the package <code>klaR</code>.</p>
<p>The output from <code>iris_nb$tables</code> gives the mean and standard deviation of the normal distribution for each feature in each class. Notice how these mean estimates match those for LDA and QDA above.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">predict</span>(iris_nb, iris_trn)<span class="op">$</span>class)</a></code></pre></div>
<pre><code>##         53          6         59         83          3        140 
## versicolor     setosa versicolor versicolor     setosa  virginica 
## Levels: setosa versicolor virginica</code></pre>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">predict</span>(iris_nb, iris_trn)<span class="op">$</span>posterior)</a></code></pre></div>
<pre><code>##            setosa   versicolor    virginica
## 53  5.055141e-125 5.374900e-01 4.625100e-01
## 6    1.000000e+00 1.766595e-17 2.305313e-26
## 59  1.128364e-100 9.819623e-01 1.803765e-02
## 83   1.309820e-64 9.999750e-01 2.495426e-05
## 3    1.000000e+00 1.148004e-23 1.144944e-32
## 140 7.356900e-189 8.546334e-06 9.999915e-01</code></pre>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" data-line-number="1">iris_nb_trn_pred =<span class="st"> </span><span class="kw">predict</span>(iris_nb, iris_trn)<span class="op">$</span>class</a>
<a class="sourceLine" id="cb216-2" data-line-number="2">iris_nb_tst_pred =<span class="st"> </span><span class="kw">predict</span>(iris_nb, iris_tst)<span class="op">$</span>class</a></code></pre></div>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_nb_trn_pred, <span class="dt">actual =</span> iris_trn<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0.04</code></pre>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" data-line-number="1"><span class="kw">calc_misclass</span>(<span class="dt">predicted =</span> iris_nb_tst_pred, <span class="dt">actual =</span> iris_tst<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## [1] 0.04</code></pre>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" data-line-number="1"><span class="kw">table</span>(<span class="dt">predicted =</span> iris_nb_tst_pred, <span class="dt">actual =</span> iris_tst<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>##             actual
## predicted    setosa versicolor virginica
##   setosa         20          0         0
##   versicolor      0         26         1
##   virginica       0          2        26</code></pre>
<p>Like LDA, naive Bayes is having trouble with virginica.</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Method
</th>
<th style="text-align:right;">
Train Error
</th>
<th style="text-align:right;">
Test Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
LDA
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0266667
</td>
</tr>
<tr>
<td style="text-align:left;">
LDA, Flat Prior
</td>
<td style="text-align:right;">
0.0133333
</td>
<td style="text-align:right;">
0.0133333
</td>
</tr>
<tr>
<td style="text-align:left;">
QDA
</td>
<td style="text-align:right;">
0.0133333
</td>
<td style="text-align:right;">
0.0266667
</td>
</tr>
<tr>
<td style="text-align:left;">
Naive Bayes
</td>
<td style="text-align:right;">
0.0400000
</td>
<td style="text-align:right;">
0.0400000
</td>
</tr>
</tbody>
</table>
<p>Summarizing the results, we see that Naive Bayes is the worst of LDA, QDA, and NB for this data. So why should we care about naive Bayes?</p>
<p>The strength of Naive Bayes comes from its ability to handle a large number of features, <span class="math inline">\(p\)</span>, even with a limited sample size <span class="math inline">\(n\)</span>. Even with the naive independence assumption, Naive Bayes works rather well in practice. Also because of this assumption, we can often train naive Bayes where LDA and QDA may be impossible to train because of the large number of parameters relative to the number of observations.</p>
<p>Here naive Bayes doesn’t get a chance to show its strength since LDA and QDA already perform well, and the number of features is low. The choice between LDA and QDA is mostly down to a consideration about the amount of complexity needed. (Also note that complexity within these models can also be altered by changing the features used. More features generally means a more flexible model.)</p>
<hr />
</div>
<div id="discrete-inputs" class="section level2">
<h2><span class="header-section-number">10.5</span> Discrete Inputs</h2>
<p>So far, we have assumed that all features are numeric. What happens with categorical features?</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" data-line-number="1"><span class="co"># create &quot;new&quot; dataset</span></a>
<a class="sourceLine" id="cb223-2" data-line-number="2">iris_trn_mod =<span class="st"> </span>iris_trn</a>
<a class="sourceLine" id="cb223-3" data-line-number="3"></a>
<a class="sourceLine" id="cb223-4" data-line-number="4"><span class="co"># made Sepal.Width categorical</span></a>
<a class="sourceLine" id="cb223-5" data-line-number="5">iris_trn_mod<span class="op">$</span>Sepal.Width =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(iris_trn<span class="op">$</span>Sepal.Width <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span>,</a>
<a class="sourceLine" id="cb223-6" data-line-number="6">                                  <span class="kw">ifelse</span>(iris_trn<span class="op">$</span>Sepal.Width <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span>,</a>
<a class="sourceLine" id="cb223-7" data-line-number="7">                                         <span class="st">&quot;Large&quot;</span>, <span class="st">&quot;Medium&quot;</span>),</a>
<a class="sourceLine" id="cb223-8" data-line-number="8">                                  <span class="st">&quot;Small&quot;</span>))</a>
<a class="sourceLine" id="cb223-9" data-line-number="9"></a>
<a class="sourceLine" id="cb223-10" data-line-number="10"><span class="co"># check levels of factor</span></a>
<a class="sourceLine" id="cb223-11" data-line-number="11"><span class="kw">levels</span>(iris_trn_mod<span class="op">$</span>Sepal.Width)</a></code></pre></div>
<pre><code>## [1] &quot;Large&quot;  &quot;Medium&quot; &quot;Small&quot;</code></pre>
<p>Here we make a new dataset where <code>Sepal.Width</code> is categorical, with levels <code>Small</code>, <code>Medium</code>, and <code>Large</code>. We then try to train classifiers using only the sepal variables.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" data-line-number="1"><span class="kw">NaiveBayes</span>(Species <span class="op">~</span><span class="st"> </span>Sepal.Length <span class="op">+</span><span class="st"> </span>Sepal.Width, <span class="dt">data =</span> iris_trn_mod)<span class="op">$</span>tables</a></code></pre></div>
<pre><code>## $Sepal.Length
##                [,1]      [,2]
## setosa     5.033333 0.4113337
## versicolor 6.068182 0.5454455
## virginica  6.534783 0.4744458
## 
## $Sepal.Width
##             var
## grouping         Large    Medium     Small
##   setosa     0.1000000 0.7666667 0.1333333
##   versicolor 0.0000000 0.3181818 0.6818182
##   virginica  0.0000000 0.4347826 0.5652174</code></pre>
<p>Naive Bayes makes a somewhat obvious and intelligent choice to model the categorical variable as a multinomial. It then estimates the probability parameters of a multinomial distribution.</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" data-line-number="1"><span class="kw">lda</span>(Species <span class="op">~</span><span class="st"> </span>Sepal.Length <span class="op">+</span><span class="st"> </span>Sepal.Width, <span class="dt">data =</span> iris_trn_mod)</a></code></pre></div>
<pre><code>## Call:
## lda(Species ~ Sepal.Length + Sepal.Width, data = iris_trn_mod)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##  0.4000000  0.2933333  0.3066667 
## 
## Group means:
##            Sepal.Length Sepal.WidthMedium Sepal.WidthSmall
## setosa         5.033333         0.7666667        0.1333333
## versicolor     6.068182         0.3181818        0.6818182
## virginica      6.534783         0.4347826        0.5652174
## 
## Coefficients of linear discriminants:
##                        LD1        LD2
## Sepal.Length      2.259576  0.6833882
## Sepal.WidthMedium 1.531021 -0.5070114
## Sepal.WidthSmall  3.119457 -2.3385379
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9879 0.0121</code></pre>
<p>LDA (and QDA) however creates dummy variables, here with <code>Large</code> as the reference level, then continues to model them as normally distributed. Not great, but better then not using a categorical variable.</p>
<hr />
</div>
<div id="source-9" class="section level2">
<h2><span class="header-section-number">10.6</span> Source</h2>
<ul>
<li><code>R</code> Markdown: <a href="generative.Rmd"><code>generative.Rmd</code></a></li>
</ul>
<hr />

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="binary-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervised-i.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/generative.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
