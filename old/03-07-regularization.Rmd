# Regularization

***

## STAT 432 Materials

- ISL Readings: Sections 6.1 - 6.4

***

```{r resampling_opts, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE, fig.align = "center")
```

```{r, message = FALSE, warning = FALSE}
library("tidyverse")
library("glmnet")
library("broom")
library("kableExtra")
```

## Reducing Variance with Added Bias

```{r}
gen_simple_data = function(sample_size = 25) {
  x = runif(n = sample_size)
  y = 0 + 5 * x + rnorm(n = sample_size)
  data.frame(x, y)
}
```

```{r}
set.seed(42)
simple_data = gen_simple_data()
```

```{r}
# fit least squares
beta_ls = lm(y ~ 0 + x, data = simple_data)

# fit a biased model
# restrict beta-hat to be at most 5
beta_05 = lm(y ~ 0 + x, data = simple_data)
beta_05$coefficients = min(beta_05$coefficients, 5)

# fit a biased model
# restrict beta-hat to be at most 4
beta_04 = lm(y ~ 0 + x, data = simple_data)
beta_04$coefficients = min(beta_04$coefficients, 4)
```

```{r}
map_dbl(list(beta_ls, beta_05, beta_04), coef)
```

```{r, fig.height = 6, fig.width = 10, echo = FALSE}
plot(simple_data, pch = 20, col = "darkgrey",
     xlim = c(0, 1), ylim = c(0, 7))
grid()

abline(0, 5, lwd = 4)
abline(beta_ls, col = "firebrick",  lwd = 3, lty = 2)
abline(beta_05, col = "dodgerblue", lwd = 3, lty = 3)
abline(beta_04, col = "limegreen",  lwd = 3, lty = 4)

# TODO: add legend
```

```{r}
# maybe write a function for each
# should switch this to make each model fit to the same data
set.seed(42)
beta_estimates = list(
  beta_ls = replicate(n = 5000, coef(lm(y ~ 0 + x, data = gen_simple_data()))["x"]),
  beta_05 = replicate(n = 5000, min(coef(lm(y ~ 0 + x, data = gen_simple_data()))["x"], 5)),
  beta_04 = replicate(n = 5000, min(coef(lm(y ~ 0 + x, data = gen_simple_data()))["x"], 4))
)
```

```{r, echo = FALSE}
tibble(
  Model = c("Least Squares", "Biased to 5", "Biased to 4"),
  Bias = map_dbl(beta_estimates, ~ {mean(.x) - 5}),
  Variance = map_dbl(beta_estimates, var),
  MSE = Bias ^ 2 + Variance
) %>% 
  kable(digits = 3) %>% 
  kable_styling("striped", full_width = FALSE)
```

## scaling matters?

```{r}
another_dgp = function(sample_size = 25) {
  x = runif(n = sample_size)
  y = -2 + 5 * x + rnorm(n = sample_size)
  tibble(x, y)
}
```


```{r}
data_for_scaling = another_dgp()
predict(lm(y ~ x, data = data_for_scaling))
coef(lm(y ~ x, data = data_for_scaling))

data_for_scaling$x = scale(data_for_scaling$x)
predict(lm(y ~ x, data = data_for_scaling))
coef(lm(y ~ x, data = data_for_scaling))
```

## Constraints in Two Dimensions

```{r}
gen_linear_data = function() {
  x1 = rnorm(100)
  x2 = rnorm(100)
  y = 0 + -5 * x1 + 5 * x2 + rnorm(100)
  tibble(x1 = x1, x2 = x2, y = y)
}
```

```{r}
data = gen_linear_data()
beta = expand.grid(beta_1 = seq(-10, 10, 0.1),
                   beta_2 = seq(-10, 10, 0.1))
beta_error = rep(0, dim(beta)[1])
for (i in 1:dim(beta)[1]){
  beta_error[i] = with(data, sum((y - (beta$beta_1[i] * x1 + beta$beta_2[i] * x2)) ^ 2 ))
}
```

```{r}
# TODO: make this into a function
# TODO: add ridge constraint
contour(x = seq(-10, 10, 0.1), 
        y = seq(-10, 10, 0.1), 
        z = matrix(beta_error, 
                   nrow = length(seq(-10, 10, 0.1)),
                   ncol = length(seq(-10, 10, 0.1))),
        nlevels = 50,
        col = "darkgrey"
)

abline(h = 0)
abline(v = 0)
a = 4
segments(0, a, a, 0, col = "dodgerblue", lwd = 2)
segments(0, -a, a, 0, col = "dodgerblue", lwd = 2)
segments(-a, 0, 0, a, col = "dodgerblue", lwd = 2)
segments(-a, 0, 0, -a, col = "dodgerblue", lwd = 2)
points(beta[which.min(beta_error), ], col = "darkorange", pch = 20, cex = 2)
```

## High Dimensional Data

```{r}
gen_wide_data = function(sample_size = 100, sig_betas = 5, p = 200) {
  
  if (p <= sample_size) {
    warning("You're not generating wide data, despite the name of the function.")
  }
  
  if (sig_betas > p) {
    stop("Cannot have more significant variables than variables!")
  }
  
  x = map_dfc(1:p, ~ rnorm(n = sample_size))
  x = x %>% rename_all( ~ str_replace(., "V", "x"))
  sig_x = x[, 1:sig_betas]
  beta = rep(3, times = sig_betas)
  y = as.matrix(sig_x) %*% beta + rnorm(n = sample_size)
  bind_cols(y = y, x)
}
```

```{r}
some_wide_data = gen_wide_data()
```

```{r}
some_wide_data
```


## Ridge Regression


$$
\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}    \right) ^ 2 + \lambda \sum_{j=1}^{p} \beta_j^2 .
$$

```{r}
set.seed(42)
data_for_ridge = gen_wide_data(sample_size = 100, sig_betas = 5, p = 200)
```

```{r}
x_ridge = data_for_ridge %>% select(-y) %>% as.matrix()
y_ridge = data_for_ridge %>% pull(y)
```

```{r}
mod_ridge = glmnet(x = x_ridge, y = y_ridge, alpha = 0)
```

```{r, fig.height = 6, fig.width = 8}
plot(mod_ridge, xvar = "lambda", label = TRUE)
grid()
```

```{r}
as_tibble(predict(mod_ridge, x_ridge[1:5, ]))
```

```{r}
set.seed(42)
mod_ridge = cv.glmnet(x = x_ridge, y = y_ridge, alpha = 0, nfolds = 5)
```

```{r}
glance(mod_ridge)
```

```{r}
plot(mod_ridge)
```


```{r}
tidy(mod_ridge)
```

## Lasso










## boston is boring

```{r}
bstn = MASS::Boston

bstn$chas = factor(bstn$chas)
bstn$rad = factor(bstn$rad)

levels(bstn$chas)
levels(bstn$rad)
```

```{r}
lm(medv ~ ., data = bstn)
```

```{r}
head(as_tibble(model.matrix(lm(medv ~ ., data = bstn))))
```

```{r}
bstn_x = model.matrix(lm(medv ~ ., data = bstn))
bstn_y = bstn$medv
```

```{r}
coef(lm.fit(x = bstn_x, y = bstn_y))
```

```{r}
bstn_x = model.matrix(lm(medv ~ ., data = bstn))[, -1]
bstn_y = bstn$medv
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot(glmnet(x = bstn_x, y = bstn_y, alpha = 0), xvar = "lambda")
grid()
plot(glmnet(x = bstn_x, y = bstn_y, alpha = 1), xvar = "lambda")
grid()
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot(cv.glmnet(x = bstn_x, y = bstn_y, alpha = 0))
plot(cv.glmnet(x = bstn_x, y = bstn_y, alpha = 1))
```

```{r}
bstn_ridge = cv.glmnet(x = bstn_x, y = bstn_y, alpha = 0)
bstn_lasso = cv.glmnet(x = bstn_x, y = bstn_y, alpha = 1)
```

```{r}
library("broom")
```

```{r}
tidy(bstn_lasso)
glance(bstn_lasso)
# TODO: pull out rows of tidy with the values from glance
```


```{r}
predict(bstn_lasso, newx = bstn_x[1:10,], type = "link")
```

```{r}
predict(bstn_lasso, newx = bstn_x[1:10,], type = "response")
```

```{r}
predict(bstn_lasso, type = "coefficients", s=c("lambda.1se","lambda.min"))
```

```{r}
predict(bstn_lasso, type = "nonzero")
```


## some more simulation

```{r}
# diag(100)
```


```{r}
p = 100
A = matrix(runif(p ^ 2) * 2 - 1, ncol = p)
Sigma = t(A) %*% A
sample_size = 500
X = MASS::mvrnorm(n = sample_size, mu = rep(0, p), Sigma = Sigma)
beta = ifelse(sample(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1), size = p, replace = TRUE), runif(n = p, -1, 1), 0) * 2
y = X %*% beta + rnorm(n = sample_size, sd = 4)
fit = glmnet::cv.glmnet(x = X, y = y, alpha = 1)
sqrt(min(fit$cvm))
```

```{r}
plot(fit, xlim = c(-6, 1), ylim = c(15, 20))
```


```{r}
# type.measure = "class"
```



- TODO: Least Absolute Shrinkage and Selection Operator
- TODO: https://statisticaloddsandends.wordpress.com/2018/11/15/a-deep-dive-into-glmnet-standardize/
- TODO: https://www.jaredlander.com/2018/02/using-coefplot-with-glmnet/
- TODO: statistical learning with sparsity book
