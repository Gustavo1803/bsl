[
["index.html", "Basics of Statistical Learning Start Here Caveat Emptor Who? Acknowledgements License", " Basics of Statistical Learning David Dalpiaz 2020-01-26 Start Here Welcome to Basics of Statistical Learning! What a boring title! The title was chosen to mirror the University of Illinois course STAT 432 - Basics of Statistical Learning. That title was chosen to meet certain University course naming conventions, hence the boring title. A more appropriate title would be “Machine Learning from the Perspective of a Statistician who uses R,” which is more descriptive, but still a boring title. Anyway, this “book” will often be referred to as BSL. Caveat Emptor This “book” is under active development. Literally every element of the book is subject to change, at any moment. This text, BSL, is the successor to R4SL, an unfinished work that began as a supplement to Introduction to Statistical Learning, but was never finished. (In some sense, this book is just a fresh start due to the author wanting to change the presentation of the material. The author is seriously worried that he will encounter the second-system effect.) Because this book is written with a course in mind, that is actively being taught, often out of convenience the text will speak directly to the students of that course. Thus, be aware that any reference to a “course” are a reference to STAT 432 @ UIUC. Since this book is under active development you may encounter errors ranging from typos, to broken code, to poorly explained topics. If you do, please let us know! Better yet, fix the issue yourself! If you are familiar with R Markdown and GitHub, pull requests are highly encouraged!. This process is partially automated by the edit button in the top-left corner of the html version. If your suggestion or fix becomes part of the book, you will be added to the list at the end of this chapter. We’ll also link to your GitHub account, or personal website upon request. If you’re not familiar with version control systems feel free to email the author, dalpiaz2 AT illinois DOT edu. (But also consider using this opportunity to learn a bit about version control!) See additional details in the Acknowledgements section below. While development is taking place, you may see “TODO” scattered throughout the text. These are mostly notes for internal use, but give the reader some idea of what development is still to come. For additional details on the development process, please see the README file on GitHub as well as the Issues page. Who? This book is targeted at advanced undergraduate or first year MS students in Statistics who have no prior machine learning experience. While both will be discussed in great detail, previous experience with both statistical modeling and R are assumed. In other words, this books is for students in STAT 432. Acknowledgements The following is a (likely incomplete) list of helpful contributors. This book was also influenced by the helpful contributors to R4SL. Jae-Ho Lee - STAT 432, Fall 2019 W. Jonas Reger - STAT 432, Spring 2020 Your name could be here! Please see the CONTRIBUTING document on GitHub for details on interacting with this project. Pull requests encouraged! Looking for ways to contribute? You’ll notice that a lot of the plotting code is not displayed in the text, but is available in the source. Currently that code was written to accomplish a task, but without much thought about the best way to accomplish the task. Try refactoring some of this code. Fix typos. Since the book is actively being developed, typos are getting added all the time. Suggest edits. Good feedback can be just as helpful as actually contributing code changes. License CC NC SA This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License "],
["ten-simple-rules-for-success-in-stat-432.html", "Chapter 1 Ten Simple Rules for Success in STAT 432 1.1 Rule 1: There Are No Rules 1.2 Rule 2: Read the Syllabus 1.3 Rule 3: Previous Learning is Not Gospel 1.4 Rule 4: All Statements Are True 1.5 Rule 5: Don’t Miss The Forest For The Trees 1.6 Rule 6: You Will Struggle 1.7 Rule 7: Keep It Simple 1.8 Rule 8: RTFM 1.9 Rule 9: There Are No Stupid Questions 1.10 Rule 10: Learn By Doing 1.11 Conclusion 1.12 Source", " Chapter 1 Ten Simple Rules for Success in STAT 432 STAT 432 is a difficult course. Although, as a result of the historical grade distribution I believe some students enter the course believing that it is an “easy” course. This document was written to help address the reasons for this difference between perception and reality. The style is stolen from the popular “Ten Rules” articles published in PLOS journals. A relevant example is Ten Simple Rules for Effective Statistical Practice. 1.1 Rule 1: There Are No Rules Perhaps it is odd to begin a list of rules exclaiming that there are no rules. Many students that enroll in STAT 432 have an extensive mathematics background where everything follows a set of rules. However, statistics is not mathematics. While there are certainly rules in statistics, in applied statistics, an analyst must make decisions that can only be guided by heuristics. Students often ask questions such as “What method should I use in this situation?” hoping for a specific answer. While it would be easy to simply provide an “answer” of some specific model or procedure, the reality is that the answer will almost always be “it depends” and the analyst will have to make a somewhat subjective decision based on an extremely long set of heuristics. (The other answer to “Which method should I use in this situation?” will be &quot;Who knows? Try a few and evaluate which is working best. Evaluating methods will be a big focus in the course. We care more about the ability to evaluate methods than understanding the inner workings of each method.) In other words, while we could simply write some sort of flow-chart that tells you what to do in any situation encountered in the course, we reject this authoritarian approach. We prefer to present some heuristics, some reasoning behind them, and allow you to think for yourself. Skepticism is encouraged. You are allowed to form your own opinions about the course material. Applied to the data analyses done in STAT 432: There is no single correct answer. There are only good arguments and bad arguments. 1.2 Rule 2: Read the Syllabus Please. A familiarity with the syllabus will make your experience in the course much smoother. I would suggest returning to the syllabus a number of times throughout the semester, perhaps shortly before the exams. 1.3 Rule 3: Previous Learning is Not Gospel A trap many students fall into is believing that everything they have previously learned is relevant in future courses. This is not the case. Just because a method was taught in STAT 420 or STAT 425 does not mean that it is relevant in STAT 432. The most common example of this is Variance Inflation Factors. Students seem to love to drag this concept into STAT 432. While it is certainly possible to appeal to VIFs in STAT 432, they seem to be misapplied more often than not. This is because VIFs are more relevant for inference than prediction. STAT 432 cares about prediction much more than STAT 420 and STAT 425. 1.4 Rule 4: All Statements Are True Rule 4 is somewhat related to Rule 3. The full rule would read: “All statements are true, given the correct conditions.” Rule 3 is relevant here because students will often search for information on the internet. They’ll arrive at some prescription such as “Method X is good at Task Y.” In reality, this statement is always more correctly stated as “Method X is good at Task Y under Condition Z.” In other words, context is extremely important. 1.5 Rule 5: Don’t Miss The Forest For The Trees STAT 432 covers a lot of content, sometimes at a surface level. When only scratching the surface, students find the lack of details unsatisfying. This is understandable, but realize that STAT 432 is a first course in machine learning. We don’t believe it is possible to learn all of machine learning in a single course. STAT 432 is about having an understanding of what machine learning is and what you can do with it. Our goal is not to teach you every single detail. (That is impossible.) Instead, we would like to provide a high level overview that will serve as a foundation for future learning. (Both self-study and future courses.) 1.6 Rule 6: You Will Struggle You will struggle, and that is a good thing. If everything in the course were “easy,” very little learning would take place. However, we are not advocating struggle for the sake of struggle. We want to support your “struggle” with the material. The course staff is not the enemy. The material is the “enemy.” We are here to help you. Do not hesitate to ask the course staff questions! Come to office hours! Post on Piazza! 1.7 Rule 7: Keep It Simple Please keep the KISS Principle in mind. (The name is somewhat unfortunate. No, we are not calling you stupid.) Complexity does not imply valuable. Within the context of STAT 432: All else being equal, we prefer “simple” methods and models. When writing reports, shorter is better. (As long as you convey the necessary information.) 1.8 Rule 8: RTFM Warning, the following link contains foul language: RTFM. RTFM is a common phrase in coding culture. While extremely insensitive, it is perhaps some of the most relevant advice for STAT 432. In short, if you experience a coding issue: Read the documentation of the function you are trying to use. (Anything you are doing in R involves running a function.) Search the internet (“Google”) for any error messages you’ve encountered. This should always be your first line of defense any time you encounter an issue. However, we do not expect you to be able to solve all your problems with this method! That’s why office hours exists! We simply would like you to get into this habit. Having gone through this step, you are more likely to solve the problem yourself. Additionally, you will be better prepared to discuss any issue with the course staff if you are unable to solve it yourself. 1.9 Rule 9: There Are No Stupid Questions It sounds cliche, but it is true. Do not hesitate to ask the course staff questions! Come to office hours! Post on Piazza! Do note that while there really are no stupid questions, there are some annoying questions. For example: Questions that can be answered by reading the syllabus. (“When is Exam 01?” “When are office hours?”) “Can you tell me what is wrong with my code?” The second bullet requires some explaining. The direct answer to that question is technically “Yes.” However, please note that the course staff is not a debugging machine. If you simply supply us with a bunch of code and asks us what is wrong, you’ll be met with a bit of frustration. We expect that you at least pinpoint where there is an issue with the code, within reason. (We will give some advice on how best to do this as the semester progresses.) In other words, try to ask your code question in a way that demonstrates that you have already thought about solving your issue. (See Rule #9.) We will always work with you to resolve your issue, but we ask that we are not your first attempt at a resolution. Corollary: There are no “quick” questions. Student often like to preface a question with the phrase “Just one quick question.” If you’re asking the question, how do you know how long it will take to answer? (I suppose if you know it’s in the syllabus, then the answer will be quick, but …) More often than not students ask excellent questions but then expect a short, succinct answer. When you ask a good question, this is often not possible. The point is, please come to office hours where we can have an in depth conversation, that is not time limited, with additional input from you! 1.10 Rule 10: Learn By Doing Students overvalue lecture and undervalue homework. STAT 432 will probably contain less “lecture” than you expect, and far too much “work” in the form of quizzes and analyses. Watching a lecture is a passive activity. Taking quizzes is an active activity. Reading is a passive activity. Performing an analysis is an active activity. In my opinion, students enjoy (or more specifically don’t dislike) lecture because it requires zero input from them. On the other hand, quizzes are frustrating, but that is a good thing! That frustration means that there is something to learn! Stated practically and with relevance to STAT 432: The quizzes and other course activities should be given highest priority when allotting your time. You should read all posted notes twice. The first time you should read every word of the assigned material from top to bottom. Try to understand as best as possible, but don’t spend too much time reading any particular section. In a first read, it is somewhat more important to read the material to know what is there than to fully understand it. When taking the quizzes or performing an analysis, return to the relevant section of the reading. (Because you read it once before, you’ll know a certain section exists, even if you don’t understand it.) Read it again. This time you’ll have more context, and a better chance of understanding. Run the code. Modify the code and run it again. Write some similar code from scratch by yourself. Now, return to the quiz. Recorded lectures will be sparse, but if they exist, watch it once (possibly at 2x speed) but then go back to focusing on the “active” activities. 1.11 Conclusion In summary, if you bring an open mind and a bit of effort, we believe that you will succeed in STAT 432. We don’t believe that the course is easy, but we hope that it is ultimately rewarding. 1.12 Source R Markdown: 01-ten-rules.Rmd "],
["machine-learning-overview.html", "Chapter 2 Machine Learning Overview 2.1 Reading 2.2 What is Machine Learning? 2.3 Machine Learning Tasks 2.4 Open Questions 2.5 Source", " Chapter 2 Machine Learning Overview STAT 432 is a course about machine learning? Let’s try to define machine learning. 2.1 Reading Required: ISL Chapter 1 Required: ISL Chapter 2 This chapter is dense because it is an overview of the entire book. Do not expect to completely understand this chapter during a first read. We will spend the rest of the semester elaborating on these concepts. However, seeing them at the beginning will be helpful. Recommended: Variance Explained: What’s the difference between data science, machine learning, and artificial intelligence? 2.2 What is Machine Learning? Machine learning (ML) is about learning functions from data. That’s it. Really. Pretty boring, right? To quickly address some buzzwords that come up when discussing machine learning: Deep learning is just a subset of machine learning. Artificial Intelligence (AI) overlaps machine learning but has much loftier goals. In general, if someone claims to be using AI, they are not. (They’re probably using function learning! For example, we will learn logistic regression in this course. People in marketing might call that AI! Someone who understands ML will simply call it function learning. Don’t buy the hype! We don’t need to call simple methods AI to make them effective.) Machine learning is not data science. Data science sometimes uses machine learning. Does big data exist? If it does, I would bet a lot of money that you haven’t seen it, and probably won’t see it. Analytics is just a fancy word for doing data analysis. Machine learning can be used in analyses! When it is, it is often called “Predictive Analytics.” What makes machine learning interesting are the uses of these functions. We could develop functions that have applications in a wide variety of fields. In medicine, we could develop a function that helps detect skin cancer. Input: Pixels from an image of mole Output: A probability of skin cancer In sport analytics, we could develop a function that helps determine player salary. Input: Lifetime statistics of an NBA player Output: An estimate of player’s salary In meteorology, we could develop a function to predict the weather. Input: Historic weather data in Champaign, IL Output: A probability of rain tomorrow in Champaign, IL In political science we could develop a function that predicts the mood of the president. Input: The text of a tweet from President Donald Trump Output: A prediction of Donald’s mood (happy, sad, angry, etc) In urban planning we could develop a function that predicts the rental prices of Airbnbs. Input: The attributes of the location for rent Output: An estimate of the rent of the property How do we learn these functions? By looking at many previous examples, that is, data! Again, we will learn functions from data. That’s what we’re here to do. 2.3 Machine Learning Tasks When doing machine learning, we will classify our tasks into one of two categories, supervised or unsupervised learning. (There are technically other tasks such as reinforcement learning and semi-supervised learning, but they are outside the scope of this text. To understand these advanced tasks, you should first learn the basics!) Within these two broad categories of ML tasks, we will define some specifics. 2.3.1 Supervised Learning In supervised learning, we want to “predict” a specific target, outcome, or response variable. In the following examples, this is the y variable. Supervised learning tasks are called regression if the response variable is numeric. If a supervised learning tasks has a categorical response, it is called classification. 2.3.1.1 Regression In the regression task, we want to predict numeric response variables. The predictor variables, which we will call the feature variables, or simply features can be either categorical or numeric. x1 x2 x3 y A -0.66 0.48 14.09 A 1.55 0.97 2.92 A -1.19 -0.81 15.00 A 0.15 0.28 9.29 B -1.09 -0.16 17.57 B 1.61 1.94 2.12 B 0.04 1.72 8.92 A 1.31 0.36 4.40 C 0.98 0.30 4.40 C 0.88 -0.39 4.52 With the data above, our goal would be to learn a function that takes as input values of the three features (x1, x2, and x3) and returns a prediction (best guess) for the true (but usually unknown) value of the response y. For example, we could obtain some “new” data that does not contain the response. x1 x2 x3 B -0.85 -2.41 We would then pass this data to our function, which would return a prediction of the value of y. Stated mathematically, our prediction will often be an estimate the conditional mean of \\(Y\\), given values of the \\(\\boldsymbol{X}\\) variables. \\[ \\mu(\\boldsymbol{x}) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\] In other words, we want to learn this function, \\(\\mu(\\boldsymbol{x})\\). Much more on this later. (You can safely ignore this for now.) 2.3.1.2 Classification Classification is similar to regression, except it considers categorical response variables. x1 x2 x3 y Q 0.46 5.42 B Q 0.72 0.83 C Q 0.93 5.93 B Q 0.26 5.68 A P 0.46 0.49 B P 0.94 3.09 B P 0.98 2.34 C P 0.12 5.43 C Q 0.47 2.68 B P 0.56 5.02 B As before we want to learn a function from this data using the same inputs, except this time, we want it to output one of A, B, or C for predictions of the y variable. Again, consider some new data: x1 x2 x3 P 0.96 5.33 While ultimately we would like our function to return one of A, B, or C, what we actually would like is an intermediate return of probabilities that y is A, B, or C. In other words, we are attempting to estimate the conditional probability that \\(Y\\) is each of the possible categories, given values of the \\(\\boldsymbol{X}\\) values. \\[ p_k(\\boldsymbol{x}) = P\\left[ Y = k \\mid \\boldsymbol{X} = \\boldsymbol{x} \\right] \\] We want to learn this function, \\(p_k(\\boldsymbol{x})\\). Much more on this later. (You can safely ignore this for now.) 2.3.2 Unsupervised Learning Unsupervised learning is a very broad task that is rather difficult to define. Essentially, it is learning without a response variable. To get a better idea about what unsupervised learning is, consider some specific tasks. x1 x2 x3 x4 x5 2.74 0.46 5.42 4.43 2.28 2.81 0.72 0.83 4.87 2.61 0.86 0.93 5.93 2.33 0.22 2.49 0.26 5.68 4.11 5.84 1.93 0.46 0.49 0.02 2.59 -8.44 -9.06 -6.91 -5.00 -4.25 -7.79 -9.02 -7.66 -9.96 -4.67 -9.60 -9.88 -4.57 -8.75 -6.16 -8.03 -9.53 -7.32 -4.56 -4.17 -7.88 -9.44 -4.98 -6.33 -6.29 2.3.2.1 Clustering Clustering is essentially the task of grouping the observations of a dataset. In the above data, can you see an obvious grouping? (Hint: Compare the first five observations to the second five observations.) In general, we try to group observations that are similar. 2.3.2.2 Density Estimation Density estimation tries to do exactly what the name implies, estimate the density. In this case, the joint density of \\(X_1, X_2, X_3, X_4, X_5\\). In other words, we would like to learn the function that generated this data. (You could take the position that this is the only machine learning tasks, and all other tasks are subset of this task. We’ll hold off on explaining this for a while.) 2.3.2.3 Outlier Detection Consider some new data: x1 x2 x3 x4 x5 67 66.68 66.26 69.49 70 Was this data generated by the same process as the data above? If no, we would call it an outlier. 2.4 Open Questions The two previous sections were probably more confusing than helpful. But of course, because we haven’t started learning yet! Hopefully, you are currently pondering one very specific question: How do we learn functions from data? That’s what this text will be about! We will spend a lot of time on this question. It is what us statisticians call fitting a model. On some level the answer is: look at a bunch of old data before predicting on new data. While we will dedicate a large amount of time to answering this question, sometimes, some of the details might go unanswered. Since this is an introductory text, we can only go so far. However, as long as we answer another question, this will be OK. How do we evaluate how well learned functions work? This text places a high priority on being able to do machine learning, specifically do machine learning in R. You can actually do a lot of machine learning without fully understanding how the learning is taking place. That makes the evaluation of ML models extremely important. 2.5 Source R Markdown: 02-ml-overview.Rmd "],
["computing.html", "Chapter 3 Computing 3.1 Reading 3.2 Additional Resources 3.3 STAT 432 Idioms 3.4 Source", " Chapter 3 Computing STAT 432 is not a course about R. It is however, a course that makes heavy use of R. Because of this, you will need to be familiar with R. This text will point out some things about R along the way, but some previous study of R is necessary. 3.1 Reading The following reading suggestions are long. While it may seem daunting to read all of this material, it will likely prove to be valuable. A smart strategy would be: “Read” as much of the information here, where “read” simply means read every single word and line of code, but don’t slow down if you don’t fully understand. Return to some selections from this reading every week spending more time understanding specific sections. Whether you’re a novice or an expert, there is a high probability that any effort towards reading these sources will provide a return. If you use the strategy above, over time you will start to see the bigger picture. Required: Hands-On Programming with R - Garrett Grolemund If you have never used R or RStudio before, Part 1 (Chapters 1 - 3) will be useful. Even if you have used R before, you will likely gain something from reading these sections. Recommended: R for Data Science - Garrett Grolemund, Hadley Wickham This book helps getting you up to speed working with data in R. While it is a lot of reading, Chapters 1 - 21 may prove useful. It probably isn’t worth sitting down and reading this from start to finish right now, but reading it here and there during some free time would be a good idea. Recommended: Advanced R - Hadley Wickham Part I (Chapters 1 - 8) of this book will help create a mental model for working with R. These chapters are not an easy read, so they should be returned to often. Chapter 2 could be safely skipped for our purposes, but is important if you will use R in the long term. Reference: Applied Statistics with R - David Dalpiaz If you are a UIUC student who took the course STAT 420, the first six chapters of that book could serve as a nice refresher, however, the three readings above are preferred. Chapter 6 contain three very useful video playlists and an R Markdown template. Note that the videos were created a few years ago, thus there may be minor differences between then and now, but the general ideas are the same. Video Playlist: R and RStudio Video Playlist: Data in R Video Playlist: R Markdown Template: R Markdown R Markdown will be used throughout the course, but you will not be required to use R Markdown until the final weeks when we begin working on data analyses. At that time, we will suggest some additional reading about R Markdown. When working on quizzes until then, you can use either an R script or an R Markdown document, whichever you are more comfortable with. 3.2 Additional Resources In addition to the above readings, the following resources are more specific or more advanced, but could still prove to be useful. 3.2.1 R Efficient R programming R Programming for Data Science R Graphics Cookbook Modern Dive What They Forgot to Teach You About R The R Inferno Data Wrangling, Exploration, and Analysis with R The tidyverse Website dplyr Website readr Website tibble Website forcats Website 3.2.2 RStudio RStudio IDE Cheatsheet RStudio Resources 3.2.3 R Markdown R Markdown Cheatsheet R Markdown: The Definitive Guide - Yihui Xie, J. J. Allaire, Garrett Grolemund R Markdown Cookbook R4DS: R Markdown Chapter 3.2.3.1 Markdown Daring Fireball - Markdown: Basics GitHub - Mastering Markdown CommonMark 3.3 STAT 432 Idioms R tutorials and advice are plentiful online. While we do not want to discourage you from using the resources above, or using your own creativity, the following sections will specify some strong suggestions for how to use R, RStudio, and R Markdown in STAT 432. In other words, information below here supersedes any information from the above sources. 3.3.1 Don’t Restore Old Workspaces Due to some odd default settings in RStudio, some students never clear their R envrioment. This is a problem for a number of reasons. (It could prevent your results from being reproducible.) To get ready for STAT 432, do the following: Clear your current enviroment. Change some RStudio defaults. Deselect &quot;Restore .RData into workspace at startup. Set “Save workspace .RData on exit:” to “Never” This will save you a lot of headache in STAT 432 and the future. 3.3.2 R Versions You should always use the most up-to-date version of R and RStudio. You must use at least R versions 3.6.2. Importantly, R versions after 3.6.0 have slightly different random number generation. Although, even with the most recent version, sometimes R keeps the old random number generation. To check that you are on the most recent random number generation, run: RNGkind() ## [1] &quot;Mersenne-Twister&quot; &quot;Inversion&quot; &quot;Rejection&quot; If your output matches the output above, you’re all set. If not, run: RNGversion(getRversion()) Then re-run RNGkind() and everything should match up and you should be go to go! 3.3.3 Code Style Code needs to be read by two distinct groups: Computers Humans Computers will complain, very loudly, when you write “bad” code, that is code does not run. We need to write code that is syntactically correct for the computer to be able to “read” our code. Computers only care about syntax. If we relate this to natural language, we would say that computers really only care about grammar and punctuation. They don’t worry about style like phrasing, tone, etc. While computers will complain about bad code, does anyone really care about wasting their time? (OK, sure, computer scientists might.) If we give them bad code, they try to run it, fail, then complain. However, they’re soulless machines, they can handle it. Humans on the other hand have a finite amount of time on this earth. Even if we solve aging, we still have to deal with the heat death of the universe. Thus, while wasting a computer’s time is no big deal, wasting a human’s time is, frankly, immoral. What does this have to do with R programming? Humans are going to read your R code. One of those humans is likely to be you, but future you. To make this reading possible and efficient, you need to develop style. Here is some code that a computer can read, but a human will struggle to read: set.seed(1337);mu=10;sample_size=50;samples=100000;x_bars=rep(0, samples); for(i in 1:samples){x_bars[i]=mean(rpois(sample_size,lambda = mu))} x_bar_hist=hist(x_bars,breaks=50,main=&quot;Histogram of Sample Means&quot;, xlab=&quot;Sample Means&quot;,col=&quot;darkorange&quot;,border = &quot;dodgerblue&quot;) Now, written again, but readable by a human: # set seed for reproducibility set.seed(1337) # setup simulation parameters mu = 10 sample_size = 50 samples = 100000 # create vector to store simulation results x_bars = rep(0, samples) # run simulation for (i in 1:samples) { x_bars[i] = mean(rpois(sample_size, lambda = mu)) } # plot results x_bar_hist = hist( x_bars, breaks = 50, main = &quot;Histogram of Sample Means&quot;, xlab = &quot;Sample Means&quot;, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot; ) To the computer, these are the same. To a human, one makes you want to pull your hair out, the other you can glance at and have a pretty good idea about what is going on. Style is subjective, but we’ll define some guiding principles, and a few rules. 3.3.4 Reference Style So as to not have to define a style from the ground up, we will use the tidyverse style as our initial reference. tidyverse Style Guide We will agree with the vast majority of the guidelines here. The exceptions are listed in the next section. 3.3.5 STAT 432 R Style Overrides All commas must be followed by a space. (Additionally, commas should never be preceded by a space.) Infix operators (==, +, -, &lt;-, etc.) should always be surrounded by spaces. Exceptions: :, ::, $, [, [[, ], ]] ^: Use x ^ 2 instead of x^2. You may use = instead of &lt;-. This is very much a minority position in the R community. But we see more examples of it being promoted every day. My reasoning for this is complicated (and I should write more about it soon) but not super important. Instead, what is important: Do not mix assignment operators. Either use = or use &lt;- but do not mix and match in the same script. Never use T or F, only TRUE or FALSE. While this should never happen, take a look at this terrifying example. FALSE == TRUE # checking for equality ## [1] FALSE F == TRUE # checking for equality ## [1] FALSE F = TRUE # A VERY BAD ASSIGNMENT! DO NOT DO THIS! F == TRUE # checking for equality, with a wild result! ## [1] TRUE # TRUE = FALSE # This won&#39;t run, which is good! Do not use ;. This is mostly a readability issue. Do not use attach(). Without going into the details, you will save yourself a lot of headache someday if you follow this advice. Do not use &lt;&lt;-. You probably didn’t know this exists. Pretend that is still the case. Do not set a working directory by using setwd() or any other method. This will make your scripts and R Markdown documents much more reproducible. Do not use absolute paths. Place a space after any # used to create a comment. No more than one newline (blank line) in a row Do not put spaces in filenames. Use dashes - or underscores _. Also consider only using lowercase. Load all packages before setting a seed. Opening (left) curly braces should not be on their own line. Except for the first argument to a function, argument names should be written in function calls. (Exception for the predict() function. Do not name the section argument to the predict() function.) Place a newline at the end of the file. 3.3.6 STAT 432 R Markdown Style Some of the previous section applies here as well, but additionally, some more specific R Markdown style guidelines: No more than one newline (blank line) in a row in an R Markdown document. No more than one newline (blank line) in a row in an R chunk. A newline before and after each chunk in an R Markdown document. No newline to start a chunk. No newline at end of chunk. (The first and last line of each chunk should contain code, or a comment for the first line.) Use headers appropriately! (Short names, good structure.) Load all needed packages at the beginning of an analysis in a single chunk. One plot per chunk! Plotting chunks should return one plot and nothing else. (No numeric printing.) 3.3.7 Style Heuristics Now that we’ve overwhelmed you with information about style, will leave you with the real advise. The most important thing to consider when evaluating the style of your code is consistency. In order, you should be consistent: with yourself! with your group! with your organization! Blindly following the rules is foolish. Breaking rules can be fun! If you do it in a way that makes life easier for everyone, by all mean, do it. 3.3.8 Objects and Functions To understand computations in R, two slogans are helpful: Everything that exists is an object. Everything that happens is a function call. — John Chambers As you continue to sharper your R skills, keep this quotation in mind. Eventually, you will realize that everything you “do” in R, you do by running a function. To debug your code, you will need to explore the objects returned by functions. To fix your code, you will need to alter the inputs to functions, which are objects. In STAT 432, the objects that we will encounter will almost always be: vectors, lists data frames model objects (Mostly lists with a class of the model type.) If you become proficient at creating, manipulating, and accessing these objects, you will likely have success in STAT 432. 3.3.9 Print versus Return One of the more confusing aspects of R is the difference between what is returned when running a function, and what is printed when running a function (interactively) as a user. Consider fitting the following linear model. cars_mod = lm(dist ~ speed, data = cars) You might think that you can simply type cars_mod to see what was returned by lm(). cars_mod ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 However, this is not what was returned. This is what was printed. To better understand what was returned, we use the `str() function. str(cars_mod) ## List of 12 ## $ coefficients : Named num [1:2] -17.58 3.93 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## $ residuals : Named num [1:50] 3.85 11.85 -5.95 12.05 2.12 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:50] -303.914 145.552 -8.115 9.885 0.194 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;(Intercept)&quot; &quot;speed&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:50] -1.85 -1.85 9.95 9.95 13.88 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ assign : int [1:2] 0 1 ## $ qr :List of 5 ## ..$ qr : num [1:50, 1:2] -7.071 0.141 0.141 0.141 0.141 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ## ..$ qraux: num [1:2] 1.14 1.27 ## ..$ pivot: int [1:2] 1 2 ## ..$ tol : num 1e-07 ## ..$ rank : int 2 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ df.residual : int 48 ## $ xlevels : Named list() ## $ call : language lm(formula = dist ~ speed, data = cars) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language dist ~ speed ## .. ..- attr(*, &quot;variables&quot;)= language list(dist, speed) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;dist&quot; &quot;speed&quot; ## .. .. .. ..$ : chr &quot;speed&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;speed&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(dist, speed) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;dist&quot; &quot;speed&quot; ## $ model :&#39;data.frame&#39;: 50 obs. of 2 variables: ## ..$ dist : num [1:50] 2 10 4 22 16 10 18 26 34 17 ... ## ..$ speed: num [1:50] 4 4 7 7 8 9 10 10 10 11 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language dist ~ speed ## .. .. ..- attr(*, &quot;variables&quot;)= language list(dist, speed) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] &quot;dist&quot; &quot;speed&quot; ## .. .. .. .. ..$ : chr &quot;speed&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;speed&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(dist, speed) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;dist&quot; &quot;speed&quot; ## - attr(*, &quot;class&quot;)= chr &quot;lm&quot; This is a huge mess, but most importantly, at the top we are told that cars_mod is a list. (It’s technically a object of class &quot;lm&quot;, but it functions like a list.) class(cars_mod) ## [1] &quot;lm&quot; is.list(cars_mod) ## [1] TRUE Thus to access certain information returned by lm() we need to access cars_mod as a list. cars_mod$coefficients ## (Intercept) speed ## -17.579095 3.932409 Note that what is returned here is a vector, so we could pull out a particular value using vector syntax. is.vector(cars_mod$coefficients) ## [1] TRUE cars_mod$coefficients[&quot;speed&quot;] ## speed ## 3.932409 Since lm() truly returns an object of type &quot;lm&quot; we can pass cars_mods to some generic functions, and then specific versions for objects of type &quot;lm&quot; will be used. coef(cars_mod) ## (Intercept) speed ## -17.579095 3.932409 predict(cars_mod, data.frame(speed = 5:10)) ## 1 2 3 4 5 6 ## 2.082949 6.015358 9.947766 13.880175 17.812584 21.744993 summary(cars_mod) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 Hey, wait, what is returned by summary()? class(summary(cars_mod)) ## [1] &quot;summary.lm&quot; is.list(summary(cars_mod)) ## [1] TRUE The summary() function returns an object of type &quot;summary.lm&quot; which functions as a list! summary(cars_mod)$fstatistic ## value numdf dendf ## 89.56711 1.00000 48.00000 You can also use the View() function, which is specific to RStudio, to view the structure of any object. View(summary(cars_mod)) # RStudio only 3.3.10 Help To get documentation about a function in R, simply put a question mark in front of the function name and RStudio will display the documentation, for example: ?log ?sin ?paste ?lm Frequently one of the most difficult things to do when learning R is asking for help. First, you need to decide to ask for help, then you need to know how to ask for help. Your very first line of defense should be to Google your error message or a short description of your issue. (The ability to solve problems using this method is quickly becoming an extremely valuable skill.) If that fails, and it eventually will, you should ask for help. There are a number of things you should include when emailing an instructor, or posting to a help website such as Stack Exchange. Describe what you expect the code to do. State the end goal you are trying to achieve. (Sometimes what you expect the code to do, is not what you want to actually do.) Provide the full text of any errors you have received. Provide enough code to recreate the error. That is, create a minimal reproducible example. If you follow these steps, you will get your issue resolved much quicker, and possibly learn more in the process. Do not be discouraged by running into errors and difficulties when learning R. (Or any technical skill.) It is simply part of the learning process. While taking STAT 432: Come to office hours! 3.3.11 Keyboard Shortcuts This section should be expanded over time, but for now, two strong suggestions: Get in the habit of using the keyboard as much as possible, and the mouse as little as possible. A keyboard is a precise entry tool, a mouse is not. Using a mouse requires you to move your hands, a keyboard does not. Hit the [TAB] key often. Like, all the time. It will autocomplete function and object names. It will autofill argument names. (This removes the need to memorize arguments!) 3.3.12 Common Issues As they arise throughout the semester, we will try to track and explain common issues here. 3.4 Source R Markdown: 03-computing.Rmd "],
["probability.html", "Chapter 4 Probability 4.1 Reading 4.2 Probability Models 4.3 Probability Axioms 4.4 Probability Rules 4.5 Random Variables 4.6 Expectations 4.7 Likelihood 4.8 References 4.9 Source", " Chapter 4 Probability STAT 432 is not a course about probability. STAT 432 is a course that uses probability. We give a very brief review of some necessary probability concepts. As the treatment is less than complete, a list of references is given at the end of the chapter. For example, we ignore the usual recap of basic set theory and omit proofs and examples. Reading the information below will likely be unsatisfying. Instead, we suggest that you skip it, engage with the relevant quizzes, then return as needed for reference. 4.1 Reading Required: Probability Distributions in R Reference: STAT 400 @ UIUC: Notes and Homework Reference: MIT 6.041: Video Lectures Reference: MIT 6.041: Lecture Notes Reference: MIT 6.041: Readings Reference: STAT 414 @ PSU: Notes 4.2 Probability Models When discussing probability models, we speak of random experiments that produce one of a number of possible outcomes. A probability model that describes the uncertainty of an experiment consists of two elements: The sample space, often denoted as \\(\\Omega\\), which is a set that contains all possible outcomes. A probability function that assigns to an event \\(A\\) a nonnegative number, \\(P[A]\\), that represents how likely it is that event \\(A\\) occurs as a result of the experiment. We call \\(P[A]\\) the probability of event \\(A\\). An event \\(A\\) could be any subset of the sample space, not necessarily a single possible outcome. The probability law must follow a number of rules, which are the result of a set of axioms that we introduce now. 4.3 Probability Axioms Given a sample space \\(\\Omega\\) for a particular experiment, the probability function associated with the experiment must satisfy the following axioms. Nonnegativity: \\(P[A] \\geq 0\\) for any event \\(A \\subset \\Omega\\). Normalization: \\(P[\\Omega] = 1\\). That is, the probability of the entire space is 1. Additivity: For mutually exclusive events \\(E_1, E_2, \\ldots\\) \\[ P\\left[\\bigcup_{i = 1}^{\\infty} E_i\\right] = \\sum_{i = 1}^{\\infty} P[E_i] \\] Using these axioms, many additional probability rules can easily be derived. 4.4 Probability Rules Given an event \\(A\\), and its complement, \\(A^c\\), that is, the outcomes in \\(\\Omega\\) which are not in \\(A\\), we have the complement rule: \\[ P[A^c] = 1 - P[A] \\] In general, for two events \\(A\\) and \\(B\\), we have the addition rule: \\[ P[A \\cup B] = P[A] + P[B] - P[A \\cap B] \\] If \\(A\\) and \\(B\\) are also disjoint, then we have: \\[ P[A \\cup B] = P[A] + P[B] \\] If we have \\(n\\) mutually exclusive events, \\(E_1, E_2, \\ldots E_n\\), then we have: \\[ P\\left[\\textstyle\\bigcup_{i = 1}^{n} E_i\\right] = \\sum_{i = 1}^{n} P[E_i] \\] Often, we would like to understand the probability of an event \\(A\\), given some information about the outcome of event \\(B\\). In that case, we have the conditional probability rule provided \\(P[B] &gt; 0\\). \\[ P[A \\mid B] = \\frac{P[A \\cap B]}{P[B]} \\] Rearranging the conditional probability rule, we obtain the multiplication rule: \\[ P[A \\cap B] = P[B] \\cdot P[A \\mid B] \\cdot \\] For a number of events \\(E_1, E_2, \\ldots E_n\\), the multiplication rule can be expanded into the chain rule: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = P[E_1] \\cdot P[E_2 \\mid E_1] \\cdot P[E_3 \\mid E_1 \\cap E_2] \\cdots P\\left[E_n \\mid \\textstyle\\bigcap_{i = 1}^{n - 1} E_i\\right] \\] Define a partition of a sample space \\(\\Omega\\) to be a set of disjoint events \\(A_1, A_2, \\ldots, A_n\\) whose union is the sample space \\(\\Omega\\). That is \\[ A_i \\cap A_j = \\emptyset \\] for all \\(i \\neq j\\), and \\[ \\bigcup_{i = 1}^{n} A_i = \\Omega. \\] Now, let \\(A_1, A_2, \\ldots, A_n\\) form a partition of the sample space where \\(P[A_i] &gt; 0\\) for all \\(i\\). Then for any event \\(B\\) with \\(P[B] &gt; 0\\) we have Bayes’ Theorem: \\[ P[A_i | B] = \\frac{P[A_i]P[B | A_i]}{P[B]} = \\frac{P[A_i]P[B | A_i]}{\\sum_{i = 1}^{n}P[A_i]P[B | A_i]} \\] The denominator of the latter equality is often called the law of total probability: \\[ P[B] = \\sum_{i = 1}^{n}P[A_i]P[B | A_i] \\] Note: When working with Bayes’ Theorem it is often useful to draw a tree diagram. Two events \\(A\\) and \\(B\\) are said to be independent if they satisfy \\[ P[A \\cap B] = P[A] \\cdot P[B] \\] This becomes the new multiplication rule for independent events. A collection of events \\(E_1, E_2, \\ldots E_n\\) is said to be independent if \\[ P\\left[\\bigcap_{i \\in S} E_i \\right] = \\prod_{i \\in S}P[E_i] \\] for every subset \\(S\\) of \\(\\{1, 2, \\ldots n\\}\\). If this is the case, then the chain rule is greatly simplified to: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = \\prod_{i=1}^{n}P[E_i] \\] 4.5 Random Variables A random variable is simply a function which maps outcomes in the sample space to real numbers. 4.5.1 Distributions We often talk about the distribution of a random variable, which can be thought of as: \\[ \\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities} \\] This is not a strict mathematical definition, but is useful for conveying the idea. If the possible values of a random variables are discrete, it is called a discrete random variable. If the possible values of a random variables are continuous, it is called a continuous random variable. 4.5.2 Discrete Random Variables The distribution of a discrete random variable \\(X\\) is most often specified by a list of possible values and a probability mass function, \\(p(x)\\). The mass function directly gives probabilities, that is, \\[ p(x) = p_X(x) = P[X = x]. \\] Note we almost always drop the subscript from the more correct \\(p_X(x)\\) and simply refer to \\(p(x)\\). The relevant random variable is discerned from context The most common example of a discrete random variable is a binomial random variable. The mass function of a binomial random variable \\(X\\), is given by \\[ p(x | n, p) = {n \\choose x} p^x(1 - p)^{n - x}, \\ \\ \\ x = 0, 1, \\ldots, n, \\ n \\in \\mathbb{N}, \\ 0 &lt; p &lt; 1. \\] This line conveys a large amount of information. The function \\(p(x | n, p)\\) is the mass function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the parameters \\(n\\) and \\(p\\). Different values of these parameters specify different binomial distributions. \\(x = 0, 1, \\ldots, n\\) indicates the sample space, that is, the possible values of the random variable. \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\) specify the parameter spaces. These are the possible values of the parameters that give a valid binomial distribution. Often all of this information is simply encoded by writing \\[ X \\sim \\text{bin}(n, p). \\] 4.5.3 Continuous Random Variables The distribution of a continuous random variable \\(X\\) is most often specified by a set of possible values and a probability density function, \\(f(x)\\). (A cumulative density or moment generating function would also suffice.) The probability of the event \\(a &lt; X &lt; b\\) is calculated as \\[ P[a &lt; X &lt; b] = \\int_{a}^{b} f(x)dx. \\] Note that densities are not probabilities. The most common example of a continuous random variable is a normal random variable. The density of a normal random variable \\(X\\), is given by \\[ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot \\exp\\left[\\frac{-1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 \\right], \\ \\ \\ -\\infty &lt; x &lt; \\infty, \\ -\\infty &lt; \\mu &lt; \\infty, \\ \\sigma &gt; 0. \\] The function \\(f(x | \\mu, \\sigma^2)\\) is the density function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the parameters \\(\\mu\\) and \\(\\sigma^2\\). Different values of these parameters specify different normal distributions. \\(-\\infty &lt; x &lt; \\infty\\) indicates the sample space. In this case, the random variable may take any value on the real line. \\(-\\infty &lt; \\mu &lt; \\infty\\) and \\(\\sigma &gt; 0\\) specify the parameter space. These are the possible values of the parameters that give a valid normal distribution. Often all of this information is simply encoded by writing \\[ X \\sim N(\\mu, \\sigma^2) \\] 4.5.4 Distributions in R R is an excellent, if not best, tool for performing probability distribution calculations. For a large number of distributions, it has four built in functions: d*(x, ...) returns the PDF at \\(x\\) (for continuous distributions) or the PMG at \\(x\\) (for discrete distributions) p*(q, ...) returns the CDF at quantile \\(q\\), that is \\(P[X \\leq q]\\) q*(p, ...) returns \\(c\\) such that \\(P[x \\leq c] = p\\) r*(n, ...) returns \\(n\\) randomly generated observations The * can be any of the disributions built in to R. The ... represents additional arguments, including the parameters of the various distributions. 4.5.5 Several Random Variables Consider two random variables \\(X\\) and \\(Y\\). We say they are independent if \\[ f(x, y) = f(x) \\cdot f(y) \\] for all \\(x\\) and \\(y\\). Here \\(f(x, y)\\) is the joint density (mass) function of \\(X\\) and \\(Y\\). We call \\(f(x)\\) the marginal density (mass) function of \\(X\\). Then \\(f(y)\\) the marginal density (mass) function of \\(Y\\). The joint density (mass) function \\(f(x, y)\\) together with the possible \\((x, y)\\) values specify the joint distribution of \\(X\\) and \\(Y\\). Similar notions exist for more than two variables. 4.6 Expectations For discrete random variables, we define the expectation of the function of a random variable \\(X\\) as follows. \\[ \\mathbb{E}[g(X)] \\triangleq \\sum_{x} g(x)p(x) \\] For continuous random variables we have a similar definition. \\[ \\mathbb{E}[g(X)] \\triangleq \\int g(x)f(x) dx \\] For specific functions \\(g\\), expectations are given names. The mean of a random variable \\(X\\) is given by \\[ \\mu_{X} = \\text{mean}[X] \\triangleq \\mathbb{E}[X]. \\] So for a discrete random variable, we would have \\[ \\text{mean}[X] = \\sum_{x} x \\cdot p(x) \\] For a continuous random variable we would simply replace the sum by an integral. The variance of a random variable \\(X\\) is given by \\[ \\sigma^2_{X} = \\text{var}[X] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2. \\] The standard deviation of a random variable \\(X\\) is given by \\[ \\sigma_{X} = \\text{sd}[X] \\triangleq \\sqrt{\\sigma^2_{X}} = \\sqrt{\\text{var}[X]}. \\] The covariance of random variables \\(X\\) and \\(Y\\) is given by \\[ \\text{cov}[X, Y] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]. \\] 4.7 Likelihood Consider \\(n\\) iid random variables \\(X_1, X_2, \\ldots X_n\\). We can then write their likelihood as \\[ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) \\triangleq f(x_1, x_2, \\ldots, x_n; \\theta) = \\prod_{i = 1}^n f(x_i; \\theta) \\] where \\(f(x_1, x_2, \\ldots, x_n; \\theta)\\) is the joint density (or mass) of \\(X_1, X_2, \\ldots X_n\\) and \\(f(x_i; \\theta)\\) is the density (or mass) function of random variable \\(X_i\\) evaluated at \\(x_i\\) with parameter \\(\\theta\\). (Note: The last equality above only holds for iid random variables.) Whereas a probability or density is a function of a possible observed value given a particular parameter value, a likelihood is the opposite. It is a function of a possible parameter values given observed data. Likelihoods are calculated when the data (the \\(x_i\\)) are known and the parameters (\\(\\theta\\)) are unknown. That is, a likelihood and a joint density will “look” the same, that is contain the same symbols. The meaning of these symbols change depending on what is known. If the data is known, and the parameters is unknown, you have a likelihood. If the parameters are known, and the data are unknown, you have a joint density. The definition above is an acknowledgement of this. The likelihood is defined to be the joint density when the data are known but the parameter(s) is unknown. Maximizing a likelihood is a common technique for fitting a model to data, however, most often we maximum the log-likelihood, as the likelihood and log-likelihood obtain their maximum at the same point. \\[ \\log \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\sum_{i = 1}^{n} \\log f(x_i; \\theta) \\] As an example, suppose that the data vector x_data contains observations from a random sample \\(X_1, X_2, \\ldots, X_n\\) that is assumed to be sampled from a Poisson distribution with (unknown) parameter \\(\\lambda\\). set.seed(42) x_data = rpois(n = 25, lambda = 6) # generating data (assume this is not known) head(x_data) # check data ## [1] 9 10 5 8 7 6 We can use R to calculate the likelihood for various possible values of \\(\\lambda\\) given this data. # calculate the likelihood when lambda = 5 prod(dpois(x = x_data, lambda = 5)) ## [1] 2.609375e-30 The above code takes advantage of the vectorized nature of the dpois() function. Often, especially for computational reasons, we prefer to directly obtain the log-likelihood. # calculate the log-likelihood when lambda = 5 sum(log(dpois(x = x_data, lambda = 5))) ## [1] -68.11844 To understand why this is necessary, repeat the above, but with a much larger sample size. Also note that the d*() functions in R have an option to return logged values. # calculate the log-likelihood when lambda = 5 sum(dpois(x = x_data, lambda = 5, log = TRUE)) ## [1] -68.11844 4.8 References Any of the following are either dedicated to, or contain a good coverage of the details of the topics above. Probability Texts Introduction to Probability by Dimitri P. Bertsekas and John N. Tsitsiklis A First Course in Probability by Sheldon Ross Machine Learning Texts with Probability Focus Probability for Statistics and Machine Learning by Anirban DasGupta Machine Learning: A Probabilistic Perspective by Kevin P. Murphy Statistics Texts with Introduction to Probability Probability and Statistical Inference by Robert V. Hogg, Elliot Tanis, and Dale Zimmerman Introduction to Mathematical Statistics by Robert V. Hogg, Joseph McKean, and Allen T. Craig 4.8.1 Videos The YouTube channel mathematicalmonk has a great Probability Primer playlist containing lectures on many fundamental probability concepts. Some of the more important concepts are covered in the following videos: Conditional Probability Independence More Independence Bayes Rule 4.9 Source R Markdown: 04-probability.Rmd "],
["statistics.html", "Chapter 5 Statistics 5.1 Reading 5.2 Statistics 5.3 Estimators 5.4 Source", " Chapter 5 Statistics STAT 432 is a course about statistics, in particular, some specific statistics. To discuss the statistics of interest in STAT 432, we will need some general concepts about statistics. Note: This section has been published while being nearly empty to provide easy access to a few definitions needed for Quiz 01. Additional information was added ahead of Quiz 02, but it is still very sparse as it is difficult to summarize all of statistics in one chapter. In reality we just need to state a few definitions here and then move on to the next chapter, where the fun begins. 5.1 Reading Reference: STAT 400 @ UIUC: Notes and Homework Reference: STAT 3202 @ OSU: Fitting a Probability Model Reference: STAT 415 @ PSU: Notes 5.2 Statistics In short: a statistic is a function of (sample) data. (This mirrors parameters being functions of (population) distributions. In SAT terminology, statistics : data :: parameter : distribution.) Consider a random variable \\(X\\), with PDF \\(f(x)\\) which defines the distribution of \\(X\\). Now consider the parameter \\(\\mu\\), which we usually refer to as the mean of a distribution. We use \\(\\mu_X\\) to note that \\(\\mu\\) is dependent on the distribution of \\(X\\). \\[ \\mu_X = \\text{E}[X] = \\int_{-\\infty}^{\\infty}xf(x)dx \\] Note that this expression is a function of \\(f(x)\\). When we change the distribution of \\(X\\), that is, it has a different \\(f(x)\\), that effects \\(\\mu\\). Now, given a random sample \\(X_1, X_2, \\ldots, X_n\\), define a statistic, \\[ \\hat{\\mu}(x_1, x_2, \\ldots, x_n) = \\frac{1}{n}\\sum_{i = 1}^{n}x_i \\] Often, we will simplify notation and instead simply write \\[ \\hat{\\mu} = \\frac{1}{n}\\sum_{i = 1}^{n}x_i \\] and the fact that \\(\\hat{\\mu}\\) is a function of the sample is implied. (You might also notice that this is the sample mean, which is often denoted by \\(\\bar{x}\\).) Another confusing aspect of statistics is that they are random variables! Sometimes we would write the above as \\[ \\hat{\\mu}(X_1, X_2, \\ldots, X_n) = \\frac{1}{n}\\sum_{i = 1}^{n}X_i \\] When written this way, we are emphasizing that the random sample has not yet be observed, thus is still random. When this is the case, we can investigate the properties of the statistic as a random variable. When the sample has been observed, we use \\(x_1, x_2, \\ldots, x_n\\) to note that we are inputting these observed values into a function, which outputs some value. (Sometimes we, and others, will be notationally sloppy and simply use lower case \\(x\\) and you will be expected to understand via context if we are dealing with random variables or observed values of random variables. This is admittedly confusing.) As a final note, suppose we observe some data \\[ x_1 = 2, x_2 = 1, x_3 =5 \\] and we calculate \\(\\hat{\\mu}\\) given these values. We would obtain \\[ \\hat{\\mu} = \\frac{8}{3} \\approx 2.66 \\] Note that 2.66 is not a statistic. It is the value of a statistic given a particular set of data. The statistic is still \\(\\hat{\\mu}\\) which has output the value 2.66. Statistics output values given some data. 5.3 Estimators Estimators are just statistics with a purpose, that is, estimators are statistics that attempt to estimate some quantity of interest, usually some parameter. Like statistics, estimators are functions of data that output values, which we call estimates. 5.3.1 Properties Bias and Variance Visually Illustrated Because they are just statistics, estimators are simply functions of data. What makes an estimator good? Essentially, an estimator is good if it produces estimates that are close to the thing being estimated. The following properties help to better define this “closeness” as a function of the errors made by estimators. To estimate some parameter \\(\\theta\\) we will consider some estimator \\(\\hat{\\theta}\\). 5.3.1.1 Bias The bias of an estimator defines the systematic error of the estimator, that is, how the estimator “misses” on average. \\[ \\text{bias}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta \\] 5.3.1.2 Variance The variance of an estimator defines how close resulting estimates are to each other. (Assuming the estimated was repeated.) \\[ \\text{var}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[ \\left( \\hat{\\theta} - \\mathbb{E}\\left[\\hat{\\theta}\\right] \\right)^2 \\right] \\] 5.3.1.3 Mean Squared Error The mean squared error (MSE) is exactly what the name suggests, it is the average squared error of the estimator. Interestingly, the MSE decomposes into terms related to the bias and the variance. We will return to this idea later for a detailed discussion in the context of machine learning. \\[ \\text{MSE}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[\\left(\\hat{\\theta} - \\theta\\right)^2\\right] = \\left(\\text{bias}\\left[\\hat{\\theta}\\right]\\right)^2 + \\text{var}\\left[\\hat{\\theta}\\right] \\] 5.3.1.4 Consistency An estimator \\(\\hat{\\theta}_n\\) is said to be a consistent estimator of \\(\\theta\\) if, for any positive \\(\\epsilon\\), \\[ \\lim_{n \\rightarrow \\infty} P\\left( \\left| \\hat{\\theta}_n - \\theta \\right| \\leq \\epsilon\\right) =1 \\] or, equivalently, \\[ \\lim_{n \\rightarrow \\infty} P\\left( \\left| \\hat{\\theta}_n - \\theta \\right| &gt; \\epsilon\\right) =0 \\] We say that \\(\\hat{\\theta}_n\\) converges in probability to \\(\\theta\\) and we write \\(\\hat{\\theta}_n \\overset P \\rightarrow \\theta\\). 5.3.2 Example: MSE of an Estimator Consider \\(X_1, X_2, X_3 \\sim N(\\mu, \\sigma^2)\\). Define two estimators for the true mean, \\(\\mu\\). \\[ \\bar{X} = \\frac{1}{n}\\sum_{i = 1}^{3} X_i \\] \\[ \\hat{\\mu} = \\frac{1}{4}X_1 + \\frac{1}{5}X_2 + \\frac{1}{6}X_3 \\] We will now calculate and compare the mean squared error of both \\(\\bar{X}\\) and \\(\\hat{\\mu}\\) as estimators of \\(\\mu\\). First, recall from properties of the sample mean that \\[ \\text{E}\\left[\\bar{X}\\right] = \\mu \\] and \\[ \\text{var}\\left[\\bar{X}\\right] = \\frac{\\sigma^2}{3} \\] Thus we have \\[ \\text{bias}\\left[\\bar{X}\\right] = \\mathbb{E}\\left[\\bar{X}\\right] - \\mu = \\mu - \\mu = 0 \\] Then, \\[ \\text{MSE}\\left[\\bar{X}\\right] \\triangleq \\left(\\text{bias}\\left[\\bar{X}\\right]\\right)^2 + \\text{var}\\left[\\bar{X}\\right] = 0 + \\frac{\\sigma^2}{3} = \\frac{\\sigma^2}{3} \\] Next, \\[ \\text{E}\\left[\\hat{\\mu}\\right] = \\frac{\\mu}{4} + \\frac{\\mu}{5} + \\frac{\\mu}{6} = \\frac{37}{60}\\mu \\] and \\[ \\text{var}\\left[\\hat{\\mu}\\right] = \\frac{\\sigma^2}{16} + \\frac{\\sigma^2}{25} + \\frac{\\sigma^2}{36} = \\frac{469}{3600}\\sigma^2 \\] Now we have \\[ \\text{bias}\\left[\\hat{\\mu}\\right] = \\mathbb{E}\\left[\\hat{\\mu}\\right] - \\mu = \\frac{37}{60}\\mu - \\mu = \\frac{-23}{60}\\mu \\] Then finally we obtain the mean squared error for \\(\\hat{\\mu}\\), \\[ \\text{MSE}\\left[\\hat{\\mu}\\right] \\triangleq \\left(\\text{bias}\\left[\\hat{\\mu}\\right]\\right)^2 + \\text{var}\\left[\\hat{\\mu}\\right] = \\left( \\frac{-23}{60}\\mu \\right)^2 + \\frac{469}{3600}\\sigma^2 \\] Note that \\(\\text{MSE}\\left[\\hat{\\mu}\\right]\\) is small when \\(\\mu\\) is close to 0. 5.3.3 Estimation Methods So far we have discussed properties of estimators, but how do we create estimators? You could just define a bunch of estimators and then evaluate them to see what works best (an idea we will return to later in the context of ML) but (the field of) statistics has develop some methods that result in estimators with desirable properties. 5.3.4 Maximum Likelihood Estimation Given a random sample \\(X_1, X_2, \\ldots, X_n\\) from a population with parameter \\(\\theta\\) and density or mass \\(f(x; \\theta)\\), we define the likelihood as \\[ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) \\triangleq f(x_1, x_2, \\ldots, x_n; \\theta) = \\prod_{i = 1}^n f(x_i; \\theta) \\] The Maximum Likelihood Estimator, \\(\\hat{\\theta}\\) \\[ \\hat{\\theta} \\triangleq \\underset{\\theta}{\\text{argmax}} \\ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\underset{\\theta}{\\text{argmax}} \\ \\log \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) \\] 5.3.4.1 Invariance Principle If \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\) and the function \\(h(\\theta)\\) is continuous, then \\(h(\\hat{\\theta})\\) is the MLE of \\(h(\\theta)\\). 5.3.5 Method of Moments While it is very unlikely that we will use the Method of Moments in STAT 432, you should still be aware of its existence. 5.3.6 Empirical Distribution Function Consider a random variable \\(X\\) with CDF \\(F(k) = P(X &lt; k)\\) and an iid random sample \\(X_1, X_2, \\ldots, X_n\\). We can estimate \\(F(k)\\) using the Empirical Distribution Function (EDF), \\[ \\hat{F}(k) = \\frac{\\text{# of elements in sample} \\leq k}{n} = \\frac{1}{n} \\sum_{i = 1}^n I(x_i \\leq k) \\] where \\(I(x_i \\leq k)\\) is an indicator such that \\[ I(x_i \\leq k) = \\begin{cases} 1 &amp; \\text{if } x_i \\leq k \\\\ 0 &amp; \\text{if } x_i &gt; k \\end{cases} \\] Given a data vector in R that is assumed to be a random sample, say, y, and some value, say k, it is easy to calculate \\(\\hat{F}(k)\\). set.seed(66) y = rnorm(n = 25, mean = 6, sd = 2.6) # generate sample k = 4 # pick some k head(y) # check data ## [1] 12.042334 6.564140 7.087301 5.503419 5.181420 4.315569 # using the EDF mean(y &lt; k) ## [1] 0.2 # using an estimated normal distribution (not quite using the MLE) pnorm(q = k, mean = mean(y), sd = sd(y)) ## [1] 0.2088465 # using the true (but assumed unknown) CDF pnorm(q = k, mean = 6, sd = 2.6) ## [1] 0.2208782 Note that technically sd(x) does not return the MLE of \\(\\sigma\\) since it uses the unbiased estimator with a denominator of \\(n - 1\\) instead of \\(n\\), but we’re being lazy for the sake of some cleaner code. plot(ecdf(y), col.01line = &quot;white&quot;, verticals = TRUE, do.points = FALSE, xlim = c(0, 15), lwd = 2, lty = 1, ylab = &quot;F(y)&quot;, xlab = &quot;y&quot;, main = &quot;Comparing the EDF to The Truth and MLE&quot;) curve(pnorm(x, mean = 6, sd = 2.6), add = TRUE, xlim = c(0, 15), col = &quot;dodgerblue&quot;, lty = 2, lwd = 2) curve(pnorm(x, mean = mean(y), sd = sd(y)), add = TRUE, xlim = c(0, 15), col = &quot;darkorange&quot;, lty = 3, lwd = 2) legend(&quot;bottomright&quot;, legend = c(&quot;EDF&quot;, &quot;Truth&quot;, &quot;MLE&quot;), col = c(&quot;black&quot;, &quot;dodgerblue&quot;, &quot;darkorange&quot;), lty = 1:3, lwd = 2) grid() We have purposefully used a “small” sample size here so that the EDF is visibly a step function. Modify the code above to increase the sample size. You should notice that the three functions converge as the sample size increases. 5.4 Source R Markdown: 05-statistics.Rmd "],
["the-backlog.html", "A The Backlog A.1 Sorted A.2 Unsorted A.3 Source", " A The Backlog The backlog will contain a list of topics of discussion that have arisen during the semester that we will return to if there is time. A.1 Sorted Why are we so darn focused on means? https://www.benkuhn.net/squared https://news.ycombinator.com/item?id=9556459 A.2 Unsorted https://atrebas.github.io/post/2019-01-15-2018-learning/ Don’t load previous worksapce. (Screenshot of this option in RStudio.) A.3 Source R Markdown: 98-backlog.Rmd "]
]
