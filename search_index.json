[
["index.html", "Basics of Statistical Learning Preface Caveat Emptor Who? Organization Acknowledgements License", " Basics of Statistical Learning David Dalpiaz 2019-11-10 Preface Welcome to Basics of Statistical Learning! What a boring title! The title was chosen to mirror the University of Illinois course STAT 432 - Basics of Statistical Learning. That title was chosen to meet certain University course naming conventions, hence the boring title. A more appropriate title would be “Machine Learning from the Perspective of a Statistician who uses R,” which is more descriptive, but still a boring title. Anyway, this book will often be referred to as BSL.1 Caveat Emptor This “book” is under active development. Literally every element of the book is subject to change, at any moment. This text, BSL, is the successor to R4SL, an unfinished work that began as a supplement to Introduction to Statistical Learning, but was never finished. (In some sense, this book is just a fresh start due to the author wanting to change the presentation of the material. The author is seriously worried that he will encounter the second-system effect.) Because this book is written with a course in mind, that is actively being taught, sometimes out of convenience, the text will speak directly to the students of that course. Thus, be aware that any reference to a “course” are a reference to STAT 432 @ UIUC. A PDF version is maintained for use offline, however, given the pace of development, this should only be used if absolutely necessary. During development formatting in the PDF version will largely be ignored. Since this book is under active development you may encounter errors ranging from typos, to broken code, to poorly explained topics. If you do, please let us know! Better yet, fix the issue yourself! If you are familiar with R Markdown and GitHub, pull requests are highly encouraged!. This process is partially automated by the edit button in the top-left corner of the html version. If your suggestion or fix becomes part of the book, you will be added to the list at the end of this chapter. We’ll also link to your GitHub account, or personal website upon request. If you’re not familiar with version control systems feel free to email the author, dalpiaz2 AT illinois DOT edu. (But also consider using this opportunity to learn a bit about version control!) See additional details in the Acknowledgements section. While development is taking place, you may see “TODO” scattered throughout the text. These are mostly notes for internal use, but give the reader some idea of what development is still to come. For additional details on the development process, please see the README file on GitHub as well as the Issues page. Who? This book is targeted at advanced undergraduate or first year MS students in Statistics who have no prior machine learning experience. While both will be discussed in great detail, previous experience with both statistical modeling and R are assumed. Organization Note: This is somewhat speculative. 01 - A Machine Learning Preview Introduction Regression (Powerlifting) Classification (Handwriting) Clustering (NBA Players) 02 - Some Machine Learning Foundations Introduction Probability (A Quick Tour) Statistics and Estimation (A Quick Tour) Density Estimation (?) 03 - A Tour of Machine Learning Introduction Data Splitting Generalization to Unseen Data Regression Bias, Variane, Loss, Risk Classification Resampling Recap and Overview of Supervised Learning Regularization Ensemble Learning Practical Issues Clustering (Unsupervised Learning) Recap the unsupervised learning that was done throughout. Introduce clustering specifically. 04 - Mathematics (Miscellaneous chapters further exploring mathematical details) Introduction Bayes Theorem Multivariate Normal 05 - Computing (Miscellaneous chapters further exploring computing details) Introduction Getting up to speed with R (Currently “Computing” in the main narrative.) purrr::map() Simulation? Bootstrap? 06 - Analysis (Examples of using the material with “real world” datasets) 07 - Appendix (Additional Readings and References) Misc papers, blogs, tutorials, etc Misc videos Acknowledgements The following is a (likely incomplete) list of helpful contributors. This book was also influenced by the helpful contributors to R4SL. Jae-Ho Lee - STAT 432, Fall 2019 Your name could be here! Please see the CONTRIBUTING document on GitHub for details on interacting with this project. Pull requests encouraged! Looking for ways to contribute? You’ll notice that a lot of the plotting code is not displayed in the text, but is available in the source. Currently that code was written to accomplish a task, but without much thought about the best way to accomplish the task. Try refactoring some of this code. Fix typos. Since the book is actively being developed, typos are getting added all the time. Suggest edits. Good feedback can be just as helpful as actually contributing code changes. TODO: Standing on the shoulder of giants. High level acknowledgements. License CC NC SA This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License Just an example footnote, please ignore.↩ "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction TODO: The Pokemon introduction? Regression Classification Clustering "],
["regression-powerlifting.html", "Chapter 2 Regression: Powerlifting 2.1 Background 2.2 Data 2.3 EDA 2.4 Modeling 2.5 Model Evaluation 2.6 Discussion", " Chapter 2 Regression: Powerlifting library(readr) library(tibble) library(dplyr) library(purrr) library(ggplot2) library(ggridges) library(lubridate) library(randomForest) library(rpart) library(rpart.plot) library(cluster) library(caret) library(factoextra) library(rsample) library(janitor) library(rvest) library(dendextend) library(knitr) library(kableExtra) library(ggthemes) TODO: Show package messaging? check conflicts! TODO: Should this be split into three analyses with different packages? 2.1 Background TODO: https://www.openpowerlifting.org/ TODO: https://en.wikipedia.org/wiki/Powerlifting 2.2 Data TODO: Why readr::col_factor() and not just col_factor()? TODO: Characters should be character and “categories” should be factors. TODO: Is na.omit() actually a good idea? pl = read_csv(&quot;data/pl.csv&quot;, col_types = cols(Sex = readr::col_factor())) pl ## # A tibble: 3,604 x 8 ## Name Sex Bodyweight Age Squat Bench Deadlift Total ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ariel Stier F 60 32 128. 72.5 150 350 ## 2 Nicole Bueno F 60 26 110 60 135 305 ## 3 Lisa Peterson F 67.5 28 118. 67.5 138. 322. ## 4 Shelby Bandula F 67.5 26 92.5 67.5 140 300 ## 5 Lisa Lindhorst F 67.5 28 92.5 62.5 132. 288. ## 6 Laura Burnett F 67.5 30 90 45 108. 242. ## 7 Suzette Bradley F 75 38 125 75 158. 358. ## 8 Norma Romero F 75 20 92.5 42.5 125 260 ## 9 Georgia Andrews F 82.5 29 108. 52.5 120 280 ## 10 Christal Bundang F 90 30 100 55 125 280 ## # … with 3,594 more rows 2.3 EDA set.seed(1) # test-train split pl_tst_trn_split = initial_split(pl, prop = 0.80) pl_trn = training(pl_tst_trn_split) pl_tst = testing(pl_tst_trn_split) # estimation-validation split pl_est_val_split = initial_split(pl_trn, prop = 0.80) pl_est = training(pl_est_val_split) pl_val = testing(pl_est_val_split) rm(pl) TODO: Train can be used however you want. (Including EDA.) TODO: Test can only be used after all model decisions have been made! pl_trn_tidy = gather(pl_trn, key = &quot;Lift&quot;, value = &quot;Weight&quot;, Squat, Bench, Deadlift) pl_trn_tidy$Lift = factor(pl_trn_tidy$Lift, levels = c(&quot;Squat&quot;, &quot;Bench&quot;, &quot;Deadlift&quot;)) TODO: https://www.tidyverse.org/ TODO: https://en.wikipedia.org/wiki/Tidy_data TODO: http://vita.had.co.nz/papers/tidy-data.pdf 2.4 Modeling dl_mod_form = formula(Deadlift ~ Sex + Bodyweight + Age + Squat + Bench) set.seed(1) lm_mod = lm(dl_mod_form, data = pl_est) knn_mod = caret::knnreg(dl_mod_form, data = pl_est) rf_mod = randomForest(dl_mod_form, data = pl_est) rp_mod = rpart(dl_mod_form, data = pl_est) TODO: Note: we are not using Name. Why? We are not using Total. Why? TODO: look what happens with Total! You’ll see it with lm(), you’ll be optimistic with randomForest(). TODO: What variables are allowed? (With respect to real world problem.) TODO: What variables lead to the best predictions? 2.5 Model Evaluation calc_rmse = function(actual, predicted) { sqrt(mean( (actual - predicted) ^ 2) ) } c(calc_rmse(actual = pl_val$Deadlift, predicted = predict(lm_mod, pl_val)), calc_rmse(actual = pl_val$Deadlift, predicted = predict(knn_mod, pl_val)), calc_rmse(actual = pl_val$Deadlift, predicted = predict(rp_mod, pl_val)), calc_rmse(actual = pl_val$Deadlift, predicted = predict(rf_mod, pl_val))) ## [1] 18.26654 19.19625 21.68142 19.23643 reg_preds = map(list(lm_mod, knn_mod, rp_mod, rf_mod), predict, pl_val) map_dbl(reg_preds, calc_rmse, actual = pl_val$Deadlift) ## [1] 18.26654 19.19625 21.68142 19.23643 TODO: Never supply data = df to predict(). You have been warned. knitr::include_graphics(&quot;img/sim-city.jpg&quot;) calc_mae = function(actual, predicted) { mean(abs(actual - predicted)) } map_dbl(reg_preds, calc_mae, actual = pl_val$Deadlift) ## [1] 14.38953 14.99748 17.14823 15.28626 reg_results = tibble( Model = c(&quot;Linear&quot;, &quot;KNN&quot;, &quot;Tree&quot;, &quot;Forest&quot;), RMSE = map_dbl(reg_preds, calc_rmse, actual = pl_val$Deadlift), MAE = map_dbl(reg_preds, calc_mae, actual = pl_val$Deadlift)) Model RMSE MAE Linear 18.26654 14.38953 KNN 19.19625 14.99748 Tree 21.68142 17.14823 Forest 19.23643 15.28626 2.6 Discussion lm_mod_final = lm(dl_mod_form, data = pl_trn) calc_rmse(actual = pl_tst$Deadlift, predicted = predict(lm_mod_final, pl_tst)) ## [1] 22.29668 TODO: Is this a good model? TODO: Is this model useful? william_biscarri = tibble( Name = &quot;William Biscarri&quot;, Age = 28, Sex = &quot;M&quot;, Bodyweight = 83, Squat = 130, Bench = 90 ) predict(lm_mod_final, william_biscarri) ## 1 ## 175.495 "],
["classification-handwriting.html", "Chapter 3 Classification: Handwriting 3.1 Background 3.2 Data 3.3 EDA 3.4 Modeling 3.5 Model Evaluation 3.6 Discussion", " Chapter 3 Classification: Handwriting library(readr) library(tibble) library(dplyr) library(purrr) library(ggplot2) library(ggridges) library(lubridate) library(randomForest) library(rpart) library(rpart.plot) library(cluster) library(caret) library(factoextra) library(rsample) library(janitor) library(rvest) library(dendextend) library(knitr) library(kableExtra) library(ggthemes) TODO: Show package messaging? check conflicts! TODO: Should this be split into three analyses with different packages? 3.1 Background TODO: https://en.wikipedia.org/wiki/MNIST_database TODO: http://yann.lecun.com/exdb/mnist/ 3.2 Data TODO: How is this data pre-processed? TODO: https://gist.github.com/daviddalpiaz/ae62ae5ccd0bada4b9acd6dbc9008706 TODO: https://github.com/itsrainingdata/mnistR TODO: https://pjreddie.com/projects/mnist-in-csv/ TODO: http://varianceexplained.org/r/digit-eda/ mnist_trn = read_csv(file = &quot;data/mnist_train_subest.csv&quot;) mnist_tst = read_csv(file = &quot;data/mnist_test.csv&quot;) mnist_trn_y = as.factor(mnist_trn$X1) mnist_tst_y = as.factor(mnist_tst$X1) mnist_trn_x = mnist_trn[, -1] mnist_tst_x = mnist_tst[, -1] TODO: If we were going to tune a model, we would need a validation split as well. We’re going to be lazy and just fit a single random forest. TODO: This is an agreed upon split. 3.3 EDA pixel_positions = expand.grid(j = sprintf(&quot;%02.0f&quot;, 1:28), i = sprintf(&quot;%02.0f&quot;, 1:28)) pixel_names = paste(&quot;pixel&quot;, pixel_positions$i, pixel_positions$j, sep = &quot;-&quot;) colnames(mnist_trn_x) = pixel_names colnames(mnist_tst_x) = pixel_names show_digit = function(arr784, col = gray(12:1 / 12), ...) { image(matrix(as.matrix(arr784), nrow = 28)[, 28:1], col = col, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, ...) grid(nx = 28, ny = 28) } 3.4 Modeling set.seed(42) mnist_rf = randomForest(x = mnist_trn_x, y = mnist_trn_y, ntree = 100) 3.5 Model Evaluation mnist_tst_pred = predict(mnist_rf, mnist_tst_x) mean(mnist_tst_pred == mnist_tst_y) ## [1] 0.8839 table(predicted = mnist_tst_pred, actual = mnist_tst_y) ## actual ## predicted 0 1 2 3 4 5 6 7 8 9 ## 0 959 0 14 6 1 15 22 1 10 10 ## 1 0 1112 5 5 1 16 5 9 5 6 ## 2 1 2 928 31 3 5 19 24 17 8 ## 3 0 2 11 820 1 24 0 1 13 13 ## 4 4 0 13 1 839 21 39 11 18 40 ## 5 3 1 1 88 3 720 18 1 25 9 ## 6 7 2 15 3 25 15 848 0 18 2 ## 7 2 1 29 24 1 14 2 928 15 30 ## 8 4 14 13 22 5 19 5 4 797 3 ## 9 0 1 3 10 103 43 0 49 56 888 3.6 Discussion par(mfrow = c(3, 3)) plot_mistake(actual = 6, predicted = 4) mnist_obs_to_check = 2 predict(mnist_rf, mnist_tst_x[mnist_obs_to_check, ], type = &quot;prob&quot;)[1, ] ## 0 1 2 3 4 5 6 7 8 9 ## 0.09 0.03 0.25 0.14 0.02 0.14 0.25 0.01 0.05 0.02 mnist_tst_y[mnist_obs_to_check] ## [1] 2 ## Levels: 0 1 2 3 4 5 6 7 8 9 show_digit(mnist_tst_x[mnist_obs_to_check, ]) "],
["clustering-basketball-players.html", "Chapter 4 Clustering: Basketball Players 4.1 Background 4.2 Data 4.3 EDA 4.4 Modeling 4.5 Model Evaluation 4.6 Discussion", " Chapter 4 Clustering: Basketball Players library(readr) library(tibble) library(dplyr) library(purrr) library(ggplot2) library(ggridges) library(lubridate) library(randomForest) library(rpart) library(rpart.plot) library(cluster) library(caret) library(factoextra) library(rsample) library(janitor) library(rvest) library(dendextend) library(knitr) library(kableExtra) library(ggthemes) TODO: Show package messaging? check conflicts! TODO: Should this be split into three analyses with different packages? 4.1 Background https://www.youtube.com/watch?v=cuLprHh_BRg https://www.youtube.com/watch?v=1FBwSO_1Mb8 https://www.basketball-reference.com/leagues/NBA_2019.html inspiration here, and others: http://blog.schochastics.net/post/analyzing-nba-player-data-ii-clustering/ 4.2 Data https://www.basketball-reference.com/leagues/NBA_2019_totals.html https://www.basketball-reference.com/leagues/NBA_2019_per_minute.html https://www.basketball-reference.com/leagues/NBA_2019_per_poss.html https://www.basketball-reference.com/leagues/NBA_2019_advanced.html nba = scrape_nba_season_player_stats() nba$pos = factor(nba$pos, levels = c(&quot;PG&quot;, &quot;SG&quot;, &quot;SF&quot;, &quot;PF&quot;, &quot;C&quot;)) ## # A tibble: 100 x 93 ## player_team pos age tm g gs mp fg fga fg_percent ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Álex Abrin… SG 25 OKC 31 2 588 56 157 0.357 ## 2 Quincy Acy… PF 28 PHO 10 0 123 4 18 0.222 ## 3 Jaylen Ada… PG 22 ATL 34 1 428 38 110 0.345 ## 4 Steven Ada… C 25 OKC 80 80 2669 481 809 0.595 ## 5 Bam Adebay… C 21 MIA 82 28 1913 280 486 0.576 ## 6 Deng Adel … SF 21 CLE 19 3 194 11 36 0.306 ## 7 DeVaughn A… SG 25 DEN 7 0 22 3 10 0.3 ## 8 LaMarcus A… C 33 SAS 81 81 2687 684 1319 0.519 ## 9 Rawle Alki… SG 21 CHI 10 1 120 13 39 0.333 ## 10 Grayson Al… SG 23 UTA 38 2 416 67 178 0.376 ## # … with 90 more rows, and 83 more variables: x3p &lt;dbl&gt;, x3pa &lt;dbl&gt;, ## # x3p_percent &lt;dbl&gt;, x2p &lt;dbl&gt;, x2pa &lt;dbl&gt;, x2p_percent &lt;dbl&gt;, ## # e_fg_percent &lt;dbl&gt;, ft &lt;dbl&gt;, fta &lt;dbl&gt;, ft_percent &lt;dbl&gt;, orb &lt;dbl&gt;, ## # drb &lt;dbl&gt;, trb &lt;dbl&gt;, ast &lt;dbl&gt;, stl &lt;dbl&gt;, blk &lt;dbl&gt;, tov &lt;dbl&gt;, ## # pf &lt;dbl&gt;, pts &lt;dbl&gt;, fg_pm &lt;dbl&gt;, fga_pm &lt;dbl&gt;, fg_percent_pm &lt;dbl&gt;, ## # x3p_pm &lt;dbl&gt;, x3pa_pm &lt;dbl&gt;, x3p_percent_pm &lt;dbl&gt;, x2p_pm &lt;dbl&gt;, ## # x2pa_pm &lt;dbl&gt;, x2p_percent_pm &lt;dbl&gt;, ft_pm &lt;dbl&gt;, fta_pm &lt;dbl&gt;, ## # ft_percent_pm &lt;dbl&gt;, orb_pm &lt;dbl&gt;, drb_pm &lt;dbl&gt;, trb_pm &lt;dbl&gt;, ## # ast_pm &lt;dbl&gt;, stl_pm &lt;dbl&gt;, blk_pm &lt;dbl&gt;, tov_pm &lt;dbl&gt;, pf_pm &lt;dbl&gt;, ## # pts_pm &lt;dbl&gt;, fg_pp &lt;dbl&gt;, fga_pp &lt;dbl&gt;, fg_percent_pp &lt;dbl&gt;, ## # x3p_pp &lt;dbl&gt;, x3pa_pp &lt;dbl&gt;, x3p_percent_pp &lt;dbl&gt;, x2p_pp &lt;dbl&gt;, ## # x2pa_pp &lt;dbl&gt;, x2p_percent_pp &lt;dbl&gt;, ft_pp &lt;dbl&gt;, fta_pp &lt;dbl&gt;, ## # ft_percent_pp &lt;dbl&gt;, orb_pp &lt;dbl&gt;, drb_pp &lt;dbl&gt;, trb_pp &lt;dbl&gt;, ## # ast_pp &lt;dbl&gt;, stl_pp &lt;dbl&gt;, blk_pp &lt;dbl&gt;, tov_pp &lt;dbl&gt;, pf_pp &lt;dbl&gt;, ## # pts_pp &lt;dbl&gt;, o_rtg_pp &lt;dbl&gt;, d_rtg_pp &lt;dbl&gt;, per &lt;dbl&gt;, ## # ts_percent &lt;dbl&gt;, x3p_ar &lt;dbl&gt;, f_tr &lt;dbl&gt;, orb_percent &lt;dbl&gt;, ## # drb_percent &lt;dbl&gt;, trb_percent &lt;dbl&gt;, ast_percent &lt;dbl&gt;, ## # stl_percent &lt;dbl&gt;, blk_percent &lt;dbl&gt;, tov_percent &lt;dbl&gt;, ## # usg_percent &lt;dbl&gt;, ows &lt;dbl&gt;, dws &lt;dbl&gt;, ws &lt;dbl&gt;, ws_48 &lt;dbl&gt;, ## # obpm &lt;dbl&gt;, dbpm &lt;dbl&gt;, bpm &lt;dbl&gt;, vorp &lt;dbl&gt; 4.3 EDA nba_for_clustering = nba %&gt;% filter(mp &gt; 2000) %&gt;% column_to_rownames(&quot;player_team&quot;) %&gt;% select(-pos, -tm) 4.4 Modeling set.seed(42) # function to compute total within-cluster sum of square wss = function(k, data) { kmeans(x = data, centers = k, nstart = 10)$tot.withinss } # Compute and plot wss for k = 1 to k = 15 k_values = 1:15 # extract wss for 2-15 clusters wss_values = map_dbl(k_values, wss, data = nba_for_clustering) plot(k_values, wss_values, type = &quot;b&quot;, pch = 19, frame = TRUE, xlab = &quot;Number of clusters K&quot;, ylab = &quot;Total within-clusters sum of squares&quot;) grid() TODO: K-Means likes clusters of roughly equal size. TODO: http://varianceexplained.org/r/kmeans-free-lunch/ nba_hc = hclust(dist(nba_for_clustering)) nba_hc_clust = cutree(nba_hc, k = 5) table(nba_hc_clust) ## nba_hc_clust ## 1 2 3 4 5 ## 38 13 28 11 1 4.5 Model Evaluation 4.6 Discussion "],
["introduction-1.html", "Chapter 5 Introduction", " Chapter 5 Introduction TODO: This section could ask be called a “detour” "],
["probability.html", "Chapter 6 Probability 6.1 Probability Models 6.2 Probability Axioms 6.3 Probability Rules 6.4 Random Variables 6.5 Expectations 6.6 Likelihood 6.7 Videos 6.8 References", " Chapter 6 Probability TODO: Note! This is copy-pasted from R4SL. We give a very brief review of some necessary probability concepts. As the treatment is less than complete, a list of references is given at the end of the chapter. For example, we ignore the usual recap of basic set theory and omit proofs and examples. 6.1 Probability Models When discussing probability models, we speak of random experiments that produce one of a number of possible outcomes. A probability model that describes the uncertainty of an experiment consists of two elements: The sample space, often denoted as \\(\\Omega\\), which is a set that contains all possible outcomes. A probability function that assigns to an event \\(A\\) a nonnegative number, \\(P[A]\\), that represents how likely it is that event \\(A\\) occurs as a result of the experiment. We call \\(P[A]\\) the probability of event \\(A\\). An event \\(A\\) could be any subset of the sample space, not necessarily a single possible outcome. The probability law must follow a number of rules, which are the result of a set of axioms that we introduce now. 6.2 Probability Axioms Given a sample space \\(\\Omega\\) for a particular experiment, the probability function associated with the experiment must satisfy the following axioms. Nonnegativity: \\(P[A] \\geq 0\\) for any event \\(A \\subset \\Omega\\). Normalization: \\(P[\\Omega] = 1\\). That is, the probability of the entire space is 1. Additivity: For mutually exclusive events \\(E_1, E_2, \\ldots\\) \\[ P\\left[\\bigcup_{i = 1}^{\\infty} E_i\\right] = \\sum_{i = 1}^{\\infty} P[E_i] \\] Using these axioms, many additional probability rules can easily be derived. 6.3 Probability Rules Given an event \\(A\\), and its complement, \\(A^c\\), that is, the outcomes in \\(\\Omega\\) which are not in \\(A\\), we have the complement rule: \\[ P[A^c] = 1 - P[A] \\] In general, for two events \\(A\\) and \\(B\\), we have the addition rule: \\[ P[A \\cup B] = P[A] + P[B] - P[A \\cap B] \\] If \\(A\\) and \\(B\\) are also disjoint, then we have: \\[ P[A \\cup B] = P[A] + P[B] \\] If we have \\(n\\) mutually exclusive events, \\(E_1, E_2, \\ldots E_n\\), then we have: \\[ P\\left[\\textstyle\\bigcup_{i = 1}^{n} E_i\\right] = \\sum_{i = 1}^{n} P[E_i] \\] Often, we would like to understand the probability of an event \\(A\\), given some information about the outcome of event \\(B\\). In that case, we have the conditional probability rule provided \\(P[B] &gt; 0\\). \\[ P[A \\mid B] = \\frac{P[A \\cap B]}{P[B]} \\] Rearranging the conditional probability rule, we obtain the multiplication rule: \\[ P[A \\cap B] = P[B] \\cdot P[A \\mid B] \\cdot \\] For a number of events \\(E_1, E_2, \\ldots E_n\\), the multiplication rule can be expanded into the chain rule: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = P[E_1] \\cdot P[E_2 \\mid E_1] \\cdot P[E_3 \\mid E_1 \\cap E_2] \\cdots P\\left[E_n \\mid \\textstyle\\bigcap_{i = 1}^{n - 1} E_i\\right] \\] Define a partition of a sample space \\(\\Omega\\) to be a set of disjoint events \\(A_1, A_2, \\ldots, A_n\\) whose union is the sample space \\(\\Omega\\). That is \\[ A_i \\cap A_j = \\emptyset \\] for all \\(i \\neq j\\), and \\[ \\bigcup_{i = 1}^{n} A_i = \\Omega. \\] Now, let \\(A_1, A_2, \\ldots, A_n\\) form a partition of the sample space where \\(P[A_i] &gt; 0\\) for all \\(i\\). Then for any event \\(B\\) with \\(P[B] &gt; 0\\) we have Bayes’ Rule: \\[ P[A_i | B] = \\frac{P[A_i]P[B | A_i]}{P[B]} = \\frac{P[A_i]P[B | A_i]}{\\sum_{i = 1}^{n}P[A_i]P[B | A_i]} \\] The denominator of the latter equality is often called the law of total probability: \\[ P[B] = \\sum_{i = 1}^{n}P[A_i]P[B | A_i] \\] Two events \\(A\\) and \\(B\\) are said to be independent if they satisfy \\[ P[A \\cap B] = P[A] \\cdot P[B] \\] This becomes the new multiplication rule for independent events. A collection of events \\(E_1, E_2, \\ldots E_n\\) is said to be independent if \\[ P\\left[\\bigcap_{i \\in S} E_i \\right] = \\prod_{i \\in S}P[E_i] \\] for every subset \\(S\\) of \\(\\{1, 2, \\ldots n\\}\\). If this is the case, then the chain rule is greatly simplified to: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = \\prod_{i=1}^{n}P[E_i] \\] 6.4 Random Variables A random variable is simply a function which maps outcomes in the sample space to real numbers. 6.4.1 Distributions We often talk about the distribution of a random variable, which can be thought of as: \\[ \\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities} \\] This is not a strict mathematical definition, but is useful for conveying the idea. If the possible values of a random variables are discrete, it is called a discrete random variable. If the possible values of a random variables are continuous, it is called a continuous random variable. 6.4.2 Discrete Random Variables The distribution of a discrete random variable \\(X\\) is most often specified by a list of possible values and a probability mass function, \\(p(x)\\). The mass function directly gives probabilities, that is, \\[ p(x) = p_X(x) = P[X = x]. \\] Note we almost always drop the subscript from the more correct \\(p_X(x)\\) and simply refer to \\(p(x)\\). The relevant random variable is discerned from context The most common example of a discrete random variable is a binomial random variable. The mass function of a binomial random variable \\(X\\), is given by \\[ p(x | n, p) = {n \\choose x} p^x(1 - p)^{n - x}, \\ \\ \\ x = 0, 1, \\ldots, n, \\ n \\in \\mathbb{N}, \\ 0 &lt; p &lt; 1. \\] This line conveys a large amount of information. The function \\(p(x | n, p)\\) is the mass function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the parameters \\(n\\) and \\(p\\). Different values of these parameters specify different binomial distributions. \\(x = 0, 1, \\ldots, n\\) indicates the sample space, that is, the possible values of the random variable. \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\) specify the parameter spaces. These are the possible values of the parameters that give a valid binomial distribution. Often all of this information is simply encoded by writing \\[ X \\sim \\text{bin}(n, p). \\] 6.4.3 Continuous Random Variables The distribution of a continuous random variable \\(X\\) is most often specified by a set of possible values and a probability density function, \\(f(x)\\). (A cumulative density or moment generating function would also suffice.) The probability of the event \\(a &lt; X &lt; b\\) is calculated as \\[ P[a &lt; X &lt; b] = \\int_{a}^{b} f(x)dx. \\] Note that densities are not probabilities. The most common example of a continuous random variable is a normal random variable. The density of a normal random variable \\(X\\), is given by \\[ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot \\exp\\left[\\frac{-1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 \\right], \\ \\ \\ -\\infty &lt; x &lt; \\infty, \\ -\\infty &lt; \\mu &lt; \\infty, \\ \\sigma &gt; 0. \\] The function \\(f(x | \\mu, \\sigma^2)\\) is the density function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the paramters \\(\\mu\\) and \\(\\sigma^2\\). Different values of these parameters specify different normal distributions. \\(-\\infty &lt; x &lt; \\infty\\) indicates the sample space. In this case, the random variable may take any value on the real line. \\(-\\infty &lt; \\mu &lt; \\infty\\) and \\(\\sigma &gt; 0\\) specify the parameter space. These are the possible values of the parameters that give a valid normal distribution. Often all of this information is simply encoded by writing \\[ X \\sim N(\\mu, \\sigma^2) \\] 6.4.4 Several Random Variables Consider two random variables \\(X\\) and \\(Y\\). We say they are independent if \\[ f(x, y) = f(x) \\cdot f(y) \\] for all \\(x\\) and \\(y\\). Here \\(f(x, y)\\) is the joint density (mass) function of \\(X\\) and \\(Y\\). We call \\(f(x)\\) the marginal density (mass) function of \\(X\\). Then \\(f(y)\\) the marginal density (mass) function of \\(Y\\). The joint density (mass) function \\(f(x, y)\\) together with the possible \\((x, y)\\) values specify the joint distribution of \\(X\\) and \\(Y\\). Similar notions exist for more than two variables. 6.5 Expectations For discrete random variables, we define the expectation of the function of a random variable \\(X\\) as follows. \\[ \\mathbb{E}[g(X)] \\triangleq \\sum_{x} g(x)p(x) \\] For continuous random variables we have a similar definition. \\[ \\mathbb{E}[g(X)] \\triangleq \\int g(x)f(x) dx \\] For specific functions \\(g\\), expectations are given names. The mean of a random variable \\(X\\) is given by \\[ \\mu_{X} = \\text{mean}[X] \\triangleq \\mathbb{E}[X]. \\] So for a discrete random variable, we would have \\[ \\text{mean}[X] = \\sum_{x} x \\cdot p(x) \\] For a continuous random variable we would simply replace the sum by an integral. The variance of a random variable \\(X\\) is given by \\[ \\sigma^2_{X} = \\text{var}[X] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2. \\] The standard deviation of a random variable \\(X\\) is given by \\[ \\sigma_{X} = \\text{sd}[X] \\triangleq \\sqrt{\\sigma^2_{X}} = \\sqrt{\\text{var}[X]}. \\] The covariance of random variables \\(X\\) and \\(Y\\) is given by \\[ \\text{cov}[X, Y] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]. \\] 6.6 Likelihood Consider \\(n\\) iid random variables \\(X_1, X_2, \\ldots X_n\\). We can then write their likelihood as \\[ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\prod_{i = i}^n f(x_i; \\theta) \\] where \\(f(x_i; \\theta)\\) is the density (or mass) function of random variable \\(X_i\\) evaluated at \\(x_i\\) with parameter \\(\\theta\\). Whereas a probability is a function of a possible observed value given a particular parameter value, a likelihood is the opposite. It is a function of a possible parameter value given observed data. Maximumizing likelihood is a common techinque for fitting a model to data. 6.7 Videos The YouTube channel mathematicalmonk has a great Probability Primer playlist containing lectures on many fundamental probability concepts. Some of the more important concepts are covered in the following videos: Conditional Probability Independence More Independence Bayes Rule 6.8 References Any of the following are either dedicated to, or contain a good coverage of the details of the topics above. Probability Texts Introduction to Probability by Dimitri P. Bertsekas and John N. Tsitsiklis A First Course in Probability by Sheldon Ross Machine Learning Texts with Probability Focus Probability for Statistics and Machine Learning by Anirban DasGupta Machine Learning: A Probabilistic Perspective by Kevin P. Murphy Statistics Texts with Introduction to Probability Probability and Statistical Inference by Robert V. Hogg, Elliot Tanis, and Dale Zimmerman Introduction to Mathematical Statistics by Robert V. Hogg, Joseph McKean, and Allen T. Craig "],
["estimation.html", "Chapter 7 Estimation 7.1 Probability 7.2 Statistics 7.3 Estimators", " Chapter 7 Estimation TODO: Where we are going, estimating conditional means and distributions. TODO: estimation = learning. “learning from data.” what are we learning about? often parameters. TODO: http://stat400.org TODO: http://stat420.org 7.1 Probability TODO: See Appendix A TODO: In R, d*(), p*(), q*(), r*() 7.2 Statistics TODO: parameters are a function of the population distribution TODO: statistics are a function of data. TODO: parameters:population::statistics::data TODO: statistic vs value of a statistic 7.3 Estimators TODO: estimator vs estimate TODO: Why such a foucs on the mean, E[X]? Because E[(X - a)^2] is minimized by E[X] https://www.benkuhn.net/squared https://news.ycombinator.com/item?id=9556459 7.3.1 Properties 7.3.1.1 Bias \\[ \\text{bias}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta \\] 7.3.1.2 Variance \\[ \\text{var}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[ \\left( \\hat{\\theta} - \\mathbb{E}\\left[\\hat{\\theta}\\right] \\right)^2 \\right] \\] 7.3.1.3 Mean Squared Error \\[ \\text{MSE}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[\\left(\\hat{\\theta} - \\theta\\right)^2\\right] = \\text{var}\\left[\\hat{\\theta}\\right] + \\left(\\text{Bias}\\left[\\hat{\\theta}\\right]\\right)^2 \\] 7.3.1.4 Consistency An estimator \\(\\hat{\\theta}_n\\) is said to be a consistent estimator of \\(\\theta\\) if, for any positive \\(\\epsilon\\), \\[ \\lim_{n \\rightarrow \\infty} P\\left( \\left| \\hat{\\theta}_n - \\theta \\right| \\leq \\epsilon\\right) =1 \\] or, equivalently, \\[ \\lim_{n \\rightarrow \\infty} P\\left( \\left| \\hat{\\theta}_n - \\theta \\right| &gt; \\epsilon\\right) =0 \\] We say that \\(\\hat{\\theta}_n\\) converges in probability to \\(\\theta\\) and we write \\(\\hat{\\theta}_n \\overset P \\rightarrow \\theta\\). 7.3.2 Methods TODO: MLE Given a random sample \\(X_1, X_2, \\ldots, X_n\\) from a population with parameter \\(\\theta\\) and density or mass \\(f(x \\mid \\theta)\\), we have: The Likelihood, \\(L(\\theta)\\), \\[ L(\\theta) = f(x_1, x_2, \\ldots, x_n) = \\prod_{i = 1}^{n} f(x_i \\mid \\theta) \\] The Maximum Likelihood Estimator, \\(\\hat{\\theta}\\) \\[ \\hat{\\theta} = \\underset{\\theta}{\\text{argmax}} \\ L(\\theta) = \\underset{\\theta}{\\text{argmax}} \\ \\log L(\\theta) \\] TODO: Invariance Principle If \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\) and the function \\(h(\\theta)\\) is continuous, then \\(h(\\hat{\\theta})\\) is the MLE of \\(h(\\theta)\\). TODO: MOM TODO: https://daviddalpiaz.github.io/stat3202-sp19/notes/fitting.html TODO: ECDF: https://en.wikipedia.org/wiki/Empirical_distribution_function "],
["introduction-2.html", "Chapter 8 Introduction", " Chapter 8 Introduction TODO: Goal: Train models that generalize well, that is, perform well on unseen data. TODO: Introduce data splitting: Test-Train Split (_tst and _trn) Estimation-Validation Split (_est and _val) Where are these “weird” terms coming from? Why not cross-validation yet? TODO: http://varianceexplained.org/r/ds-ml-ai/s TODO: define most of the terms that will be seen here at least those that apply to both regression and classification TODO: This section is the heart of STAT 432. TODO: Note simplifications that we will use in this section. No cross-validation No data pre-processing Minimial care for categorical variables Either don’t use them, or let the methods take care of them. "],
["regression.html", "Chapter 9 Regression 9.1 Setup 9.2 Modeling 9.3 Procedure 9.4 Data Splitting 9.5 Metrics 9.6 Model Complexity 9.7 Overfitting 9.8 Multiple Features 9.9 Example Analysis 9.10 MISC TODOS", " Chapter 9 Regression BLUF: Use regression, which is one of the two supervised learning tasks (the other being classification) to make predictions of new observations of numeric response variables. Start by randomly splitting the data (which includes both the response and the features) into a test set and a training set. Do not use the test data for anything other than supplying a final assesment of how well a chosen model performs at the prediction task. That is, never use the test data to make any modeling decisions. Use the training data however you please, but it is recommended to further split this data into an estimation set and a validation set. The estimation set should be used to train models for evaluation. For example, use the estimation data to learn the model parameters of a parametric model. Do not use data used in training of models (the estimation data) when evaluating models as doing so will mask overfitting of complex (flexible) models. Use the lm() function to train linear models. Use the knnreg() function from the caret pacakge to train k-nearest neighbors models. Use the rpart() function from the rpart package to train decision tree models. Use the validation set to evaluate models that have been trained using the estimation data. For example, use the validation data to select the value of tuning parameters that are often used in non-parametric models. Use numeric metrics such as root-mean-square error (RMSE) or graphical summaries such as actual versus predicted plots. Although it ignores some practical and statistical considerations (which will be discussed later), the model that acheives the lowest RMSE on the validation data will be deemed the “best” model. After finding this model, refit the model to the entire training dataset. Report the RMSE of this model on the test data as a final quantification of performance. TODO: add ISL readings TODO: &lt;www.stat420.org&gt; TODO: add “why least squares?” readings library(&quot;tidyverse&quot;) library(&quot;caret&quot;) library(&quot;rpart&quot;) library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) 9.1 Setup \\[ Y = f(\\boldsymbol{X}) + \\epsilon \\] TODO: signal \\(f(X)\\) TODO: noise \\(\\epsilon\\) TODO: goal: learn the signal, not the noise TODO: random variables versus potential realized values \\[ \\boldsymbol{X} = (X_1, X_2, \\ldots, X_p) \\] \\[ \\boldsymbol{x} = (x_1, x_2, \\ldots, x_p) \\] \\[ \\mathbb{E}\\left[\\left(Y - f(\\boldsymbol{X})^2 \\right)\\right] \\] TODO: define regression function above is minimzed when f(x) = mu(x) \\[ \\mu(\\boldsymbol{x}) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\] TODO: want to learn these “things” which are regression functions line_reg_fun = function(x) { x } \\[ \\mu_{l}(x) = x \\] quad_reg_fun = function(x) { x ^ 2 } \\[ \\mu_{q}(x) = x^2 \\] sine_reg_fun = function(x) { sin(x) } \\[ \\mu_{s}(x) = \\sin(x) \\] gen_sim_data = function(f, sample_size = 50, sd = 1) { x = runif(n = sample_size, min = -5, max = 5) y = rnorm(n = sample_size, mean = f(x), sd = sd) tibble::tibble(x = x, y = y) } set.seed(5) line_data = gen_sim_data(f = line_reg_fun, sample_size = 50, sd = 1.0) quad_data = gen_sim_data(f = quad_reg_fun, sample_size = 50, sd = 2.0) sine_data = gen_sim_data(f = sine_reg_fun, sample_size = 50, sd = 0.5) set.seed(42) line_data_unseen = gen_sim_data(f = line_reg_fun, sample_size = 100000, sd = 1.0) quad_data_unseen = gen_sim_data(f = quad_reg_fun, sample_size = 100000, sd = 2.0) sine_data_unseen = gen_sim_data(f = sine_reg_fun, sample_size = 100000, sd = 0.5) 9.2 Modeling TODO: for now, only use formula syntax https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/ https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/ 9.2.1 Linear Models TODO: assume form of mean relationship. linear combination TODO: how to go from y = b0 + b1x1 + … + eps to lm(y ~ stuff) TODO: least squares, least squares is least squares (difference in assumptions) lm_line_int = lm(y ~ 1, data = line_data) lm_line_slr = lm(y ~ poly(x, degree = 1), data = line_data) lm_line_ply = lm(y ~ poly(x, degree = 20), data = line_data) 9.2.2 k-Nearest Neighbors TODO: caret::knnreg() TODO: for now, don’t worry about scaling, factors, etc. 9.2.2.1 Linear Data knn_line_25 = knnreg(y ~ x, data = line_data, k = 25) knn_line_05 = knnreg(y ~ x, data = line_data, k = 5) knn_line_01 = knnreg(y ~ x, data = line_data, k = 1) calc_dist = function(p1, p2) { sqrt(sum((p1 - p2) ^ 2)) } line_data %&gt;% mutate(dist = purrr::map_dbl(x, calc_dist, p2 = 0)) %&gt;% top_n(dist, n = -5) %&gt;% pull(y) %&gt;% mean() # also consider median ## [1] -0.1310472 predict(knn_line_05, data.frame(x = 0)) ## [1] -0.1310472 k Train RMSE Test RMSE 25 1.406 1.379 5 0.931 1.061 1 0.000 1.409 9.2.2.2 Quadratic Data knn_quad_25 = knnreg(y ~ x, data = quad_data, k = 25) knn_quad_05 = knnreg(y ~ x, data = quad_data, k = 5) knn_quad_01 = knnreg(y ~ x, data = quad_data, k = 1) k Train RMSE Test RMSE 25 6.393 6.564 5 2.236 2.509 1 0.000 3.067 9.2.2.3 Sinusoidal Data knn_sine_25 = knnreg(y ~ x, data = sine_data, k = 25) knn_sine_05 = knnreg(y ~ x, data = sine_data, k = 5) knn_sine_01 = knnreg(y ~ x, data = sine_data, k = 1) k Train RMSE Test RMSE 25 0.814 0.841 5 0.349 0.570 1 0.000 0.647 9.2.3 Decision Trees TODO: rpart::rpart() TODO: https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf TODO: http://www.milbo.org/doc/prp.pdf TODO: maybe notes about pruning and CV 9.2.3.1 Linear Data tree_line_010 = rpart(y ~ x, data = line_data, cp = 0.10, minsplit = 2) tree_line_001 = rpart(y ~ x, data = line_data, cp = 0.01, minsplit = 2) tree_line_000 = rpart(y ~ x, data = line_data, cp = 0.00, minsplit = 2) k Train RMSE Test RMSE 25 1.394 1.548 5 0.914 1.144 1 0.000 1.409 9.2.3.2 Quadratic Data tree_quad_010 = rpart(y ~ x, data = quad_data, cp = 0.10, minsplit = 2) tree_quad_001 = rpart(y ~ x, data = quad_data, cp = 0.01, minsplit = 2) tree_quad_000 = rpart(y ~ x, data = quad_data, cp = 0.00, minsplit = 2) k Train RMSE Test RMSE 25 3.376 3.869 5 1.692 2.621 1 0.000 3.067 9.2.3.3 Sinusoidal Data tree_sine_010 = rpart(y ~ x, data = sine_data, cp = 0.10, minsplit = 2) tree_sine_001 = rpart(y ~ x, data = sine_data, cp = 0.01, minsplit = 2) tree_sine_000 = rpart(y ~ x, data = sine_data, cp = 0.00, minsplit = 2) k Train RMSE Test RMSE 25 0.414 0.659 5 0.235 0.629 1 0.000 0.647 9.3 Procedure TODO: Look at data TODO: Pick candiate models TODO: Tune / train models TODO: Pick “best” model based on validation RMSE (note the issues with this) TODO: Use best model / report test metrics 9.4 Data Splitting TODO: want to generalize to unseen data TODO: for now, all variables should either be numeric, or factor TODO: Training (Train) Data TODO: Testing (Test) Data TODO: Estimation Data TODO: Validation Data https://en.wikipedia.org/wiki/Infinite_monkey_theorem \\[ \\mathcal{D} = \\{ (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i = 1, 2, \\ldots n \\} \\] \\[ \\mathcal{D} = \\mathcal{D}_{\\texttt{trn}} \\cup \\mathcal{D}_{\\texttt{tst}} \\] \\[ \\mathcal{D}_{\\texttt{trn}} = \\mathcal{D}_{\\texttt{est}} \\cup \\mathcal{D}_{\\texttt{val}} \\] 9.5 Metrics TODO: RMSE \\[ \\text{rmse}\\left(\\hat{f}_{\\texttt{set}}, \\mathcal{D}_{\\texttt{set}} \\right) = \\sqrt{\\frac{1}{n_{\\texttt{set}}}\\displaystyle\\sum_{i \\in {\\texttt{set}}}^{}\\left(y_i - \\hat{f}_{\\texttt{set}}({x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\texttt{trn}} = \\text{rmse}\\left(\\hat{f}_{\\texttt{est}}, \\mathcal{D}_{\\texttt{est}}\\right) = \\sqrt{\\frac{1}{n_{\\texttt{est}}}\\displaystyle\\sum_{i \\in {\\texttt{est}}}^{}\\left(y_i - \\hat{f}_{\\texttt{est}}({x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\texttt{val}} = \\text{rmse}\\left(\\hat{f}_{\\texttt{est}}, \\mathcal{D}_{\\texttt{val}}\\right) = \\sqrt{\\frac{1}{n_{\\texttt{val}}}\\displaystyle\\sum_{i \\in {\\texttt{val}}}^{}\\left(y_i - \\hat{f}_{\\texttt{est}}({x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\texttt{tst}} = \\text{rmse}\\left(\\hat{f}_{\\texttt{trn}}, \\mathcal{D}_{\\texttt{tst}}\\right) = \\sqrt{\\frac{1}{n_{\\texttt{tst}}}\\displaystyle\\sum_{i \\in {\\texttt{tst}}}^{}\\left(y_i - \\hat{f}_{\\texttt{trn}}({x}_i)\\right)^2} \\] TODO: MAE TODO: MAPE https://en.wikipedia.org/wiki/Mean_absolute_percentage_error but probably don’t use calc_rmse = function(model, data, response) { actual = data[[response]] predicted = predict(model, data) sqrt(mean((actual - predicted) ^ 2)) } 9.6 Model Complexity TODO: what determines the complexity of the above models? lm: terms, xforms, interactions knn: k (also terms, xforms, interactions) tree: cp (with rpart, also others that we’ll keep mostly hidden) (also terms, xforms, interactions) 9.7 Overfitting TODO: too complex TODO: usual picture with training and validation error TODO: define for the purposes of this course 9.8 Multiple Features TODO: more features = more complex TODO: how do the three models add additional features? 9.9 Example Analysis TODO: Diamonds analysis TODO: model.matrix() 9.10 MISC TODOS lex fridman with ian: dataset (represent), model, optimize https://www.youtube.com/watch?v=Z6rxFNMGdn0 want to minimize E[(y - y_hat)^2] predict() creates estimate of E[Y|X] with supplied model \\[ \\mathbb{E}\\left[\\left|Y - f(\\boldsymbol{X}) \\right|\\right] \\] \\[ m(\\boldsymbol{x}) = \\mathbb{M}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\] # define a data generating process gen = function() { x = runif(100) y = 2 * x + rnorm(100) tibble(x, y) } # generate and check data df = gen() # define midpoint calculation calc_midpoints = function(x) { x = sort(x) x[-length(x)] + diff(x) / 2 } # calculate midpoints mids = with(df, calc_midpoints(x)) # calculate mse for a proposed split calc_mse_split = function(df, cut) { left = dplyr::filter(df, x &lt; cut) right = dplyr::filter(df, x &gt; cut) mse_left = with(left, sum((y - mean(y)) ^ 2)) mse_right = with(right, sum((y - mean(y)) ^ 2)) mse_left + mse_right # also consider mae } # calculate mse for each possible split, find best which.min(purrr::map_dbl(mids, calc_mse_split, df = df)) ## [1] 55 "],
["biasvariance-tradeoff.html", "Chapter 10 Bias–Variance Tradeoff 10.1 Reducible and Irreducible Error 10.2 Bias-Variance Decomposition 10.3 Simulation 10.4 Estimating Expected Prediction Error 10.5 Reproducibility", " Chapter 10 Bias–Variance Tradeoff Consider the general regression setup where we are given a random pair \\((X, Y) \\in \\mathbb{R}^p \\times \\mathbb{R}\\). We would like to “predict” \\(Y\\) with some function of \\(X\\), say, \\(f(X)\\). To clarify what we mean by “predict,” we specify that we would like \\(f(X)\\) to be “close” to \\(Y\\). To further clarify what we mean by “close,” we define the squared error loss of estimating \\(Y\\) using \\(f(X)\\). \\[ L(Y, f(X)) \\triangleq (Y - f(X)) ^ 2 \\] Now we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the risk of estimating \\(Y\\) using \\(f(X)\\). \\[ R(Y, f(X)) \\triangleq \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}_{X, Y}[(Y - f(X)) ^ 2] \\] Before attempting to minimize the risk, we first re-write the risk after conditioning on \\(X\\). \\[ \\mathbb{E}_{X, Y} \\left[ (Y - f(X)) ^ 2 \\right] = \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X} \\left[ ( Y - f(X) ) ^ 2 \\mid X = x \\right] \\] Minimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to \\(Y \\mid X\\), essentially minimizing the risk pointwise, for each \\(x\\). It turns out, that the risk is minimized by the conditional mean of \\(Y\\) given \\(X\\), \\[ f(x) = \\mathbb{E}(Y \\mid X = x) \\] which we call the regression function. Note that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss. \\[ L(Y, f(X)) \\triangleq | Y - f(X) | \\] The risk would then be minimized by the conditional median. \\[ f(x) = \\text{median}(Y \\mid X = x) \\] Despite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations. Now, given data \\(\\mathcal{D} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}\\), our goal becomes finding some \\(\\hat{f}\\) that is a good estimate of the regression function \\(f\\). We’ll see that this amounts to minimizing what we call the reducible error. 10.1 Reducible and Irreducible Error Suppose that we obtain some \\(\\hat{f}\\), how well does it estimate \\(f\\)? We define the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\). A good \\(\\hat{f}\\) will have a low expected prediction error. \\[ \\text{EPE}\\left(Y, \\hat{f}(X)\\right) \\triangleq \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ \\left( Y - \\hat{f}(X) \\right)^2 \\right] \\] This expectation is over \\(X\\), \\(Y\\), and also \\(\\mathcal{D}\\). The estimate \\(\\hat{f}\\) is actually random depending on the sampled data \\(\\mathcal{D}\\). We could actually write \\(\\hat{f}(X, \\mathcal{D})\\) to make this dependence explicit, but our notation will become cumbersome enough as it is. Like before, we’ll condition on \\(X\\). This results in the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\). \\[ \\text{EPE}\\left(Y, \\hat{f}(x)\\right) = \\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[ \\left(Y - \\hat{f}(X) \\right)^2 \\mid X = x \\right] = \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error} + \\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_\\textrm{irreducible error} \\] A number of things to note here: The expected prediction error is for a random \\(Y\\) given a fixed \\(x\\) and a random \\(\\hat{f}\\). As such, the expectation is over \\(Y \\mid X\\) and \\(\\mathcal{D}\\). Our estimated function \\(\\hat{f}\\) is random depending on the sampled data, \\(\\mathcal{D}\\), which is used to perform the estimation. The expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\) has been decomposed into two errors: The reducible error, which is the expected squared error loss of estimation \\(f(x)\\) using \\(\\hat{f}(x)\\) at a fixed point \\(x\\). The only thing that is random here is \\(\\mathcal{D}\\), the data used to obtain \\(\\hat{f}\\). (Both \\(f\\) and \\(x\\) are fixed.) We’ll often call this reducible error the mean squared error of estimating \\(f(x)\\) using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) \\triangleq \\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]\\] The irreducible error. This is simply the variance of \\(Y\\) given that \\(X = x\\), essentially noise that we do not want to learn. This is also called the Bayes error. As the name suggests, the reducible error is the error that we have some control over. But how do we control this error? 10.2 Bias-Variance Decomposition After decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error. Recall the definition of the bias of an estimator. \\[ \\text{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta \\] Also recall the definition of the variance of an estimator. \\[ \\mathbb{V}(\\hat{\\theta}) = \\text{var}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left [ ( \\hat{\\theta} -\\mathbb{E}\\left[\\hat{\\theta}\\right] )^2 \\right] \\] Using this, we further decompose the reducible error (mean squared error) into bias squared and variance. \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] = \\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} + \\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)} \\] This is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function \\(f\\) using \\(\\hat{f}\\) at some point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\text{bias}^2 \\left(\\hat{f}(x) \\right) + \\text{var} \\left(\\hat{f}(x) \\right) \\] In a perfect world, we would be able to find some \\(\\hat{f}\\) which is unbiased, that is \\(\\text{bias}\\left(\\hat{f}(x) \\right) = 0\\), which also has low variance. In practice, this isn’t always possible. It turns out, there is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Complex models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance. In the context of regression, models are biased when: Parametric: The form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic. Non-parametric: The model provides too much smoothing. In the context of regression, models are variable when: Parametric: The form of the model incorporates too many variables, or the form of the relationship is too complex. For example, a parametric model assumes a cubic relationship, but the true relationship is linear. Non-parametric: The model does not provide enough smoothing. It is very, “wiggly.” So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate complexity for the data. Recall that when fitting models, we’ve seen that train RMSE decreases as model complexity is increasing. (Technically it is non-increasing.) For test RMSE, we expect to see a U-shaped curve. Importantly, test RMSE decreases, until a certain complexity, then begins to increase. Now we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this. The three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the complexity that minimizes the prediction error. To summarize, if we assume that irreducible error can be written as \\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2 \\] then we can write the full decomposition of the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}\\) when \\(X = x\\) as \\[ \\text{EPE}\\left(Y, \\hat{f}(x)\\right) = \\underbrace{\\text{bias}^2\\left(\\hat{f}(x)\\right) + \\text{var}\\left(\\hat{f}(x)\\right)}_\\textrm{reducible error} + \\sigma^2. \\] As model complexity increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model complexity to find a model that well predict well on unseen observations. 10.3 Simulation We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function \\(f(x) = x^2\\). f = function(x) { x ^ 2 } More specifically, we’d like to predict an observation, \\(Y\\), given that \\(X = x\\) by using \\(\\hat{f}(x)\\) where \\[ \\mathbb{E}[Y \\mid X = x] = f(x) = x^2 \\] and \\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2. \\] Alternatively, we could write this as \\[ Y = f(X) + \\epsilon \\] where \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mathbb{V}[\\epsilon] = \\sigma ^ 2\\). In this formulation, we call \\(f(X)\\) the signal and \\(\\epsilon\\) the noise. To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following R code. gen_sim_data = function(f, sample_size = 100) { x = runif(n = sample_size, min = 0, max = 1) y = rnorm(n = sample_size, mean = f(x), sd = 0.3) data.frame(x, y) } Also note that if you prefer to think of this situation using the \\(Y = f(X) + \\epsilon\\) formulation, the following code represents the same data generating process. gen_sim_data = function(f, sample_size = 100) { x = runif(n = sample_size, min = 0, max = 1) eps = rnorm(n = sample_size, mean = 0, sd = 0.75) y = f(x) + eps data.frame(x, y) } To completely specify the data generating process, we have made more model assumptions than simply \\(\\mathbb{E}[Y \\mid X = x] = x^2\\) and \\(\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\\). In particular, The \\(x_i\\) in \\(\\mathcal{D}\\) are sampled from a uniform distribution over \\([0, 1]\\). The \\(x_i\\) and \\(\\epsilon\\) are independent. The \\(y_i\\) in \\(\\mathcal{D}\\) are sampled from the conditional normal distribution. \\[ Y \\mid X \\sim N(f(x), \\sigma^2) \\] Using this setup, we will generate datasets, \\(\\mathcal{D}\\), with a sample size \\(n = 100\\) and fit four models. \\[ \\begin{aligned} \\texttt{predict(fit0, x)} &amp;= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\ \\texttt{predict(fit1, x)} &amp;= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\ \\texttt{predict(fit2, x)} &amp;= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\ \\texttt{predict(fit9, x)} &amp;= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9 \\end{aligned} \\] To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models. set.seed(1) sim_data = gen_sim_data(f) fit_0 = lm(y ~ 1, data = sim_data) fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data) Note that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes. Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild. The following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each. This plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance. While the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance. We could have also fit \\(k\\)-nearest neighbors models to these three datasets. Here we see that when \\(k = 100\\) we have a biased model with very low variance. (It’s actually the same as the 0 predictor linear model.) When \\(k = 5\\), we again have a highly variable model. These two sets of plots reinforce our intuition about the bias-variance tradeoff. Complex models (ninth degree polynomial and \\(k\\) = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and \\(k = 100\\)) are very biased, but have extremely low variance. We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates for \\(f(x)\\) given by these four models at the point \\(x = 0.90\\). We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult. set.seed(1) n_sims = 250 n_models = 4 x = data.frame(x = 0.90) # fixed point at which we make predictions predictions = matrix(0, nrow = n_sims, ncol = n_models) for (sim in 1:n_sims) { # simulate new, random, training data # this is the only random portion of the bias, var, and mse calculations # this allows us to calculate the expectation over D sim_data = gen_sim_data(f) # fit models fit_0 = lm(y ~ 1, data = sim_data) fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data) # get predictions predictions[sim, 1] = predict(fit_0, x) predictions[sim, 2] = predict(fit_1, x) predictions[sim, 3] = predict(fit_2, x) predictions[sim, 4] = predict(fit_9, x) } Note that this is one of many ways we could have accomplished this task using R. For example we could have used a combination of replicate() and *apply() functions. Alternatively, we could have used a tidyverse approach, which likely would have used some combination of dplyr, tidyr, and purrr. Our approach, which would be considered a base R approach, was chosen to make it as clear as possible what is being done. The tidyverse approach is rapidly gaining popularity in the R community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach. Also of note, while it may seem like the output stored in predictions would meet the definition of tidy data given by Hadley Wickham since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction. This may be revised by the author later when there are more examples of how to do this from the R community. The above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, \\(f(x = 0.90) = (0.9)^2 = 0.81\\), is given by the solid black horizontal line. Two things are immediately clear: As complexity increases, bias decreases. (The mean of a model’s predictions is closer to the truth.) As complexity increases, variance increases. (The variance about the mean of a model’s predictions increases.) The goal of this simulation study is to show that the following holds true for each of the four models. \\[ \\text{MSE}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\underbrace{\\left(\\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] - f(0.90) \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}_k(0.90) \\right)} + \\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}_k(0.90) - \\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}_k(0.90) \\right)} \\] We’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.) To estimate the mean squared error of our predictions, we’ll use \\[ \\widehat{\\text{MSE}}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(f(0.90) - \\hat{f}_k(0.90) \\right)^2 \\] We also write an accompanying R function. get_mse = function(truth, estimate) { mean((estimate - truth) ^ 2) } Similarly, for the bias of our predictions we use, \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k(0.90) \\right) - f(0.90) \\] And again, we write an accompanying R function. get_bias = function(estimate, truth) { mean(estimate) - truth } Lastly, for the variance of our predictions we have \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k(0.90) \\right)^2 \\] While there is already R function for variance, the following is more appropriate in this situation. get_var = function(estimate) { mean((estimate - mean(estimate)) ^ 2) } To quickly obtain these results for each of the four models, we utilize the apply() function. bias = apply(predictions, 2, get_bias, truth = f(x = 0.90)) variance = apply(predictions, 2, get_var) mse = apply(predictions, 2, get_mse, truth = f(x = 0.90)) We summarize these results in the following table. Degree Mean Squared Error Bias Squared Variance 0 0.22643 0.22476 0.00167 1 0.00829 0.00508 0.00322 2 0.00387 0.00005 0.00381 9 0.01019 0.00002 0.01017 A number of things to notice here: We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as complexity increases. The squared bias trend which we see here is decreasing as complexity increases, which we expect to see in general. The exact opposite is true of variance. As model complexity increases, variance increases. The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error! We can check for these trends with the diff() function in R. all(diff(bias ^ 2) &lt; 0) ## [1] TRUE all(diff(variance) &gt; 0) ## [1] TRUE diff(mse) &lt; 0 ## 1 2 9 ## TRUE TRUE FALSE The models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance. Models with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees. \\[ \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9 \\] Then, since least squares estimation is unbiased, importantly, \\[ \\mathbb{E}[\\hat{\\beta}_d] = \\beta_d = 0 \\] for \\(d = 3, 4, \\ldots 9\\), we have \\[ \\mathbb{E}\\left[\\hat{f}_9(x)\\right] = \\beta_0 + \\beta_1 x + \\beta_2 x^2 \\] Now we can finally verify the bias-variance decomposition. bias ^ 2 + variance == mse ## 0 1 2 9 ## FALSE FALSE FALSE TRUE But wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models. all.equal(bias ^ 2 + variance, mse) ## [1] TRUE See ?all.equal() for details. So far, we’ve focused our efforts on looking at the mean squared error of estimating \\(f(0.90)\\) using \\(\\hat{f}(0.90)\\). We could also look at the expected prediction error of using \\(\\hat{f}(X)\\) when \\(X = 0.90\\) to estimate \\(Y\\). \\[ \\text{EPE}\\left(Y, \\hat{f}_k(0.90)\\right) = \\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[ \\left(Y - \\hat{f}_k(X) \\right)^2 \\mid X = 0.90 \\right] \\] We can estimate this quantity for each of the four models using the simulation study we already performed. get_epe = function(realized, estimate) { mean((realized - estimate) ^ 2) } y = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3) epe = apply(predictions, 2, get_epe, realized = y) epe ## 0 1 2 9 ## 0.3180470 0.1104055 0.1095955 0.1205570 What about the unconditional expected prediction error. That is, for any \\(X\\), not just \\(0.90\\). Specifically, the expected prediction error of estimating \\(Y\\) using \\(\\hat{f}(X)\\). The following (new) simulation study provides an estimate of \\[ \\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ \\left( Y - \\hat{f}_k(X) \\right)^2 \\right] \\] for the quadratic model, that is \\(k = 2\\) as we have defined \\(k\\). set.seed(1) n_sims = 1000 X = runif(n = n_sims, min = 0, max = 1) Y = rnorm(n = n_sims, mean = f(X), sd = 0.3) f_hat_X = rep(0, length(X)) for (i in seq_along(X)) { sim_data = gen_sim_data(f) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i])) } mean((Y - f_hat_X) ^ 2) ## [1] 0.09997319 Note that in practice, we should use many more simulations in this study. 10.4 Estimating Expected Prediction Error While previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally. Assuming \\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2. \\] we have \\[ \\text{EPE}\\left(Y, \\hat{f}(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ (Y - \\hat{f}(X))^2 \\right] = \\underbrace{\\mathbb{E}_{X} \\left[\\text{bias}^2\\left(\\hat{f}(X)\\right)\\right] + \\mathbb{E}_{X} \\left[\\text{var}\\left(\\hat{f}(X)\\right)\\right]}_\\textrm{reducible error} + \\sigma^2 \\] Lastly, we note that if \\[ \\mathcal{D} = \\mathcal{D}_{\\texttt{trn}} \\cup \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i = 1, 2, \\ldots n \\] where \\[ \\mathcal{D}_{\\texttt{trn}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{trn} \\] and \\[ \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{tst} \\] Then, if we use \\(\\mathcal{D}_{\\texttt{trn}}\\) to fit (train) a model, we can use the test mean squared error \\[ \\sum_{i \\in \\texttt{tst}}\\left(y_i - \\hat{f}(x_i)\\right) ^ 2 \\] as an estimate of \\[ \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ (Y - \\hat{f}(X))^2 \\right] \\] the expected prediction error. (In practice we prefer RMSE to MSE for comparing models and reporting because of the units.) How good is this estimate? Well, if \\(\\mathcal{D}\\) is a random sample from \\((X, Y)\\), and \\(\\texttt{tst}\\) are randomly sampled observations randomly sampled from \\(i = 1, 2, \\ldots, n\\), then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set. How variable? It turns out, pretty variable. While it’s a justified estimate, eventually we’ll introduce cross-validation as a procedure better suited to performing this estimation to select a model. 10.5 Reproducibility The R Markdown file for this chapter can be found here. The file was created using R version 3.6.1. "],
["classification.html", "Chapter 11 Classification 11.1 STAT 432 Materials 11.2 Bayes Classifier 11.3 Building a Classifier 11.4 Modeling 11.5 MISC TODO STUFF", " Chapter 11 Classification 11.1 STAT 432 Materials Slides | Classification: Introduction Code | Some Classification Code Slides | Classification: Binary Classification Code | Some Binary Classification Code Slides | Classification: Nonparametric Classification Reading | STAT 420: Logistic Regression Slides | Classification: Logistic Regression library(&quot;dplyr&quot;) library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) library(&quot;tibble&quot;) library(&quot;caret&quot;) library(&quot;rpart&quot;) library(&quot;nnet&quot;) 11.2 Bayes Classifier TODO: Not the same as naïve Bayes classifier \\[ p_k(x) = P\\left[ Y = k \\mid X = x \\right] \\] \\[ C^B(x) = \\underset{k \\in \\{1, 2, \\ldots K\\}}{\\text{argmax}} P\\left[ Y = k \\mid X = x \\right] \\] 11.2.1 Bayes Error Rate \\[ 1 - \\mathbb{E}_X\\left[ \\underset{k}{\\text{max}} \\ P[Y = k \\mid X = x] \\right] \\] 11.3 Building a Classifier \\[ \\hat{p}_k(x) = \\hat{P}\\left[ Y = k \\mid X = x \\right] \\] \\[ \\hat{C}(x) = \\underset{k \\in \\{1, 2, \\ldots K\\}}{\\text{argmax}} \\hat{p}_k(x) \\] TODO: first estimation conditional distribution, then classify to label with highest probability set.seed(1) joint_probs = round(1:12 / sum(1:12), 2) joint_probs = sample(joint_probs) joint_dist = matrix(data = joint_probs, nrow = 3, ncol = 4) colnames(joint_dist) = c(&quot;$X = 1$&quot;, &quot;$X = 2$&quot;, &quot;$X = 3$&quot;, &quot;$X = 4$&quot;) rownames(joint_dist) = c(&quot;$Y = A$&quot;, &quot;$Y = B$&quot;, &quot;$Y = C$&quot;) joint_dist %&gt;% kable() %&gt;% kable_styling(&quot;striped&quot;, full_width = FALSE) %&gt;% column_spec(column = 1, bold = TRUE, background = &quot;white&quot;, border_right = TRUE) \\(X = 1\\) \\(X = 2\\) \\(X = 3\\) \\(X = 4\\) \\(Y = A\\) 0.12 0.01 0.04 0.14 \\(Y = B\\) 0.05 0.03 0.10 0.15 \\(Y = C\\) 0.09 0.06 0.08 0.13 # marginal distribution of Y t(colSums(joint_dist)) %&gt;% kable() %&gt;% kable_styling(full_width = FALSE) \\(X = 1\\) \\(X = 2\\) \\(X = 3\\) \\(X = 4\\) 0.26 0.1 0.22 0.42 # marginal distribution of X t(rowSums(joint_dist)) %&gt;% kable() %&gt;% kable_styling(full_width = FALSE) \\(Y = A\\) \\(Y = B\\) \\(Y = C\\) 0.31 0.33 0.36 gen_data = function(n = 100) { x = sample(c(0, 1), prob = c(0.4, 0.6), size = n, replace = TRUE) y = ifelse(test = {x == 0}, yes = sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), size = n, prob = c(0.25, 0.50, 0.25), replace = TRUE), no = sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), size = n, prob = c(0.1, 0.1, 0.4) / 0.6, replace = TRUE)) tibble(x = x, y = factor(y)) } test_cases = tibble(x = c(0, 1)) set.seed(42) some_data = gen_data() predict(knn3(y ~ x, data = some_data), test_cases) ## A B C ## [1,] 0.2608696 0.39130435 0.3478261 ## [2,] 0.1481481 0.07407407 0.7777778 predict(rpart(y ~ x, data = some_data), test_cases) ## A B C ## 1 0.2608696 0.39130435 0.3478261 ## 2 0.1481481 0.07407407 0.7777778 predict(multinom(y ~ x, data = some_data, trace = FALSE), test_cases, type = &quot;prob&quot;) ## A B C ## 1 0.2608699 0.39130496 0.3478251 ## 2 0.1481478 0.07407414 0.7777781 11.4 Modeling 11.4.1 Linear Models TODO: use nnet::multinom in place of glm()? always? 11.4.2 k-Nearest Neighbors TODO: use caret::knn3() 11.4.3 Decision Trees TODO: use rpart::rpart() 11.5 MISC TODO STUFF TODO: https://topepo.github.io/caret/visualizations.html TODO: https://en.wikipedia.org/wiki/Confusion_matrix TODO: https://en.wikipedia.org/wiki/Matthews_correlation_coefficient TODO: https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf TODO: https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf TODO: http://www.oranlooney.com/post/viz-tsne/ TODO: https://web.expasy.org/pROC/ TODO: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-12-77 TODO: https://en.wikipedia.org/wiki/Receiver_operating_characteristic TODO: https://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.751&amp;rep=rep1&amp;type=pdf https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall06/lectures/naiveBayes.pdf http://www.stat.cmu.edu/~ryantibs/statml/lectures/linearclassification.pdf https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf sim_2d_logistic = function(beta_0, beta_1, beta_2, n) { par(mfrow = c(1, 2)) prob_plane = as_tibble(expand.grid(x1 = -220:220 / 100, x2 = -220:220 / 100)) prob_plane$p = with(prob_plane, boot::inv.logit(beta_0 + beta_1 * x1 + beta_2 * x2)) do_to_db = colorRampPalette(c(&#39;darkorange&#39;, &quot;white&quot;, &#39;dodgerblue&#39;)) plot(x2 ~ x1, data = prob_plane, col = do_to_db(100)[as.numeric(cut(prob_plane$p, seq(0, 1, length.out = 101)))], xlim = c(-2, 2), ylim = c(-2, 2), pch = 20) abline(-beta_0 / beta_2, -beta_1 / beta_2, col = &quot;black&quot;, lwd = 2) x1 = runif(n = n, -2, 2) x2 = runif(n = n, -2, 2) y = rbinom(n = n, size = 1, prob = boot::inv.logit(beta_0 + beta_1 * x1 + beta_2 * x2)) y = ifelse(y == 1, &quot;dodgerblue&quot;, &quot;orange&quot;) asdf = tibble(x1, x2, y) plot(x2 ~ x1, data = asdf, col = y, xlim = c(-2, 2), ylim = c(-2, 2), pch = 20) grid() abline(-beta_0 / beta_2, -beta_1 / beta_2, col = &quot;black&quot;, lwd = 2) } sim_2d_logistic(beta_0 = 2 * 0.5, beta_1 = 2* 0.7, beta_2 = 2* 0.5, n = 100) "],
["resampling.html", "Chapter 12 Resampling 12.1 STAT 432 Materials 12.2 Validation-Set Approach 12.3 Cross-Validation 12.4 Test Data 12.5 MISC TODOS", " Chapter 12 Resampling 12.1 STAT 432 Materials Code | Some Resampling Code library(&quot;dplyr&quot;) library(&quot;rsample&quot;) library(&quot;tibble&quot;) library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) library(&quot;purrr&quot;) In this chapter we introduce cross-validation. We will highlight the need for cross-validation by comparing it to our previous approach, which was to use a single validation set inside of the training data. To illustrate the use of cross-validation, we’ll consider a regression setup with a single feature \\(x\\), and a regression function \\(f(x) = x^3\\). Adding an additional noise parameter, and the distribution of the feature variable, we define the entire data generating process as \\[ X \\sim \\text{Unif}\\left(a = -1, b = 1\\right) \\\\ Y \\mid X \\sim \\text{Normal}\\left(\\mu = x^3, \\sigma^2 = 0.25 ^ 2\\right) \\] We write an R function that generates datasets according to this process. gen_sim_data = function(sample_size) { x = runif(n = sample_size, min = -1, max = 1) y = rnorm(n = sample_size, mean = x ^ 3, sd = 0.25) tibble(x, y) } We first simulate a single train dataset, which we also split into an estimation and validation set. We also simulate a large test dataset. (Which we could not do in pratice, but is possible here.) set.seed(1) sim_trn = gen_sim_data(sample_size = 200) sim_idx = sample(1:nrow(sim_trn), 160) sim_est = sim_trn[sim_idx, ] sim_val = sim_trn[-sim_idx, ] sim_tst = gen_sim_data(sample_size = 10000) We plot this training data, as well as the true regression function. plot(y ~ x, data = sim_trn, col = &quot;dodgerblue&quot;, pch = 20) grid() curve(x ^ 3, add = TRUE, col = &quot;black&quot;, lwd = 2) calc_rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } Recall that we needed this validation set because the training error was far too optimistic for highly flexible models. This would lead us to always use the most flexible model. (That is, data that is used to fit a model should not be used to validate a model.) tibble( &quot;Polynomial Degree&quot; = 1:10, &quot;Train RMSE&quot; = map_dbl(1:10, ~ calc_rmse(actual = sim_est$y, predicted = predict(lm(y ~ poly(x, .x), data = sim_est), sim_est))), &quot;Validation RMSE&quot; = map_dbl(1:10, ~ calc_rmse(actual = sim_val$y, predicted = predict(lm(y ~ poly(x, .x), data = sim_est), sim_val))) ) %&gt;% kable(digits = 4) %&gt;% kable_styling(&quot;striped&quot;, full_width = FALSE) Polynomial Degree Train RMSE Validation RMSE 1 0.2865 0.3233 2 0.2861 0.3220 3 0.2400 0.2746 4 0.2398 0.2754 5 0.2288 0.2832 6 0.2288 0.2833 7 0.2287 0.2820 8 0.2286 0.2805 9 0.2286 0.2803 10 0.2267 0.2886 12.2 Validation-Set Approach TODO: consider fitting polynomial models of degree k = 1:10 to data from this data generating process TODO: here, we can consider k, the polynomial degree, as a tuning parameter TODO: perform simulation study to evaluate how well validation set approach works num_sims = 100 num_degrees = 10 val_rmse = matrix(0, ncol = num_degrees, nrow = num_sims) TODO: each simulation we will… set.seed(42) for (i in 1:num_sims) { # simulate data sim_trn = gen_sim_data(sample_size = 200) # set aside validation set sim_idx = sample(1:nrow(sim_trn), 160) sim_est = sim_trn[sim_idx, ] sim_val = sim_trn[-sim_idx, ] # fit models and store RMSEs for (j in 1:num_degrees) { #fit model fit = glm(y ~ poly(x, degree = j), data = sim_est) # calculate error val_rmse[i, j] = calc_rmse(actual = sim_val$y, predicted = predict(fit, sim_val)) } } TODO: issues are hard to “see” but have to do with variability TODO: sometimes we are selecting models that are not flexible enough! 12.3 Cross-Validation Instead of using a single estimation-validation split, we instead look to use \\(K\\)-fold cross-validation. \\[ \\text{RMSE-CV}_{K} = \\sum_{k = 1}^{K} \\frac{n_k}{n} \\text{RMSE}_k \\] \\[ \\text{RMSE}_k = \\sqrt{\\frac{1}{n_k} \\sum_{i \\in C_k} \\left( y_i - \\hat{f}^{-k}(x_i) \\right)^2 } \\] \\(n_k\\) is the number of observations in fold \\(k\\) \\(C_k\\) are the observations in fold \\(k\\) \\(\\hat{f}^{-k}()\\) is the trained model using the training data without fold \\(k\\) If \\(n_k\\) is the same in each fold, then \\[ \\text{RMSE-CV}_{K} = \\frac{1}{K}\\sum_{k = 1}^{K} \\text{RMSE}_k \\] TODO: create and add graphic that shows the splitting process TODO: Can be used with any metric, MSE, RMSE, class-err, class-acc There are many ways to perform cross-validation in R, depending on the statistical learning method of interest. Some methods, for example glm() through boot::cv.glm() and knn() through knn.cv() have cross-validation capabilities built-in. We’ll use glm() for illustration. First we need to convince ourselves that glm() can be used to perform the same tasks as lm(). glm_fit = glm(y ~ poly(x, 3), data = sim_trn) coef(glm_fit) ## (Intercept) poly(x, 3)1 poly(x, 3)2 poly(x, 3)3 ## -0.02516901 5.06661745 -0.09349681 2.64581436 lm_fit = lm(y ~ poly(x, 3), data = sim_trn) coef(lm_fit) ## (Intercept) poly(x, 3)1 poly(x, 3)2 poly(x, 3)3 ## -0.02516901 5.06661745 -0.09349681 2.64581436 By default, cv.glm() will report leave-one-out cross-validation (LOOCV). sqrt(boot::cv.glm(sim_trn, glm_fit)$delta) ## [1] 0.2488233 0.2488099 We are actually given two values. The first is exactly the LOOCV-MSE. The second is a minor correction that we will not worry about. We take a square root to obtain LOOCV-RMSE. In practice, we often prefer 5 or 10-fold cross-validation for a number of reason, but often most importantly, for computational efficiency. sqrt(boot::cv.glm(sim_trn, glm_fit, K = 5)$delta) ## [1] 0.2470322 0.2466417 We repeat the above simulation study, this time performing 5-fold cross-validation. With a total sample size of \\(n = 200\\) each validation set has 40 observations, as did the single validation set in the previous simulations. cv_rmse = matrix(0, ncol = num_degrees, nrow = num_sims) set.seed(42) for (i in 1:num_sims) { # simulate data, use all data for training sim_trn = gen_sim_data(sample_size = 200) # fit models and store RMSE for (j in 1:num_degrees) { #fit model fit = glm(y ~ poly(x, degree = j), data = sim_trn) # calculate error cv_rmse[i, j] = sqrt(boot::cv.glm(sim_trn, fit, K = 5)$delta[1]) } } Polynomial Degree Mean, Val SD, Val Mean, CV SD, CV 1 0.290 0.031 0.293 0.015 2 0.291 0.031 0.295 0.014 3 0.247 0.027 0.251 0.010 4 0.248 0.028 0.252 0.010 5 0.248 0.027 0.253 0.010 6 0.249 0.027 0.254 0.011 7 0.251 0.027 0.255 0.012 8 0.252 0.027 0.257 0.011 9 0.253 0.028 0.258 0.012 10 0.255 0.027 0.259 0.012 TODO: differences: less variance, better selections 12.4 Test Data The following example, inspired by The Elements of Statistical Learning, will illustrate the need for a dedicated test set which is never used in model training. We do this, if for no other reason, because it gives us a quick sanity check that we have cross-validated correctly. To be specific we will always test-train split the data, then perform cross-validation within the training data. Essentially, this example will also show how to not cross-validate properly. It will also show an example of cross-validation in a classification setting. calc_misclass = function(actual, predicted) { mean(actual != predicted) } Consider a binary response \\(Y\\) with equal probability to take values \\(0\\) and \\(1\\). \\[ Y \\sim \\text{bern}(p = 0.5) \\] Also consider \\(p = 10,000\\) independent predictor variables, \\(X_j\\), each with a standard normal distribution. \\[ X_j \\sim N(\\mu = 0, \\sigma^2 = 1) \\] We simulate \\(n = 100\\) observations from this data generating process. Notice that the way we’ve defined this process, none of the \\(X_j\\) are related to \\(Y\\). set.seed(42) n = 200 p = 10000 x = replicate(p, rnorm(n)) y = c(rbinom(n = n, size = 1, prob = 0.5)) full_data = as_tibble(data.frame(y, x)) full_data ## # A tibble: 200 x 10,001 ## y X1 X2 X3 X4 X5 X6 X7 X8 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.37 -2.00 1.33 -0.248 0.689 2.33 -0.747 0.877 ## 2 1 -0.565 0.334 -0.869 0.422 0.725 0.524 0.0366 -1.77 ## 3 1 0.363 1.17 0.0555 0.988 0.217 0.971 0.323 -0.0457 ## 4 0 0.633 2.06 0.0491 0.836 -0.202 0.377 0.380 -0.395 ## 5 0 0.404 -1.38 -0.578 -0.661 -1.37 -0.996 0.877 -0.128 ## 6 1 -0.106 -1.15 -0.999 1.56 -0.309 -0.597 0.933 1.10 ## 7 1 1.51 -0.706 -0.00243 -1.62 -0.453 0.165 -2.43 -1.26 ## 8 0 -0.0947 -1.05 0.656 0.864 0.663 -2.93 1.73 -0.265 ## 9 1 2.02 -0.646 1.48 -0.512 1.31 -0.848 0.456 2.55 ## 10 0 -0.0627 -0.185 -1.91 -1.92 0.501 0.799 -0.570 -1.48 ## # … with 190 more rows, and 9,992 more variables: X9 &lt;dbl&gt;, X10 &lt;dbl&gt;, ## # X11 &lt;dbl&gt;, X12 &lt;dbl&gt;, X13 &lt;dbl&gt;, X14 &lt;dbl&gt;, X15 &lt;dbl&gt;, X16 &lt;dbl&gt;, ## # X17 &lt;dbl&gt;, X18 &lt;dbl&gt;, X19 &lt;dbl&gt;, X20 &lt;dbl&gt;, X21 &lt;dbl&gt;, X22 &lt;dbl&gt;, ## # X23 &lt;dbl&gt;, X24 &lt;dbl&gt;, X25 &lt;dbl&gt;, X26 &lt;dbl&gt;, X27 &lt;dbl&gt;, X28 &lt;dbl&gt;, ## # X29 &lt;dbl&gt;, X30 &lt;dbl&gt;, X31 &lt;dbl&gt;, X32 &lt;dbl&gt;, X33 &lt;dbl&gt;, X34 &lt;dbl&gt;, ## # X35 &lt;dbl&gt;, X36 &lt;dbl&gt;, X37 &lt;dbl&gt;, X38 &lt;dbl&gt;, X39 &lt;dbl&gt;, X40 &lt;dbl&gt;, ## # X41 &lt;dbl&gt;, X42 &lt;dbl&gt;, X43 &lt;dbl&gt;, X44 &lt;dbl&gt;, X45 &lt;dbl&gt;, X46 &lt;dbl&gt;, ## # X47 &lt;dbl&gt;, X48 &lt;dbl&gt;, X49 &lt;dbl&gt;, X50 &lt;dbl&gt;, X51 &lt;dbl&gt;, X52 &lt;dbl&gt;, ## # X53 &lt;dbl&gt;, X54 &lt;dbl&gt;, X55 &lt;dbl&gt;, X56 &lt;dbl&gt;, X57 &lt;dbl&gt;, X58 &lt;dbl&gt;, ## # X59 &lt;dbl&gt;, X60 &lt;dbl&gt;, X61 &lt;dbl&gt;, X62 &lt;dbl&gt;, X63 &lt;dbl&gt;, X64 &lt;dbl&gt;, ## # X65 &lt;dbl&gt;, X66 &lt;dbl&gt;, X67 &lt;dbl&gt;, X68 &lt;dbl&gt;, X69 &lt;dbl&gt;, X70 &lt;dbl&gt;, ## # X71 &lt;dbl&gt;, X72 &lt;dbl&gt;, X73 &lt;dbl&gt;, X74 &lt;dbl&gt;, X75 &lt;dbl&gt;, X76 &lt;dbl&gt;, ## # X77 &lt;dbl&gt;, X78 &lt;dbl&gt;, X79 &lt;dbl&gt;, X80 &lt;dbl&gt;, X81 &lt;dbl&gt;, X82 &lt;dbl&gt;, ## # X83 &lt;dbl&gt;, X84 &lt;dbl&gt;, X85 &lt;dbl&gt;, X86 &lt;dbl&gt;, X87 &lt;dbl&gt;, X88 &lt;dbl&gt;, ## # X89 &lt;dbl&gt;, X90 &lt;dbl&gt;, X91 &lt;dbl&gt;, X92 &lt;dbl&gt;, X93 &lt;dbl&gt;, X94 &lt;dbl&gt;, ## # X95 &lt;dbl&gt;, X96 &lt;dbl&gt;, X97 &lt;dbl&gt;, X98 &lt;dbl&gt;, X99 &lt;dbl&gt;, X100 &lt;dbl&gt;, ## # X101 &lt;dbl&gt;, X102 &lt;dbl&gt;, X103 &lt;dbl&gt;, X104 &lt;dbl&gt;, X105 &lt;dbl&gt;, ## # X106 &lt;dbl&gt;, X107 &lt;dbl&gt;, X108 &lt;dbl&gt;, … Before attempting to perform cross-validation, we test-train split the data, using half of the available data for each. (In practice, with this little data, it would be hard to justify a separate test dataset, but here we do so to illustrate another point.) trn_idx = sample(1:nrow(full_data), trunc(nrow(full_data) * 0.5)) trn_data = full_data[trn_idx, ] tst_data = full_data[-trn_idx, ] Now we would like to train a logistic regression model to predict \\(Y\\) using the available predictor data. However, here we have \\(p &gt; n\\), which prevents us from fitting logistic regression. To overcome this issue, we will first attempt to find a subset of relevant predictors. To do so, we’ll simply find the predictors that are most correlated with the response. # find correlation between y and each predictor variable correlations = apply(trn_data[, -1], 2, cor, y = trn_data$y) While many of these correlations are small, many very close to zero, some are as large as 0.40. Since our training data has 50 observations, we’ll select the 25 predictors with the largest (absolute) correlations. selected = order(abs(correlations), decreasing = TRUE)[1:25] correlations[selected] ## X4942 X867 X8617 X8044 X406 X4358 ## 0.4005771 0.3847397 0.3809371 0.3692479 -0.3571329 0.3553777 ## X7725 X1986 X3784 X77 X7010 X9354 ## -0.3459522 -0.3448612 0.3298109 -0.3252776 -0.3242813 0.3227353 ## X8450 X2355 X4381 X2486 X5947 X5767 ## 0.3220087 0.3192606 0.3157441 0.3149892 0.3131235 0.3114936 ## X1227 X1464 X8223 X188 X4203 X2234 ## -0.3105052 -0.3104528 0.3084551 0.3065491 0.3039848 -0.3036512 ## X1098 ## -0.3036153 We subset the training and test sets to contain only the response as well as these 25 predictors. trn_screen = trn_data[c(1, selected)] tst_screen = tst_data[c(1, selected)] Then we finally fit an additive logistic regression using this subset of predictors. We perform 10-fold cross-validation to obtain an estimate of the classification error. add_log_mod = glm(y ~ ., data = trn_screen, family = &quot;binomial&quot;) boot::cv.glm(trn_screen, add_log_mod, K = 10)$delta[1] ## [1] 0.3742339 The 10-fold cross-validation is suggesting a classification error estimate of almost 30%. add_log_pred = (predict(add_log_mod, newdata = tst_screen, type = &quot;response&quot;) &gt; 0.5) * 1 calc_misclass(predicted = add_log_pred, actual = tst_screen$y) ## [1] 0.48 However, if we obtain an estimate of the error using the set, we see an error rate of about 50%. No better than guessing! But since \\(Y\\) has no relationship with the predictors, this is actually what we would expect. This incorrect method we’ll call screen-then-validate. Now, we will correctly screen-while-validating. Essentially, instead of simply cross-validating the logistic regression, we also need to cross validate the screening process. That is, we won’t simply use the same variables for each fold, we get the “best” predictors for each fold. For methods that do not have a built-in ability to perform cross-validation, or for methods that have limited cross-validation capability, we will need to write our own code for cross-validation. (Spoiler: This is not completely true, but let’s pretend it is, so we can see how to perform cross-validation from scratch.) This essentially amounts to randomly splitting the data, then looping over the splits. The createFolds() function from the caret() package will make this much easier. caret::createFolds(trn_data$y, k = 10) ## $Fold01 ## [1] 17 23 27 44 45 76 85 87 93 97 ## ## $Fold02 ## [1] 6 14 15 26 37 38 55 68 69 71 ## ## $Fold03 ## [1] 3 4 7 29 39 52 54 57 59 82 ## ## $Fold04 ## [1] 19 21 40 46 48 56 73 78 91 96 ## ## $Fold05 ## [1] 25 34 36 58 61 65 66 75 83 89 ## ## $Fold06 ## [1] 2 9 10 62 74 79 80 90 92 98 ## ## $Fold07 ## [1] 8 31 32 41 43 53 60 67 88 95 ## ## $Fold08 ## [1] 12 18 33 35 42 49 51 64 84 94 ## ## $Fold09 ## [1] 11 13 16 20 28 47 50 77 99 100 ## ## $Fold10 ## [1] 1 5 22 24 30 63 70 72 81 86 # use the caret package to obtain 10 &quot;folds&quot; folds = caret::createFolds(trn_data$y, k = 10) # for each fold # - pre-screen variables on the 9 training folds # - fit model to these variables # - get error on validation fold fold_err = rep(0, length(folds)) for (i in seq_along(folds)) { # split for fold i est_fold = trn_data[-folds[[i]], ] val_fold = trn_data[folds[[i]], ] # screening for fold i correlations = apply(est_fold[, -1], 2, cor, y = est_fold[,1]) selected = order(abs(correlations), decreasing = TRUE)[1:25] est_fold_screen = est_fold[ , c(1, selected)] val_fold_screen = val_fold[ , c(1, selected)] # error for fold i add_log_mod = glm(y ~ ., data = est_fold_screen, family = &quot;binomial&quot;) add_log_prob = predict(add_log_mod, newdata = val_fold_screen, type = &quot;response&quot;) add_log_pred = ifelse(add_log_prob &gt; 0.5, yes = 1, no = 0) fold_err[i] = mean(add_log_pred != val_fold_screen$y) } # report all 10 validation fold errors fold_err ## [1] 0.4 0.9 0.6 0.4 0.6 0.3 0.7 0.5 0.6 0.6 # properly cross-validated error # this roughly matches what we expect in the test set mean(fold_err) ## [1] 0.56 TODO: note that, even cross-validated correctly, this isn’t a brilliant variable selection procedure. (it completely ignores interactions and correlations among the predictors. however, if it works, it works.) next chapters… TODO: calculate test error 12.5 MISC TODOS TODO: https://github.com/topepo/caret/issues/70 TODO: https://stats.stackexchange.com/questions/266225/step-by-step-explanation-of-k-fold-cross-validation-with-grid-search-to-optimise TODO: https://weina.me/nested-cross-validation/ rsample::nested_cv http://appliedpredictivemodeling.com/blog/2014/11/27/08ks7leh0zof45zpf5vqe56d1sahb0 http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm http://appliedpredictivemodeling.com/blog/2017/9/2/njdc83d01pzysvvlgik02t5qnaljnd \\[ x, \\boldsymbol{x}, X, \\boldsymbol{X}, \\mathbf{x}, \\mathbf{X}, \\mathbb{X}, \\mathcal{X}, \\mathfrak{x}, \\mathfrak{X}, \\mathrm{x}, \\mathrm{X} \\] "],
["supervised-learning.html", "Chapter 13 Supervised Learning", " Chapter 13 Supervised Learning TODO: write an overview / review of the previous two chapters TODO: do two analyses? TODO: use caret in this chapter "],
["regularization.html", "Chapter 14 Regularization 14.1 STAT 432 Materials 14.2 Reducing Variance with Added Bias 14.3 scaling matters? 14.4 Constraints in Two Dimensions 14.5 High Dimensional Data 14.6 Ridge Regression 14.7 Lasso 14.8 boston is boring 14.9 some more simulation", " Chapter 14 Regularization 14.1 STAT 432 Materials ISL Readings: Sections 6.1 - 6.4 library(&quot;tidyverse&quot;) library(&quot;glmnet&quot;) library(&quot;broom&quot;) library(&quot;kableExtra&quot;) 14.2 Reducing Variance with Added Bias gen_simple_data = function(sample_size = 25) { x = runif(n = sample_size) y = 0 + 5 * x + rnorm(n = sample_size) data.frame(x, y) } set.seed(42) simple_data = gen_simple_data() # fit least squares beta_ls = lm(y ~ 0 + x, data = simple_data) # fit a biased model # restrict beta-hat to be at most 5 beta_05 = lm(y ~ 0 + x, data = simple_data) beta_05$coefficients = min(beta_05$coefficients, 5) # fit a biased model # restrict beta-hat to be at most 4 beta_04 = lm(y ~ 0 + x, data = simple_data) beta_04$coefficients = min(beta_04$coefficients, 4) map_dbl(list(beta_ls, beta_05, beta_04), coef) ## [1] 5.325953 5.000000 4.000000 # maybe write a function for each # should switch this to make each model fit to the same data set.seed(42) beta_estimates = list( beta_ls = replicate(n = 5000, coef(lm(y ~ 0 + x, data = gen_simple_data()))[&quot;x&quot;]), beta_05 = replicate(n = 5000, min(coef(lm(y ~ 0 + x, data = gen_simple_data()))[&quot;x&quot;], 5)), beta_04 = replicate(n = 5000, min(coef(lm(y ~ 0 + x, data = gen_simple_data()))[&quot;x&quot;], 4)) ) Model Bias Variance MSE Least Squares -0.003 0.13 0.130 Biased to 5 -0.134 0.04 0.058 Biased to 4 -1.000 0.00 1.001 14.3 scaling matters? another_dgp = function(sample_size = 25) { x = runif(n = sample_size) y = -2 + 5 * x + rnorm(n = sample_size) tibble(x, y) } data_for_scaling = another_dgp() predict(lm(y ~ x, data = data_for_scaling)) ## 1 2 3 4 5 6 ## -1.3821582 -1.9123198 2.3840155 -1.9167476 2.6436459 -0.0673255 ## 7 8 9 10 11 12 ## -0.2236885 2.4778373 1.1021074 1.6899061 0.2489363 -0.7390997 ## 13 14 15 16 17 18 ## 3.4760505 0.4906860 -0.9683105 0.4202554 1.7825769 -0.2720839 ## 19 20 21 22 23 24 ## 0.9580110 1.1225404 2.9943423 3.7554539 1.2570305 -0.9414314 ## 25 ## 2.3061219 coef(lm(y ~ x, data = data_for_scaling)) ## (Intercept) x ## -2.229674 6.287082 data_for_scaling$x = scale(data_for_scaling$x) predict(lm(y ~ x, data = data_for_scaling)) ## 1 2 3 4 5 6 ## -1.3821582 -1.9123198 2.3840155 -1.9167476 2.6436459 -0.0673255 ## 7 8 9 10 11 12 ## -0.2236885 2.4778373 1.1021074 1.6899061 0.2489363 -0.7390997 ## 13 14 15 16 17 18 ## 3.4760505 0.4906860 -0.9683105 0.4202554 1.7825769 -0.2720839 ## 19 20 21 22 23 24 ## 0.9580110 1.1225404 2.9943423 3.7554539 1.2570305 -0.9414314 ## 25 ## 2.3061219 coef(lm(y ~ x, data = data_for_scaling)) ## (Intercept) x ## 0.8274541 1.6506452 14.4 Constraints in Two Dimensions gen_linear_data = function() { x1 = rnorm(100) x2 = rnorm(100) y = 0 + -5 * x1 + 5 * x2 + rnorm(100) tibble(x1 = x1, x2 = x2, y = y) } data = gen_linear_data() beta = expand.grid(beta_1 = seq(-10, 10, 0.1), beta_2 = seq(-10, 10, 0.1)) beta_error = rep(0, dim(beta)[1]) for (i in 1:dim(beta)[1]){ beta_error[i] = with(data, sum((y - (beta$beta_1[i] * x1 + beta$beta_2[i] * x2)) ^ 2 )) } # TODO: make this into a function # TODO: add ridge constraint contour(x = seq(-10, 10, 0.1), y = seq(-10, 10, 0.1), z = matrix(beta_error, nrow = length(seq(-10, 10, 0.1)), ncol = length(seq(-10, 10, 0.1))), nlevels = 50, col = &quot;darkgrey&quot; ) abline(h = 0) abline(v = 0) a = 4 segments(0, a, a, 0, col = &quot;dodgerblue&quot;, lwd = 2) segments(0, -a, a, 0, col = &quot;dodgerblue&quot;, lwd = 2) segments(-a, 0, 0, a, col = &quot;dodgerblue&quot;, lwd = 2) segments(-a, 0, 0, -a, col = &quot;dodgerblue&quot;, lwd = 2) points(beta[which.min(beta_error), ], col = &quot;darkorange&quot;, pch = 20, cex = 2) 14.5 High Dimensional Data gen_wide_data = function(sample_size = 100, sig_betas = 5, p = 200) { if (p &lt;= sample_size) { warning(&quot;You&#39;re not generating wide data, despite the name of the function.&quot;) } if (sig_betas &gt; p) { stop(&quot;Cannot have more significant variables than variables!&quot;) } x = map_dfc(1:p, ~ rnorm(n = sample_size)) x = x %&gt;% rename_all( ~ str_replace(., &quot;V&quot;, &quot;x&quot;)) sig_x = x[, 1:sig_betas] beta = rep(3, times = sig_betas) y = as.matrix(sig_x) %*% beta + rnorm(n = sample_size) bind_cols(y = y, x) } some_wide_data = gen_wide_data() some_wide_data ## # A tibble: 100 x 201 ## y[,1] x1 x2 x3 x4 x5 x6 x7 x8 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.00 1.75 -0.449 1.09 -0.981 0.217 -0.390 1.13 1.83 ## 2 -7.81 -0.792 -0.659 -0.823 -0.275 0.281 0.931 0.680 0.354 ## 3 4.29 0.648 0.533 -0.415 1.19 -0.451 0.914 0.953 0.527 ## 4 7.22 1.06 0.913 0.0341 0.793 -0.522 0.680 -0.124 1.74 ## 5 -2.09 -0.603 -0.594 -0.141 0.897 0.162 -0.0729 -0.550 2.81 ## 6 -6.24 0.461 -0.996 -1.25 -0.386 -0.175 -0.153 -0.914 0.230 ## 7 10.2 1.45 1.60 0.426 0.0269 0.318 0.0255 -2.15 0.669 ## 8 2.42 0.576 0.757 0.537 -0.828 -0.348 -0.640 2.25 0.942 ## 9 -0.0202 -0.287 0.276 -0.550 0.537 -0.0493 1.42 -0.558 -0.283 ## 10 1.49 0.563 -0.264 0.0107 0.00393 -0.367 1.75 -0.893 1.75 ## # … with 90 more rows, and 192 more variables: x9 &lt;dbl&gt;, x10 &lt;dbl&gt;, ## # x11 &lt;dbl&gt;, x12 &lt;dbl&gt;, x13 &lt;dbl&gt;, x14 &lt;dbl&gt;, x15 &lt;dbl&gt;, x16 &lt;dbl&gt;, ## # x17 &lt;dbl&gt;, x18 &lt;dbl&gt;, x19 &lt;dbl&gt;, x20 &lt;dbl&gt;, x21 &lt;dbl&gt;, x22 &lt;dbl&gt;, ## # x23 &lt;dbl&gt;, x24 &lt;dbl&gt;, x25 &lt;dbl&gt;, x26 &lt;dbl&gt;, x27 &lt;dbl&gt;, x28 &lt;dbl&gt;, ## # x29 &lt;dbl&gt;, x30 &lt;dbl&gt;, x31 &lt;dbl&gt;, x32 &lt;dbl&gt;, x33 &lt;dbl&gt;, x34 &lt;dbl&gt;, ## # x35 &lt;dbl&gt;, x36 &lt;dbl&gt;, x37 &lt;dbl&gt;, x38 &lt;dbl&gt;, x39 &lt;dbl&gt;, x40 &lt;dbl&gt;, ## # x41 &lt;dbl&gt;, x42 &lt;dbl&gt;, x43 &lt;dbl&gt;, x44 &lt;dbl&gt;, x45 &lt;dbl&gt;, x46 &lt;dbl&gt;, ## # x47 &lt;dbl&gt;, x48 &lt;dbl&gt;, x49 &lt;dbl&gt;, x50 &lt;dbl&gt;, x51 &lt;dbl&gt;, x52 &lt;dbl&gt;, ## # x53 &lt;dbl&gt;, x54 &lt;dbl&gt;, x55 &lt;dbl&gt;, x56 &lt;dbl&gt;, x57 &lt;dbl&gt;, x58 &lt;dbl&gt;, ## # x59 &lt;dbl&gt;, x60 &lt;dbl&gt;, x61 &lt;dbl&gt;, x62 &lt;dbl&gt;, x63 &lt;dbl&gt;, x64 &lt;dbl&gt;, ## # x65 &lt;dbl&gt;, x66 &lt;dbl&gt;, x67 &lt;dbl&gt;, x68 &lt;dbl&gt;, x69 &lt;dbl&gt;, x70 &lt;dbl&gt;, ## # x71 &lt;dbl&gt;, x72 &lt;dbl&gt;, x73 &lt;dbl&gt;, x74 &lt;dbl&gt;, x75 &lt;dbl&gt;, x76 &lt;dbl&gt;, ## # x77 &lt;dbl&gt;, x78 &lt;dbl&gt;, x79 &lt;dbl&gt;, x80 &lt;dbl&gt;, x81 &lt;dbl&gt;, x82 &lt;dbl&gt;, ## # x83 &lt;dbl&gt;, x84 &lt;dbl&gt;, x85 &lt;dbl&gt;, x86 &lt;dbl&gt;, x87 &lt;dbl&gt;, x88 &lt;dbl&gt;, ## # x89 &lt;dbl&gt;, x90 &lt;dbl&gt;, x91 &lt;dbl&gt;, x92 &lt;dbl&gt;, x93 &lt;dbl&gt;, x94 &lt;dbl&gt;, ## # x95 &lt;dbl&gt;, x96 &lt;dbl&gt;, x97 &lt;dbl&gt;, x98 &lt;dbl&gt;, x99 &lt;dbl&gt;, x100 &lt;dbl&gt;, ## # x101 &lt;dbl&gt;, x102 &lt;dbl&gt;, x103 &lt;dbl&gt;, x104 &lt;dbl&gt;, x105 &lt;dbl&gt;, ## # x106 &lt;dbl&gt;, x107 &lt;dbl&gt;, x108 &lt;dbl&gt;, … 14.6 Ridge Regression \\[ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right) ^ 2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 . \\] set.seed(42) data_for_ridge = gen_wide_data(sample_size = 100, sig_betas = 5, p = 200) x_ridge = data_for_ridge %&gt;% select(-y) %&gt;% as.matrix() y_ridge = data_for_ridge %&gt;% pull(y) mod_ridge = glmnet(x = x_ridge, y = y_ridge, alpha = 0) plot(mod_ridge, xvar = &quot;lambda&quot;, label = TRUE) grid() as_tibble(predict(mod_ridge, x_ridge[1:5, ])) ## # A tibble: 5 x 100 ## s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.407 -0.357 -0.354 -0.352 -0.349 -0.346 -0.344 -0.341 -0.337 -0.334 ## 2 -0.407 -0.400 -0.400 -0.400 -0.399 -0.399 -0.398 -0.398 -0.398 -0.397 ## 3 -0.407 -0.361 -0.359 -0.356 -0.354 -0.352 -0.349 -0.346 -0.343 -0.340 ## 4 -0.407 -0.322 -0.318 -0.314 -0.310 -0.305 -0.301 -0.295 -0.290 -0.285 ## 5 -0.407 -0.441 -0.442 -0.444 -0.446 -0.447 -0.449 -0.451 -0.453 -0.456 ## # … with 90 more variables: s10 &lt;dbl&gt;, s11 &lt;dbl&gt;, s12 &lt;dbl&gt;, s13 &lt;dbl&gt;, ## # s14 &lt;dbl&gt;, s15 &lt;dbl&gt;, s16 &lt;dbl&gt;, s17 &lt;dbl&gt;, s18 &lt;dbl&gt;, s19 &lt;dbl&gt;, ## # s20 &lt;dbl&gt;, s21 &lt;dbl&gt;, s22 &lt;dbl&gt;, s23 &lt;dbl&gt;, s24 &lt;dbl&gt;, s25 &lt;dbl&gt;, ## # s26 &lt;dbl&gt;, s27 &lt;dbl&gt;, s28 &lt;dbl&gt;, s29 &lt;dbl&gt;, s30 &lt;dbl&gt;, s31 &lt;dbl&gt;, ## # s32 &lt;dbl&gt;, s33 &lt;dbl&gt;, s34 &lt;dbl&gt;, s35 &lt;dbl&gt;, s36 &lt;dbl&gt;, s37 &lt;dbl&gt;, ## # s38 &lt;dbl&gt;, s39 &lt;dbl&gt;, s40 &lt;dbl&gt;, s41 &lt;dbl&gt;, s42 &lt;dbl&gt;, s43 &lt;dbl&gt;, ## # s44 &lt;dbl&gt;, s45 &lt;dbl&gt;, s46 &lt;dbl&gt;, s47 &lt;dbl&gt;, s48 &lt;dbl&gt;, s49 &lt;dbl&gt;, ## # s50 &lt;dbl&gt;, s51 &lt;dbl&gt;, s52 &lt;dbl&gt;, s53 &lt;dbl&gt;, s54 &lt;dbl&gt;, s55 &lt;dbl&gt;, ## # s56 &lt;dbl&gt;, s57 &lt;dbl&gt;, s58 &lt;dbl&gt;, s59 &lt;dbl&gt;, s60 &lt;dbl&gt;, s61 &lt;dbl&gt;, ## # s62 &lt;dbl&gt;, s63 &lt;dbl&gt;, s64 &lt;dbl&gt;, s65 &lt;dbl&gt;, s66 &lt;dbl&gt;, s67 &lt;dbl&gt;, ## # s68 &lt;dbl&gt;, s69 &lt;dbl&gt;, s70 &lt;dbl&gt;, s71 &lt;dbl&gt;, s72 &lt;dbl&gt;, s73 &lt;dbl&gt;, ## # s74 &lt;dbl&gt;, s75 &lt;dbl&gt;, s76 &lt;dbl&gt;, s77 &lt;dbl&gt;, s78 &lt;dbl&gt;, s79 &lt;dbl&gt;, ## # s80 &lt;dbl&gt;, s81 &lt;dbl&gt;, s82 &lt;dbl&gt;, s83 &lt;dbl&gt;, s84 &lt;dbl&gt;, s85 &lt;dbl&gt;, ## # s86 &lt;dbl&gt;, s87 &lt;dbl&gt;, s88 &lt;dbl&gt;, s89 &lt;dbl&gt;, s90 &lt;dbl&gt;, s91 &lt;dbl&gt;, ## # s92 &lt;dbl&gt;, s93 &lt;dbl&gt;, s94 &lt;dbl&gt;, s95 &lt;dbl&gt;, s96 &lt;dbl&gt;, s97 &lt;dbl&gt;, ## # s98 &lt;dbl&gt;, s99 &lt;dbl&gt; set.seed(42) mod_ridge = cv.glmnet(x = x_ridge, y = y_ridge, alpha = 0, nfolds = 5) glance(mod_ridge) ## # A tibble: 1 x 2 ## lambda.min lambda.1se ## &lt;dbl&gt; &lt;dbl&gt; ## 1 35.4 67.9 plot(mod_ridge) tidy(mod_ridge) ## # A tibble: 100 x 6 ## lambda estimate std.error conf.low conf.high nzero ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 3543. 45.1 4.01 41.1 49.1 200 ## 2 3382. 45.1 4.02 41.1 49.1 200 ## 3 3228. 45.1 4.01 41.1 49.1 200 ## 4 3081. 45.0 3.99 41.0 49.0 200 ## 5 2941. 45.0 3.96 41.0 48.9 200 ## 6 2808. 45.0 3.96 41.0 48.9 200 ## 7 2680. 45.0 3.95 41.0 48.9 200 ## 8 2558. 44.9 3.95 41.0 48.9 200 ## 9 2442. 44.9 3.95 41.0 48.9 200 ## 10 2331. 44.9 3.95 41.0 48.9 200 ## # … with 90 more rows 14.7 Lasso 14.8 boston is boring bstn = MASS::Boston bstn$chas = factor(bstn$chas) bstn$rad = factor(bstn$rad) levels(bstn$chas) ## [1] &quot;0&quot; &quot;1&quot; levels(bstn$rad) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;24&quot; lm(medv ~ ., data = bstn) ## ## Call: ## lm(formula = medv ~ ., data = bstn) ## ## Coefficients: ## (Intercept) crim zn indus chas1 ## 35.259615 -0.108821 0.054896 0.023760 2.524163 ## nox rm age dis rad2 ## -17.573132 3.665491 0.000461 -1.554546 1.488905 ## rad3 rad4 rad5 rad6 rad7 ## 4.681253 2.576234 2.918493 1.185839 4.878992 ## rad8 rad24 tax ptratio black ## 4.839836 7.461674 -0.008748 -0.972419 0.009394 ## lstat ## -0.529226 head(as_tibble(model.matrix(lm(medv ~ ., data = bstn)))) ## # A tibble: 6 x 21 ## `(Intercept)` crim zn indus chas1 nox rm age dis rad2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 0 ## 2 1 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 1 ## 3 1 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 1 ## 4 1 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 0 ## 5 1 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 0 ## 6 1 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 0 ## # … with 11 more variables: rad3 &lt;dbl&gt;, rad4 &lt;dbl&gt;, rad5 &lt;dbl&gt;, ## # rad6 &lt;dbl&gt;, rad7 &lt;dbl&gt;, rad8 &lt;dbl&gt;, rad24 &lt;dbl&gt;, tax &lt;dbl&gt;, ## # ptratio &lt;dbl&gt;, black &lt;dbl&gt;, lstat &lt;dbl&gt; bstn_x = model.matrix(lm(medv ~ ., data = bstn)) bstn_y = bstn$medv coef(lm.fit(x = bstn_x, y = bstn_y)) ## (Intercept) crim zn indus chas1 ## 3.525962e+01 -1.088210e-01 5.489638e-02 2.376030e-02 2.524163e+00 ## nox rm age dis rad2 ## -1.757313e+01 3.665491e+00 4.610055e-04 -1.554546e+00 1.488905e+00 ## rad3 rad4 rad5 rad6 rad7 ## 4.681253e+00 2.576234e+00 2.918493e+00 1.185839e+00 4.878992e+00 ## rad8 rad24 tax ptratio black ## 4.839836e+00 7.461674e+00 -8.748175e-03 -9.724194e-01 9.393803e-03 ## lstat ## -5.292258e-01 bstn_x = model.matrix(lm(medv ~ ., data = bstn))[, -1] bstn_y = bstn$medv par(mfrow = c(1, 2)) plot(glmnet(x = bstn_x, y = bstn_y, alpha = 0), xvar = &quot;lambda&quot;) grid() plot(glmnet(x = bstn_x, y = bstn_y, alpha = 1), xvar = &quot;lambda&quot;) grid() par(mfrow = c(1, 2)) plot(cv.glmnet(x = bstn_x, y = bstn_y, alpha = 0)) plot(cv.glmnet(x = bstn_x, y = bstn_y, alpha = 1)) bstn_ridge = cv.glmnet(x = bstn_x, y = bstn_y, alpha = 0) bstn_lasso = cv.glmnet(x = bstn_x, y = bstn_y, alpha = 1) library(&quot;broom&quot;) tidy(bstn_lasso) ## # A tibble: 83 x 6 ## lambda estimate std.error conf.low conf.high nzero ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 6.78 84.3 8.10 76.2 92.4 0 ## 2 6.18 77.2 7.94 69.2 85.1 1 ## 3 5.63 70.4 7.51 62.9 77.9 2 ## 4 5.13 64.0 7.06 57.0 71.1 2 ## 5 4.67 58.5 6.59 51.9 65.0 2 ## 6 4.26 53.8 6.17 47.6 60.0 2 ## 7 3.88 49.9 5.82 44.1 55.8 2 ## 8 3.53 46.7 5.53 41.2 52.3 2 ## 9 3.22 44.1 5.28 38.8 49.3 2 ## 10 2.93 41.6 5.08 36.5 46.7 3 ## # … with 73 more rows glance(bstn_lasso) ## # A tibble: 1 x 2 ## lambda.min lambda.1se ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00694 0.315 # TODO: pull out rows of tidy with the values from glance predict(bstn_lasso, newx = bstn_x[1:10,], type = &quot;link&quot;) ## 1 ## 1 30.39658 ## 2 25.38272 ## 3 31.23894 ## 4 31.17211 ## 5 30.57321 ## 6 27.58938 ## 7 23.54446 ## 8 20.54428 ## 9 12.51911 ## 10 20.51929 predict(bstn_lasso, newx = bstn_x[1:10,], type = &quot;response&quot;) ## 1 ## 1 30.39658 ## 2 25.38272 ## 3 31.23894 ## 4 31.17211 ## 5 30.57321 ## 6 27.58938 ## 7 23.54446 ## 8 20.54428 ## 9 12.51911 ## 10 20.51929 predict(bstn_lasso, type = &quot;coefficients&quot;, s=c(&quot;lambda.1se&quot;,&quot;lambda.min&quot;)) ## 21 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 19.386070871 ## crim -0.027179560 ## zn 0.004205338 ## indus . ## chas1 2.033274012 ## nox -4.744669654 ## rm 4.223773143 ## age . ## dis -0.485529479 ## rad2 . ## rad3 1.353021994 ## rad4 . ## rad5 . ## rad6 -0.286318856 ## rad7 . ## rad8 0.336943107 ## rad24 . ## tax . ## ptratio -0.810303776 ## black 0.006775041 ## lstat -0.519927242 predict(bstn_lasso, type = &quot;nonzero&quot;) ## X1 ## 1 1 ## 2 2 ## 3 4 ## 4 5 ## 5 6 ## 6 8 ## 7 10 ## 8 13 ## 9 15 ## 10 18 ## 11 19 ## 12 20 14.9 some more simulation # diag(100) p = 100 A = matrix(runif(p ^ 2) * 2 - 1, ncol = p) Sigma = t(A) %*% A sample_size = 500 X = MASS::mvrnorm(n = sample_size, mu = rep(0, p), Sigma = Sigma) beta = ifelse(sample(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1), size = p, replace = TRUE), runif(n = p, -1, 1), 0) * 2 y = X %*% beta + rnorm(n = sample_size, sd = 4) fit = glmnet::cv.glmnet(x = X, y = y, alpha = 1) sqrt(min(fit$cvm)) ## [1] 4.086107 plot(fit, xlim = c(-6, 1), ylim = c(15, 20)) # type.measure = &quot;class&quot; TODO: Least Absolute Shrinkage and Selection Operator TODO: https://statisticaloddsandends.wordpress.com/2018/11/15/a-deep-dive-into-glmnet-standardize/ TODO: https://www.jaredlander.com/2018/02/using-coefplot-with-glmnet/ TODO: statistical learning with sparsity book "],
["ensembles.html", "Chapter 15 Ensembles 15.1 STAT 432 Materials 15.2 Bagging 15.3 Random Forest 15.4 Boosting", " Chapter 15 Ensembles 15.1 STAT 432 Materials Slides | Ensemble Methods ISL Readings: Sections 8.1 - 8.2 library(&quot;tibble&quot;) library(&quot;rpart&quot;) library(&quot;rpart.plot&quot;) library(&quot;caret&quot;) library(&quot;purrr&quot;) library(&quot;randomForest&quot;) library(&quot;gbm&quot;) library(&quot;xgboost&quot;) library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) 15.2 Bagging sin_dgp = function(sample_size = 150) { x = runif(n = sample_size, min = -10, max = 10) y = 2 * sin(x) + rnorm(n = sample_size) tibble(x = x, y = y) } set.seed(42) par(mfrow = c(1, 3)) some_data = sin_dgp() plot(some_data, pch = 20, col = &quot;darkgrey&quot;) grid() curve(2 * sin(x), add = TRUE, col = &quot;black&quot;, lwd = 2) fit_1 = rpart(y ~ x, data = some_data, cp = 0) curve(predict(fit_1, tibble(x = x)), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) some_data = sin_dgp() plot(some_data, pch = 20, col = &quot;darkgrey&quot;) grid() curve(2 * sin(x), add = TRUE, col = &quot;black&quot;, lwd = 2) fit_2 = rpart(y ~ x, data = some_data, cp = 0) curve(predict(fit_2, tibble(x = x)), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) some_data = sin_dgp() plot(some_data, pch = 20, col = &quot;darkgrey&quot;) grid() curve(2 * sin(x), add = TRUE, col = &quot;black&quot;, lwd = 2) fit_3 = rpart(y ~ x, data = some_data, cp = 0) curve(predict(fit_3, tibble(x = x)), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) par(mfrow = c(1, 3)) rpart.plot(fit_1) rpart.plot(fit_2) rpart.plot(fit_3) bag_pred = function(x) { apply(t(map_df(boot_reps, predict, data.frame(x = x))), 2, mean) } set.seed(42) boot_idx = caret::createResample(y = some_data$y, times = 100) boot_reps = map(boot_idx, ~ rpart(y ~ x, data = some_data[.x, ], cp = 0)) bag_pred(x = c(-1, 0 , 1)) ## [1] -1.87295394 0.06556445 1.18010317 par(mfrow = c(1, 3)) some_data = sin_dgp() plot(some_data, pch = 20, col = &quot;darkgrey&quot;) grid() curve(2 * sin(x), add = TRUE, col = &quot;black&quot;, lwd = 2) boot_idx = caret::createResample(y = some_data$y, times = 100) boot_reps = map(boot_idx, ~ rpart(y ~ x, data = some_data[.x, ], cp = 0)) curve(bag_pred(x = x), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) some_data = sin_dgp() plot(some_data, pch = 20, col = &quot;darkgrey&quot;) grid() curve(2 * sin(x), add = TRUE, col = &quot;black&quot;, lwd = 2) boot_idx = caret::createResample(y = some_data$y, times = 100) boot_reps = map(boot_idx, ~ rpart(y ~ x, data = some_data[.x, ], cp = 0)) curve(bag_pred(x = x), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) some_data = sin_dgp() plot(some_data, pch = 20, col = &quot;darkgrey&quot;) grid() curve(2 * sin(x), add = TRUE, col = &quot;black&quot;, lwd = 2) boot_idx = caret::createResample(y = some_data$y, times = 100) boot_reps = map(boot_idx, ~ rpart(y ~ x, data = some_data[.x, ], cp = 0)) curve(bag_pred(x = x), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) 15.2.1 Simultation Study new_obs = tibble(x = 0, y = (2 * sin(0))) sim_bagging_vs_single = function() { some_data = sin_dgp() single = predict(rpart(y ~ x, data = some_data, cp = 0), new_obs) boot_idx = caret::createResample(y = some_data$y, times = 100) boot_reps = map(boot_idx, ~ rpart(y ~ x, data = some_data[.x, ], cp = 0)) bagged = mean(map_dbl(boot_reps, predict, new_obs)) c(single = single, bagged = bagged) } set.seed(42) sim_results = replicate(n = 250, sim_bagging_vs_single()) apply(sim_results, 1, mean) ## single.1 bagged ## -0.02487810 0.03302126 apply(sim_results, 1, var) ## single.1 bagged ## 1.0147130 0.3356579 15.3 Random Forest set.seed(42) two_class_data = as_tibble(caret::twoClassSim(n = 1250, noiseVars = 20)) two_class_data ## # A tibble: 1,250 x 36 ## TwoFactor1 TwoFactor2 Linear01 Linear02 Linear03 Linear04 Linear05 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.68 0.843 0.617 0.761 0.0712 -0.368 0.294 ## 2 -0.496 -0.955 -0.00454 -0.172 0.970 -1.28 -0.404 ## 3 -0.577 1.51 -0.0913 -0.265 0.310 -1.98 -2.18 ## 4 1.06 0.567 0.400 -0.424 -0.140 -1.24 0.198 ## 5 1.21 -0.171 0.589 0.689 -0.326 -0.541 1.92 ## 6 -0.161 -0.112 -0.0169 1.05 -0.119 -0.219 -0.928 ## 7 1.20 2.68 0.419 -2.19 0.894 -1.30 -0.329 ## 8 0.812 -1.06 0.801 -0.503 0.211 1.45 0.952 ## 9 2.74 2.44 0.680 0.880 -0.489 -0.197 1.64 ## 10 -0.357 0.196 1.31 1.55 -0.220 0.891 -0.764 ## # … with 1,240 more rows, and 29 more variables: Linear06 &lt;dbl&gt;, ## # Linear07 &lt;dbl&gt;, Linear08 &lt;dbl&gt;, Linear09 &lt;dbl&gt;, Linear10 &lt;dbl&gt;, ## # Nonlinear1 &lt;dbl&gt;, Nonlinear2 &lt;dbl&gt;, Nonlinear3 &lt;dbl&gt;, Noise01 &lt;dbl&gt;, ## # Noise02 &lt;dbl&gt;, Noise03 &lt;dbl&gt;, Noise04 &lt;dbl&gt;, Noise05 &lt;dbl&gt;, ## # Noise06 &lt;dbl&gt;, Noise07 &lt;dbl&gt;, Noise08 &lt;dbl&gt;, Noise09 &lt;dbl&gt;, ## # Noise10 &lt;dbl&gt;, Noise11 &lt;dbl&gt;, Noise12 &lt;dbl&gt;, Noise13 &lt;dbl&gt;, ## # Noise14 &lt;dbl&gt;, Noise15 &lt;dbl&gt;, Noise16 &lt;dbl&gt;, Noise17 &lt;dbl&gt;, ## # Noise18 &lt;dbl&gt;, Noise19 &lt;dbl&gt;, Noise20 &lt;dbl&gt;, Class &lt;fct&gt; fit = randomForest(Class ~ ., data = two_class_data) fit ## ## Call: ## randomForest(formula = Class ~ ., data = two_class_data) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 5 ## ## OOB estimate of error rate: 18.24% ## Confusion matrix: ## Class1 Class2 class.error ## Class1 562 96 0.1458967 ## Class2 132 460 0.2229730 all.equal(predict(fit), predict(fit, two_class_data)) ## [1] &quot;228 string mismatches&quot; tibble( &quot;Training Observation&quot; = 1:10, &quot;OOB Predictions&quot; = head(predict(fit), n = 10), &quot;Full Forest Predictions&quot; = head(predict(fit, two_class_data), n = 10) ) %&gt;% kable() %&gt;% kable_styling(&quot;striped&quot;, full_width = FALSE) Training Observation OOB Predictions Full Forest Predictions 1 Class1 Class1 2 Class1 Class1 3 Class2 Class2 4 Class1 Class1 5 Class2 Class1 6 Class1 Class1 7 Class2 Class2 8 Class1 Class1 9 Class2 Class2 10 Class1 Class1 predict(fit, two_class_data, type = &quot;prob&quot;)[2, ] ## Class1 Class2 ## 0.822 0.178 predict(fit, two_class_data, predict.all = TRUE)$individual[2, ] ## [1] &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [8] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [15] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; ## [22] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [29] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [36] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; ## [43] &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [50] &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [57] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [64] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [71] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [78] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [85] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [92] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [99] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [106] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; ## [113] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [120] &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [127] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; ## [134] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; ## [141] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; ## [148] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [155] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [162] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [169] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; ## [176] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [183] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [190] &quot;Class2&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [197] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [204] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [211] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [218] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class2&quot; ## [225] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [232] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; ## [239] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [246] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [253] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; ## [260] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [267] &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; ## [274] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [281] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [288] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [295] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; ## [302] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [309] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; ## [316] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [323] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; ## [330] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [337] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [344] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [351] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [358] &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; ## [365] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [372] &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class2&quot; ## [379] &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; ## [386] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class2&quot; ## [393] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [400] &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class2&quot; ## [407] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; ## [414] &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [421] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [428] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; ## [435] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [442] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [449] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; &quot;Class1&quot; ## [456] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [463] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; ## [470] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class2&quot; &quot;Class2&quot; ## [477] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [484] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [491] &quot;Class1&quot; &quot;Class2&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; ## [498] &quot;Class1&quot; &quot;Class1&quot; &quot;Class1&quot; table(predict(fit, two_class_data, predict.all = TRUE)$individual[2, ]) ## ## Class1 Class2 ## 411 89 table(predict(fit, two_class_data, predict.all = TRUE)$individual[2, ]) / 500 ## ## Class1 Class2 ## 0.822 0.178 mean(fit$oob.times / 500) ## [1] 0.368216 exp(-1) ## [1] 0.3678794 fit$importance ## MeanDecreaseGini ## TwoFactor1 70.393099 ## TwoFactor2 69.608199 ## Linear01 11.352368 ## Linear02 38.903234 ## Linear03 34.207241 ## Linear04 25.245839 ## Linear05 24.898924 ## Linear06 25.374816 ## Linear07 17.307651 ## Linear08 17.704504 ## Linear09 13.516845 ## Linear10 12.400794 ## Nonlinear1 14.781874 ## Nonlinear2 9.795264 ## Nonlinear3 13.552729 ## Noise01 10.566502 ## Noise02 10.898295 ## Noise03 11.989430 ## Noise04 11.418045 ## Noise05 10.899319 ## Noise06 10.821215 ## Noise07 11.185296 ## Noise08 10.604101 ## Noise09 11.301260 ## Noise10 11.191305 ## Noise11 11.084711 ## Noise12 11.121672 ## Noise13 11.678859 ## Noise14 10.835472 ## Noise15 11.372321 ## Noise16 10.558511 ## Noise17 10.862320 ## Noise18 12.148751 ## Noise19 11.807009 ## Noise20 11.396611 varImpPlot(fit) fit_caret_rf = train(Class ~ ., data = two_class_data, trControl = trainControl(method = &quot;oob&quot;)) fit_caret_rf ## Random Forest ## ## 1250 samples ## 35 predictor ## 2 classes: &#39;Class1&#39;, &#39;Class2&#39; ## ## No pre-processing ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.8168 0.6309813 ## 18 0.8304 0.6592744 ## 35 0.8288 0.6564099 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 18. 15.4 Boosting sin_dgp = function(sample_size = 150) { x = runif(n = sample_size, min = -10, max = 10) y = 2 * sin(x) + rnorm(n = sample_size) tibble(x = x, y = y) } set.seed(42) sim_data = sin_dgp() plot(sim_data, ylim = c(-6, 6), pch = 20, col = &quot;darkgrey&quot;) grid() curve(2 * sin(x), col = &quot;black&quot;, add = TRUE, lwd = 2) sim_data_for_boosting = sim_data par(mfrow = c(1, 3)) splits = 99 while(splits &gt; 0) { plot(sim_data_for_boosting, ylim = c(-6, 6), pch = 20, col = &quot;darkgrey&quot;) grid() fit = rpart(y ~ x, data = sim_data_for_boosting, maxdepth = 2) curve(predict(fit, data.frame(x = x)), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) sim_data_for_boosting$y = sim_data_for_boosting$y - 0.4 * predict(fit) splits = nrow(fit$frame) - 1 } sim_data_for_boosting = sim_data tree_list = list() for (i in 1:100) { fit = rpart(y ~ x, data = sim_data_for_boosting, maxdepth = 2) tree_list[[i]] = fit sim_data_for_boosting$y = sim_data_for_boosting$y - 0.4 * predict(fit) } names(tree_list) = 1:100 boost_pred = function(x) { apply(t(map_df(tree_list, predict, data.frame(x = x))), 2, function(x) {0.4 * sum(x)}) } plot(sim_data, ylim = c(-6, 6), pch = 20, col = &quot;darkgrey&quot;) grid() curve(boost_pred(x), add = TRUE, lwd = 2, col = &quot;dodgerblue&quot;) curve(2 * sin(x), add = TRUE, col = &quot;black&quot;, lwd = 2) fit_caret_gbm = train(Class ~ ., data = two_class_data, method = &quot;gbm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), verbose = FALSE) fit_caret_gbm ## Stochastic Gradient Boosting ## ## 1250 samples ## 35 predictor ## 2 classes: &#39;Class1&#39;, &#39;Class2&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1000, 1001, 999, 1000, 1000 ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.7791409 0.5535950 ## 1 100 0.8223827 0.6422082 ## 1 150 0.8384244 0.6751165 ## 2 50 0.8263923 0.6510654 ## 2 100 0.8504245 0.6995372 ## 2 150 0.8496213 0.6980252 ## 3 50 0.8408117 0.6802285 ## 3 100 0.8576086 0.7140311 ## 3 150 0.8640214 0.7270698 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 150, ## interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10. fit_caret_xgb = train(Class ~ ., data = two_class_data, method = &quot;xgbTree&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), tuneLength = 2) fit_caret_xgb ## eXtreme Gradient Boosting ## ## 1250 samples ## 35 predictor ## 2 classes: &#39;Class1&#39;, &#39;Class2&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1000, 1001, 999, 1000, 1000 ## Resampling results across tuning parameters: ## ## eta max_depth colsample_bytree subsample nrounds Accuracy ## 0.3 1 0.6 0.5 50 0.8392214 ## 0.3 1 0.6 0.5 100 0.8544055 ## 0.3 1 0.6 1.0 50 0.8296052 ## 0.3 1 0.6 1.0 100 0.8464150 ## 0.3 1 0.8 0.5 50 0.8360308 ## 0.3 1 0.8 0.5 100 0.8512021 ## 0.3 1 0.8 1.0 50 0.8288276 ## 0.3 1 0.8 1.0 100 0.8512214 ## 0.3 2 0.6 0.5 50 0.8488278 ## 0.3 2 0.6 0.5 100 0.8536214 ## 0.3 2 0.6 1.0 50 0.8464213 ## 0.3 2 0.6 1.0 100 0.8576247 ## 0.3 2 0.8 0.5 50 0.8456597 ## 0.3 2 0.8 0.5 100 0.8592150 ## 0.3 2 0.8 1.0 50 0.8600023 ## 0.3 2 0.8 1.0 100 0.8631991 ## 0.4 1 0.6 0.5 50 0.8384118 ## 0.4 1 0.6 0.5 100 0.8447894 ## 0.4 1 0.6 1.0 50 0.8384277 ## 0.4 1 0.6 1.0 100 0.8584119 ## 0.4 1 0.8 0.5 50 0.8472086 ## 0.4 1 0.8 0.5 100 0.8599926 ## 0.4 1 0.8 1.0 50 0.8432149 ## 0.4 1 0.8 1.0 100 0.8656152 ## 0.4 2 0.6 0.5 50 0.8400150 ## 0.4 2 0.6 0.5 100 0.8400117 ## 0.4 2 0.6 1.0 50 0.8440118 ## 0.4 2 0.6 1.0 100 0.8472470 ## 0.4 2 0.8 0.5 50 0.8400278 ## 0.4 2 0.8 0.5 100 0.8448117 ## 0.4 2 0.8 1.0 50 0.8520055 ## 0.4 2 0.8 1.0 100 0.8520119 ## Kappa ## 0.6769540 ## 0.7078959 ## 0.6565702 ## 0.6910691 ## 0.6705396 ## 0.7012276 ## 0.6551720 ## 0.7010765 ## 0.6965017 ## 0.7061904 ## 0.6916100 ## 0.7142662 ## 0.6901581 ## 0.7177094 ## 0.7189731 ## 0.7256871 ## 0.6750451 ## 0.6882491 ## 0.6748621 ## 0.7156206 ## 0.6931865 ## 0.7188617 ## 0.6845350 ## 0.7300746 ## 0.6789671 ## 0.6795340 ## 0.6868629 ## 0.6933817 ## 0.6791399 ## 0.6885384 ## 0.7030056 ## 0.7029730 ## ## Tuning parameter &#39;gamma&#39; was held constant at a value of 0 ## ## Tuning parameter &#39;min_child_weight&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were nrounds = 100, max_depth = 1, ## eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 ## and subsample = 1. "],
["practical-issues.html", "Chapter 16 Practical Issues 16.1 STAT 432 Materials 16.2 Feature Scaling 16.3 Categorical Features", " Chapter 16 Practical Issues 16.1 STAT 432 Materials Suggested Reading: The caret Package: Model Training and Tuning library(&quot;tidyverse&quot;) library(&quot;caret&quot;) library(&quot;rpart&quot;) library(&quot;rpart.plot&quot;) 16.2 Feature Scaling # generate date with x1 and x2 on very different scales gen_reg_data = function(sample_size = 250, beta_1 = 1, beta_2 = 1) { x_1 = runif(n = sample_size, min = 0, max = 1) x_2 = runif(n = sample_size, min = 0, max = 1000) y = beta_1 * x_1 + beta_2 * x_2 + rnorm(n = sample_size) tibble(x_1 = x_1, x_2 = x_2, y = y) } # generate train and test sets set.seed(42) trn = gen_reg_data(beta_1 = 10, beta_2 = 0.001) tst = gen_reg_data(beta_1 = 10, beta_2 = 0.001) trn %&gt;% mutate(x1_scaled = scale(x_1), x2_scaled = scale(x_2)) ## # A tibble: 250 x 5 ## x_1 x_2 y x1_scaled[,1] x2_scaled[,1] ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.915 334. 8.39 1.39 -0.462 ## 2 0.937 188. 9.61 1.47 -0.958 ## 3 0.286 270. 1.93 -0.765 -0.681 ## 4 0.830 531. 9.03 1.10 0.207 ## 5 0.642 21.5 7.74 0.455 -1.53 ## 6 0.519 799. 4.96 0.0340 1.12 ## 7 0.737 110. 6.74 0.780 -1.22 ## 8 0.135 540. 1.93 -1.29 0.238 ## 9 0.657 571. 6.12 0.507 0.345 ## 10 0.705 619. 7.29 0.672 0.507 ## # … with 240 more rows # create scaled datasets scale_trn = preProcess(trn[, 1:2]) trn_scaled = predict(scale_trn, trn) tst_scaled = predict(scale_trn, tst) trn_scaled ## # A tibble: 250 x 3 ## x_1 x_2 y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.39 -0.462 8.39 ## 2 1.47 -0.958 9.61 ## 3 -0.765 -0.681 1.93 ## 4 1.10 0.207 9.03 ## 5 0.455 -1.53 7.74 ## 6 0.0340 1.12 4.96 ## 7 0.780 -1.22 6.74 ## 8 -1.29 0.238 1.93 ## 9 0.507 0.345 6.12 ## 10 0.672 0.507 7.29 ## # … with 240 more rows # linear models -&gt; scaling doesn&#39;t matter # knn -&gt; scaling does matter! # tress -&gt; scaling doesn&#39;t matter lm_u = lm(y ~ ., data = trn) lm_s = lm(y ~ ., data = trn_scaled) coef(lm_u) ## (Intercept) x_1 x_2 ## -0.0816891578 10.0859803604 0.0009962049 coef(lm_s) ## (Intercept) x_1 x_2 ## 5.5221016 2.9395824 0.2928511 head(cbind(predict(lm_u, tst), predict(lm_s, tst_scaled))) ## [,1] [,2] ## 1 9.2073071 9.2073071 ## 2 1.3001017 1.3001017 ## 3 8.5186045 8.5186045 ## 4 5.4156184 5.4156184 ## 5 5.6232737 5.6232737 ## 6 0.1839076 0.1839076 all(predict(lm_u, tst) == predict(lm_s, tst_scaled)) ## [1] FALSE identical(predict(lm_u, tst), predict(lm_s, tst_scaled)) ## [1] FALSE all.equal(predict(lm_u, tst), predict(lm_s, tst_scaled)) ## [1] TRUE knn_u = knnreg(y ~ ., data = trn) knn_s = knnreg(y ~ ., data = trn_scaled) all.equal(predict(knn_u, tst), predict(knn_s, tst_scaled)) ## [1] &quot;Mean relative difference: 0.4687973&quot; tree_u = rpart(y ~ ., data = trn) tree_s = rpart(y ~ ., data = trn_scaled) identical(predict(tree_u, tst), predict(tree_s, tst_scaled)) #! ## [1] TRUE all.equal(predict(tree_u, tst), predict(tree_s, tst_scaled)) ## [1] TRUE set.seed(42) fit_caret_unscaled = train( y ~ ., data = trn, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5) ) fit_caret_unscaled ## k-Nearest Neighbors ## ## 250 samples ## 2 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 200, 200, 200, 200, 200 ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 3.470440 0.018004804 2.851823 ## 7 3.261563 0.007334744 2.694560 ## 9 3.254494 0.016335313 2.685868 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 9. set.seed(42) fit_caret_scaled = train( y ~ ., data = trn, method = &quot;knn&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 5)) fit_caret_scaled ## k-Nearest Neighbors ## ## 250 samples ## 2 predictor ## ## Pre-processing: centered (2), scaled (2) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 200, 200, 200, 200, 200 ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 1.169505 0.8571820 0.9371406 ## 7 1.119903 0.8693966 0.9064097 ## 9 1.088236 0.8775694 0.8802256 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 9. predict(fit_caret_scaled, tst[1:10, ]) ## [1] 8.9527676 1.5439899 8.1698569 5.1921137 5.6422941 0.9428402 5.8606918 ## [8] 7.5202928 3.0247919 8.7029136 # re-generate train and test sets set.seed(42) trn = gen_reg_data(beta_1 = 1, beta_2 = 1) tst = gen_reg_data(beta_1 = 1, beta_2 = 1) set.seed(42) fit_caret_unscaled = train( y ~ ., data = trn, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5) ) fit_caret_unscaled ## k-Nearest Neighbors ## ## 250 samples ## 2 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 200, 200, 200, 200, 200 ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 3.938920 0.9998145 2.835701 ## 7 4.664098 0.9997504 3.370510 ## 9 5.315904 0.9996846 3.924115 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 5. set.seed(42) fit_caret_scaled = train( y ~ ., data = trn, method = &quot;knn&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 5)) fit_caret_scaled ## k-Nearest Neighbors ## ## 250 samples ## 2 predictor ## ## Pre-processing: centered (2), scaled (2) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 200, 200, 200, 200, 200 ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 27.58391 0.9914624 21.49055 ## 7 28.15013 0.9915600 21.19110 ## 9 28.90703 0.9914946 22.63816 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 5. 16.3 Categorical Features gen_cat_data = function(sample_size = 250) { # generate categorical x data x = sample(LETTERS[1:10], size = sample_size, replace = TRUE) # generate y data, different means for different categories y = case_when( x == &quot;A&quot; ~ rnorm(n = sample_size, mean = 1), x == &quot;B&quot; ~ rnorm(n = sample_size, mean = 1), x == &quot;C&quot; ~ rnorm(n = sample_size, mean = 1), x == &quot;D&quot; ~ rnorm(n = sample_size, mean = 5), x == &quot;E&quot; ~ rnorm(n = sample_size, mean = 5), x == &quot;F&quot; ~ rnorm(n = sample_size, mean = 5), x == &quot;G&quot; ~ rnorm(n = sample_size, mean = 11), x == &quot;H&quot; ~ rnorm(n = sample_size, mean = 11), x == &quot;I&quot; ~ rnorm(n = sample_size, mean = 13), x == &quot;J&quot; ~ rnorm(n = sample_size, mean = 13), ) # return tibble tibble(x = x, y = y) } x_levels = data.frame( x = LETTERS[1:10] ) cat_trn = gen_cat_data() head(cat_trn, n = 10) ## # A tibble: 10 x 2 ## x y ## &lt;chr&gt; &lt;dbl&gt; ## 1 J 12.0 ## 2 F 3.49 ## 3 G 11.0 ## 4 E 2.15 ## 5 E 3.36 ## 6 C 1.43 ## 7 F 4.67 ## 8 E 6.18 ## 9 I 13.4 ## 10 B 0.806 lm(y ~ x, data = cat_trn) ## ## Call: ## lm(formula = y ~ x, data = cat_trn) ## ## Coefficients: ## (Intercept) xB xC xD xE ## 0.66805 0.04599 0.38439 4.38387 4.12645 ## xF xG xH xI xJ ## 4.35950 10.15284 10.11491 12.27431 12.45589 rpart.plot(rpart(y ~ x, data = cat_trn)) knn_cat_mod = knnreg(y ~ x, data = cat_trn, use.all = TRUE) predict(knn_cat_mod, x_levels) ## [1] 0.6680481 0.7140409 1.0524340 5.0519187 4.7944962 5.0275456 ## [7] 10.8208879 10.7829575 12.9423601 13.1239336 tree_cat_mod = rpart(y ~ x, data = cat_trn, cp = 0) predict(tree_cat_mod, x_levels) ## 1 2 3 4 5 6 ## 0.6680481 0.7140409 1.0524340 5.0519187 4.7944962 5.0275456 ## 7 8 9 10 ## 10.8208879 10.7829575 12.9423601 13.1239336 all.equal(predict(knn_cat_mod, cat_trn), predict(rpart(y ~ x, data = cat_trn, cp = 0)), check.attributes = FALSE) ## [1] TRUE lm_cat_mod = lm(y ~ x, data = cat_trn) predict(lm_cat_mod, x_levels) ## 1 2 3 4 5 6 ## 0.6680481 0.7140409 1.0524340 5.0519187 4.7944962 5.0275456 ## 7 8 9 10 ## 10.8208879 10.7829575 12.9423601 13.1239336 dum_trn = as_tibble(predict(dummyVars(~., data = cat_trn), cat_trn)) lm(y ~ . + 0, data = dum_trn) ## ## Call: ## lm(formula = y ~ . + 0, data = dum_trn) ## ## Coefficients: ## xA xB xC xD xE xF xG xH xI ## 0.668 0.714 1.052 5.052 4.794 5.028 10.821 10.783 12.942 ## xJ ## 13.124 rpart.plot(rpart(y ~ ., data = dum_trn)) "],
["introduction-3.html", "Chapter 17 Introduction", " Chapter 17 Introduction asdf "],
["introduction-4.html", "Chapter 18 Introduction", " Chapter 18 Introduction asdf "],
["resources.html", "Chapter 19 Resources 19.1 Resources 19.2 BSL Idioms 19.3 Common Issues", " Chapter 19 Resources This is not a book about R. It is however, a book that uses R. Because of this, you will need to be familiar with R. The text will point out some thing about R along the way, but some previous study of R is necessary. The following (freely available) readings are highly recommended: Hands-On Programming with R - Garrett Grolemund If you have never used R or RStudio before, Part 1, Chapters 1 - 3, will be useful. R for Data Science - Garrett Grolemund, Hadley Wickham This book helps getting you up to speed working with data in R. While it is a lot of reading, Chapters 1 - 21 are highly recommended. Advanced R - Hadley Wickham Part I, Chapters 1 - 8, of this book will help create a mental model for working with R. These chapters are not an easy read, so they should be returned to often. (Chapter 2 could be safely skipped for our purposes, but is important if you will use R in the long term.) If you are a UIUC student who took the course STAT 420, the first six chapters of that book could serve as a nice refresher. Applied Statistics with R - David Dalpiaz 19.1 Resources The following resources are more specific or more advanced, but could still prove to be useful. 19.1.1 R Efficient R programming R Programming for Data Science R Graphics Cookbook Modern Dive The tidyverse Website dplyr Website readr Website tibble Website forcats Website 19.1.2 RStudio RStudio IDE Cheatsheet RStudio Resources 19.1.3 R Markdown R Markdown Cheatsheet R Markdown: The Definitive Guide - Yihui Xie, J. J. Allaire, Garrett Grolemund R4DS R Markdown Chapter 19.1.3.1 Markdown Daring Fireball - Markdown: Basics GitHub - Mastering Markdown CommonMark 19.2 BSL Idioms Things here supersede everything above. 19.2.1 Reference Style tidyverse Style Guide 19.2.2 BSL Style Overrides TODO: = instead of &lt;- http://thecoatlessprofessor.com/programming/an-opinionated-tale-of-why-you-should-replace---with-/ TODO: never use T or F, only TRUE or FALSE FALSE == TRUE ## [1] FALSE F == TRUE ## [1] FALSE F = TRUE F == TRUE ## [1] TRUE TODO: never ever ever use attach() TODO: never ever ever use &lt;&lt;- TODO: never ever ever use setwd() or set a working directory some other way TODO: a newline before and after any chunk TODO: use headers appropriately! (short names, good structure) TODO: never ever ever put spaces in filenames. use -. (others will use _.) TODO: load all needed packages at the beginning of an analysis in a single chunk (TODO: pros and cons of this approach) TODO: one plot per chunk! no other printed output Be consistent… with yourself! with your group! with your organization! set.seed(1337);mu=10;sample_size=50;samples=100000; x_bars=rep(0, samples) for(i in 1:samples) { x_bars[i]=mean(rpois(sample_size,lambda = mu))} x_bar_hist=hist(x_bars,breaks=50,main=&quot;Histogram of Sample Means&quot;,xlab=&quot;Sample Means&quot;,col=&quot;darkorange&quot;,border = &quot;dodgerblue&quot;) mean(x_bars&gt;mu-2*sqrt(mu)/sqrt(sample_size)&amp;x_bars&lt;mu+2*sqrt(mu)/sqrt(sample_size)) 19.2.3 Objects and Functions To understand computations in R, two slogans are helpful: Everything that exists is an object. Everything that happens is a function call. — John Chambers TODO: Functions + Objects these are the inputs and outputs of functions: functions vectors lists tibbles (dfs) 19.2.4 Print versus Return cars_mod = lm(dist ~ speed, data = cars) summary(cars_mod) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 is.list(summary(cars_mod)) ## [1] TRUE names(summary(cars_mod)) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; str(summary(cars_mod)) ## List of 11 ## $ call : language lm(formula = dist ~ speed, data = cars) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language dist ~ speed ## .. ..- attr(*, &quot;variables&quot;)= language list(dist, speed) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;dist&quot; &quot;speed&quot; ## .. .. .. ..$ : chr &quot;speed&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;speed&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(dist, speed) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;dist&quot; &quot;speed&quot; ## $ residuals : Named num [1:50] 3.85 11.85 -5.95 12.05 2.12 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ coefficients : num [1:2, 1:4] -17.579 3.932 6.758 0.416 -2.601 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot; ## $ aliased : Named logi [1:2] FALSE FALSE ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## $ sigma : num 15.4 ## $ df : int [1:3] 2 48 2 ## $ r.squared : num 0.651 ## $ adj.r.squared: num 0.644 ## $ fstatistic : Named num [1:3] 89.6 1 48 ## ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; ## $ cov.unscaled : num [1:2, 1:2] 0.19311 -0.01124 -0.01124 0.00073 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; # RStudio only View(summary(cars_mod)) 19.2.5 Help TODO: ?, google, stack overflow, (office hours, course forums) 19.2.6 Keyboard Shortcuts TODO: copy-paste, switch program, switch tab, etc… TODO: TAB!!! TODO: new chunk! TODO: style! TODO: keyboard shortcut for keyboard shortcut 19.3 Common Issues TODO: cannot find function called &quot;&quot; TODO: https://stat545.com/ TODO: https://atrebas.github.io/post/2019-01-15-2018-learning/ TODO: https://www.burns-stat.com/pages/Tutor/R_inferno.pdf "],
["simulation-and-bootstrap.html", "Chapter 20 Simulation and Bootstrap 20.1 STAT 432 Materials 20.2 Misc Notes", " Chapter 20 Simulation and Bootstrap 20.1 STAT 432 Materials Slides | Bootstrap Note: In the future this should become two chapters, and be greatly expanded. Perhaps introduce simulation, ECDF, and plug-in-principle very early. library(microbenchmark) # create some data set.seed(1) data = rexp(n = 100, rate = 0.5) # quantities of interest 1 / 0.5 # mean ## [1] 2 qexp(0.5, rate = 0.5) ## [1] 1.386294 pexp(8, rate = 0.5, lower.tail = FALSE) # probability P[X &gt; 8] ## [1] 0.01831564 # estimates using sampled data mean(data) # boring ## [1] 2.061353 median(data) ## [1] 1.617539 mean(data &gt; 8) ## [1] 0.02 # histogram of data with population distribution hist(data, border = &quot;dodgerblue&quot;, col = &quot;grey&quot;, breaks = 20, probability = TRUE, ylim = c(0, 0.5), xlim = c(0, 20)) box() grid() curve(dexp(x, rate = 0.5), add = TRUE, col = &quot;darkorange&quot;, n = 250, lwd = 2) # define graphical parameters ylim = c(0, 0.5) xlim = c(0, 20) # resampling from the true distribution par(mfrow = c(1, 3)) hist(rexp(n = 100, rate = 0.5), border = &quot;dodgerblue&quot;, col = &quot;grey&quot;, breaks = 20, probability = TRUE, ylim = ylim, xlim = xlim, main = &quot;Sample From Population&quot;, xlab = &quot;x&quot;) box() grid() curve(dexp(x, rate = 0.5), add = TRUE, col = &quot;darkorange&quot;) hist(rexp(n = 100, rate = 0.5), border = &quot;dodgerblue&quot;, col = &quot;grey&quot;, breaks = 20, probability = TRUE, ylim = ylim, xlim = xlim, main = &quot;Sample From Population&quot;, xlab = &quot;x&quot;) box() grid() curve(dexp(x, rate = 0.5), add = TRUE, col = &quot;darkorange&quot;) hist(rexp(n = 100, rate = 0.5), border = &quot;dodgerblue&quot;, col = &quot;grey&quot;, breaks = 20, probability = TRUE, ylim = ylim, xlim = xlim, main = &quot;Sample From Population&quot;, xlab = &quot;x&quot;) box() grid() curve(dexp(x, rate = 0.5), add = TRUE, col = &quot;darkorange&quot;) qexp(0.5, rate = 0.5) # true populaiton median ## [1] 1.386294 median(data) # sample median ## [1] 1.617539 # simulating medians # - simulate from known distribution # - calculate median on simualted data # - store result simulated_medians = replicate(n = 10000, median(rexp(n = 100, rate = 0.5))) # use EDCD and plug-in principle to learn about distribution and make estimates mean(simulated_medians) ## [1] 1.397316 sd(simulated_medians) ## [1] 0.1993581 quantile(simulated_medians, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 1.029307 1.807971 # plot of ECDF of sample median hist(simulated_medians, col = &quot;darkgrey&quot;) box() grid() pexp(8, rate = 0.5, lower.tail = FALSE) # true probability P[X &gt; 8] ## [1] 0.01831564 mean(data &gt; 8) # estimate using ECDF of sample data, mean(I(x &gt; 8)) ## [1] 0.02 # simulating estimates of P[X &gt; 8] # - simulate from known distribution # - calculate proportion of obs greater than 8 in simualted data, mean(I(x &gt; 8)) # - store result simulated_probs = replicate(n = 10000, mean(rexp(n = 100, rate = 0.5) &gt; 8)) # use EDCD and plug-in principle to learn about distribution and make estimates mean(simulated_probs) ## [1] 0.018517 sd(simulated_probs) ## [1] 0.01353547 quantile(simulated_probs, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 0.00 0.05 # plot of ECDF of sample median hist(simulated_probs, col = &quot;darkgrey&quot;) box() grid() # bootstrap resampling sample(x = data, replace = TRUE) ## [1] 0.29409198 4.35754514 5.51848758 2.47520710 0.89490379 2.21187254 ## [7] 2.64093586 3.14397369 5.78993707 0.27959052 2.36328556 1.18923530 ## [13] 2.78147026 0.41973316 1.52405971 4.56770695 3.56953081 1.62907161 ## [19] 0.29141345 1.54837553 0.21214525 4.72903051 1.13173105 4.35754514 ## [25] 1.95479161 0.26514281 2.15976227 2.36328556 0.70374100 0.60348187 ## [31] 0.17934816 2.46758302 1.60834182 5.51848758 1.45042861 1.50308538 ## [37] 0.52165649 6.43557804 1.54837553 1.18923530 1.15742493 9.66562549 ## [43] 2.46758302 0.60256599 0.89490379 0.89490379 0.11852241 0.07453705 ## [49] 4.20075446 1.07936568 2.04545175 3.56953081 0.11852241 0.77757354 ## [55] 4.35754514 0.57118197 3.14397369 0.61889571 2.10908633 0.58824078 ## [61] 1.45042861 1.28378518 1.54837553 7.91786570 0.47005490 1.17695944 ## [67] 5.78993707 1.62907161 0.40702070 0.47005490 9.66562549 0.77757354 ## [73] 1.02834859 1.17695944 2.15976227 1.67401298 0.11887832 0.70374100 ## [79] 2.27366283 1.51036367 1.98911158 0.52165649 0.57118197 1.30949327 ## [85] 2.50621071 1.54837553 2.04545175 2.47520710 2.78147026 0.21214525 ## [91] 0.26514281 1.98911158 1.10928280 0.57118197 0.57118197 7.91786570 ## [97] 0.57118197 7.91786570 1.62907161 3.14397369 # bootstrap resample from original data par(mfrow = c(1, 3)) hist(sample(x = data, replace = TRUE), border = &quot;dodgerblue&quot;, col = &quot;grey&quot;, breaks = 20, probability = TRUE, ylim = ylim, xlim = xlim, main = &quot;Bootstrap Resample&quot;, xlab = &quot;x&quot;) box() grid() curve(dexp(x, rate = 0.5), add = TRUE, col = &quot;darkorange&quot;) hist(sample(x = data, replace = TRUE), border = &quot;dodgerblue&quot;, col = &quot;grey&quot;, breaks = 20, probability = TRUE, ylim = ylim, xlim = xlim, main = &quot;Bootstrap Resample&quot;, xlab = &quot;x&quot;) box() grid() curve(dexp(x, rate = 0.5), add = TRUE, col = &quot;darkorange&quot;) hist(sample(x = data, replace = TRUE), border = &quot;dodgerblue&quot;, col = &quot;grey&quot;, breaks = 20, probability = TRUE, ylim = ylim, xlim = xlim, main = &quot;Bootstrap Resample&quot;, xlab = &quot;x&quot;) box() grid() curve(dexp(x, rate = 0.5), add = TRUE, col = &quot;darkorange&quot;) qexp(0.5, rate = 0.5) # true populaiton median ## [1] 1.386294 median(data) # sample median ## [1] 1.617539 # boostrapping medians # - sample with replacement from sample data # - calculate median on resampled data # - store result replicated_medians = replicate(n = 10000, median(sample(data, replace = TRUE))) # use EDCD and plug-in principle to learn about distribution and make estimates mean(replicated_medians) ## [1] 1.626611 sd(replicated_medians) ## [1] 0.2100116 # a 95% CI for the true median quantile(replicated_medians, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 1.189235 2.045452 # plot of ECDF of sample median hist(replicated_medians, col = &quot;darkgrey&quot;) box() grid() pexp(8, rate = 0.5, lower.tail = FALSE) # true probability P[X &gt; 8] ## [1] 0.01831564 mean(data &gt; 8) # estimate using ECDF of sample data, mean(I(x &gt; 8)) ## [1] 0.02 # boostrapping mean(I(x &gt; 8)) # - sample with replacement from sample data # - calculate proportion of obs greater than 8 in simualted data, mean(I(x &gt; 8)) # - store result replicated_probs = replicate(n = 10000, mean(sample(data, replace = TRUE) &gt; 8)) # use EDCD and plug-in principle to learn about distribution and make estimates mean(replicated_probs) ## [1] 0.019998 sd(replicated_probs) ## [1] 0.01390249 # a 95% CI for the true P[X &gt; 8] quantile(replicated_probs, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 0.00 0.05 # plot of ECDF of sample median hist(replicated_probs, col = &quot;darkgrey&quot;) box() grid() # sample from exponential with mean 2 # generate bootstrap replicates of the median # calculate 95% confidence interval use percentile (quantile) method # check if true median is in confidence interval each time (hope for ~95%!) true_median = qexp(0.5, rate = 0.5) # median check_if_in_interval = function() { data = rexp(n = 100, rate = 0.5) replicated_medians = replicate(n = 2000, median(sample(data, replace = TRUE))) interval = quantile(replicated_medians, probs = c(0.025, 0.975)) interval[1] &lt; true_median &amp; true_median &lt; interval[2] } median_in_out = replicate(n = 200, check_if_in_interval()) mean(median_in_out) ## [1] 0.925 microbenchmark(check_if_in_interval()) ## Unit: milliseconds ## expr min lq mean median uq ## check_if_in_interval() 80.59274 87.10038 92.86923 92.61586 96.87218 ## max neval ## 153.976 100 20.2 Misc Notes TODO: https://statistics.stanford.edu/sites/g/files/sbiybj6031/f/BIO%2083.pdf "],
["data-manipulation.html", "Chapter 21 Data Manipulation 21.1 dplyr 21.2 data.table 21.3 Data Splitting with dplyr::anti_join", " Chapter 21 Data Manipulation library(&quot;tidyverse&quot;) 21.1 dplyr The tidyverse Website dplyr Website dplyr Cheat Sheet R4DS: Data Transformation R4DS: Pipes 21.2 data.table data.table Wiki data.table Website data.table Cheat Sheet 21.3 Data Splitting with dplyr::anti_join set.seed(42) # simulate an estimation and validation dataset sim_est = as_tibble(caret::twoClassSim(n = 100)) sim_val = as_tibble(caret::twoClassSim(n = 100)) # merge the rows of the estimation and validation datasets # to create a training dataset sim_trn = sim_est %&gt;% bind_rows(sim_val) # re-split the data to get better proportions, 80-20 split sim_est = sim_trn %&gt;% sample_frac(0.8) sim_val = sim_trn %&gt;% anti_join(sim_est) # split datasets together are not the training dataset (check for order) identical(bind_rows(sim_est, sim_val), sim_trn) ## [1] FALSE # bind_rows(sim_est, sim_val)[1, ] == sim_trn[1, ] # split datasets together contain the same observations as the training dataset setequal(bind_rows(sim_est, sim_val), sim_trn) ## [1] TRUE "],
["introduction-5.html", "Chapter 22 Introduction", " Chapter 22 Introduction asdf "],
["introduction-6.html", "Chapter 23 Introduction", " Chapter 23 Introduction asdf "],
["additional-reading.html", "Chapter 24 Additional Reading 24.1 Books 24.2 Papers 24.3 Blog Posts 24.4 Miscellaneous", " Chapter 24 Additional Reading 24.1 Books An Introduction to Statistical Learning Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani The Elements of Statistical Learning Trevor Hastie, Robert Tibshirani, and Jerome Friedman Mathematics for Machine Learning Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong Understanding Machine Learning: From Theory to Algorithms Shai Shalev-Shwartz and Shai Ben-David Foundations of Machine Learning Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar The caret Package Max Kuhn Feature Engineering and Selection: A Practical Approach for Predictive Models Max Kuhn and Kjell Johnson Applied Predictive Modeling Max Kuhn and Kjell Johnson Machine Learning: A Probabilistic Perspective Kevin Murphy Probability for Statistics and Machine Learning Anirban DasGupta From Linear Models to Machine Learning Norman Matloff 24.2 Papers Do we Need Hundreds of Classifiers to Solve Real WorldClassification Problems? Manuel Fernandez-Delgado, Eva Cernadas, Senen Barro, Dinani Amorim Statistical Modeling: The Two Cultures Leo Breiman 50 Years of Data Science David Donoho 24.3 Blog Posts Peekaboo: Don’t cite the No Free Lunch Theorem 24.4 Miscellaneous Machine Learning verus Statistics, A Glossar Rob Tibshirani "]
]
