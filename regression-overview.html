<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Regression Overview | Basics of Statistical Learning</title>
  <meta name="description" content="Chapter 9 Regression Overview | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.17.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Regression Overview | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://statisticallearning.org/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Regression Overview | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />


<meta name="date" content="2020-02-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="biasvariance-tradeoff.html"/>
<link rel="next" href="the-backlog.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Start Here</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#caveat-emptor"><i class="fa fa-check"></i>Caveat Emptor</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html"><i class="fa fa-check"></i><b>1</b> Ten Simple Rules for Success in STAT 432</a><ul>
<li class="chapter" data-level="1.1" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-1-there-are-no-rules"><i class="fa fa-check"></i><b>1.1</b> Rule 1: There Are No Rules</a></li>
<li class="chapter" data-level="1.2" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-2-read-the-syllabus"><i class="fa fa-check"></i><b>1.2</b> Rule 2: Read the Syllabus</a></li>
<li class="chapter" data-level="1.3" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-3-previous-learning-is-not-gospel"><i class="fa fa-check"></i><b>1.3</b> Rule 3: Previous Learning is Not Gospel</a></li>
<li class="chapter" data-level="1.4" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-4-all-statements-are-true"><i class="fa fa-check"></i><b>1.4</b> Rule 4: All Statements Are True</a></li>
<li class="chapter" data-level="1.5" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-5-dont-miss-the-forest-for-the-trees"><i class="fa fa-check"></i><b>1.5</b> Rule 5: Don’t Miss The Forest For The Trees</a></li>
<li class="chapter" data-level="1.6" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-6-you-will-struggle"><i class="fa fa-check"></i><b>1.6</b> Rule 6: You Will Struggle</a></li>
<li class="chapter" data-level="1.7" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-7-keep-it-simple"><i class="fa fa-check"></i><b>1.7</b> Rule 7: Keep It Simple</a></li>
<li class="chapter" data-level="1.8" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-8-rtfm"><i class="fa fa-check"></i><b>1.8</b> Rule 8: RTFM</a></li>
<li class="chapter" data-level="1.9" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-9-there-are-no-stupid-questions"><i class="fa fa-check"></i><b>1.9</b> Rule 9: There Are No Stupid Questions</a></li>
<li class="chapter" data-level="1.10" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#rule-10-learn-by-doing"><i class="fa fa-check"></i><b>1.10</b> Rule 10: Learn By Doing</a></li>
<li class="chapter" data-level="1.11" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#conclusion"><i class="fa fa-check"></i><b>1.11</b> Conclusion</a></li>
<li class="chapter" data-level="1.12" data-path="ten-simple-rules-for-success-in-stat-432.html"><a href="ten-simple-rules-for-success-in-stat-432.html#source"><i class="fa fa-check"></i><b>1.12</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html"><i class="fa fa-check"></i><b>2</b> Machine Learning Overview</a><ul>
<li class="chapter" data-level="2.1" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html#reading"><i class="fa fa-check"></i><b>2.1</b> Reading</a></li>
<li class="chapter" data-level="2.2" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.2</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.3" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html#machine-learning-tasks"><i class="fa fa-check"></i><b>2.3</b> Machine Learning Tasks</a><ul>
<li class="chapter" data-level="2.3.1" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html#supervised-learning"><i class="fa fa-check"></i><b>2.3.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.3.2" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html#open-questions"><i class="fa fa-check"></i><b>2.4</b> Open Questions</a></li>
<li class="chapter" data-level="2.5" data-path="machine-learning-overview.html"><a href="machine-learning-overview.html#source-1"><i class="fa fa-check"></i><b>2.5</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="computing.html"><a href="computing.html"><i class="fa fa-check"></i><b>3</b> Computing</a><ul>
<li class="chapter" data-level="3.1" data-path="computing.html"><a href="computing.html#reading-1"><i class="fa fa-check"></i><b>3.1</b> Reading</a></li>
<li class="chapter" data-level="3.2" data-path="computing.html"><a href="computing.html#additional-resources"><i class="fa fa-check"></i><b>3.2</b> Additional Resources</a><ul>
<li class="chapter" data-level="3.2.1" data-path="computing.html"><a href="computing.html#r"><i class="fa fa-check"></i><b>3.2.1</b> R</a></li>
<li class="chapter" data-level="3.2.2" data-path="computing.html"><a href="computing.html#rstudio"><i class="fa fa-check"></i><b>3.2.2</b> RStudio</a></li>
<li class="chapter" data-level="3.2.3" data-path="computing.html"><a href="computing.html#r-markdown"><i class="fa fa-check"></i><b>3.2.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="computing.html"><a href="computing.html#stat-432-idioms"><i class="fa fa-check"></i><b>3.3</b> STAT 432 Idioms</a><ul>
<li class="chapter" data-level="3.3.1" data-path="computing.html"><a href="computing.html#dont-restore-old-workspaces"><i class="fa fa-check"></i><b>3.3.1</b> Don’t Restore Old Workspaces</a></li>
<li class="chapter" data-level="3.3.2" data-path="computing.html"><a href="computing.html#r-versions"><i class="fa fa-check"></i><b>3.3.2</b> R Versions</a></li>
<li class="chapter" data-level="3.3.3" data-path="computing.html"><a href="computing.html#code-style"><i class="fa fa-check"></i><b>3.3.3</b> Code Style</a></li>
<li class="chapter" data-level="3.3.4" data-path="computing.html"><a href="computing.html#reference-style"><i class="fa fa-check"></i><b>3.3.4</b> Reference Style</a></li>
<li class="chapter" data-level="3.3.5" data-path="computing.html"><a href="computing.html#stat-432-r-style-overrides"><i class="fa fa-check"></i><b>3.3.5</b> STAT 432 R Style Overrides</a></li>
<li class="chapter" data-level="3.3.6" data-path="computing.html"><a href="computing.html#stat-432-r-markdown-style"><i class="fa fa-check"></i><b>3.3.6</b> STAT 432 R Markdown Style</a></li>
<li class="chapter" data-level="3.3.7" data-path="computing.html"><a href="computing.html#style-heuristics"><i class="fa fa-check"></i><b>3.3.7</b> Style Heuristics</a></li>
<li class="chapter" data-level="3.3.8" data-path="computing.html"><a href="computing.html#objects-and-functions"><i class="fa fa-check"></i><b>3.3.8</b> Objects and Functions</a></li>
<li class="chapter" data-level="3.3.9" data-path="computing.html"><a href="computing.html#print-versus-return"><i class="fa fa-check"></i><b>3.3.9</b> Print versus Return</a></li>
<li class="chapter" data-level="3.3.10" data-path="computing.html"><a href="computing.html#help"><i class="fa fa-check"></i><b>3.3.10</b> Help</a></li>
<li class="chapter" data-level="3.3.11" data-path="computing.html"><a href="computing.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>3.3.11</b> Keyboard Shortcuts</a></li>
<li class="chapter" data-level="3.3.12" data-path="computing.html"><a href="computing.html#common-issues"><i class="fa fa-check"></i><b>3.3.12</b> Common Issues</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="computing.html"><a href="computing.html#source-2"><i class="fa fa-check"></i><b>3.4</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#reading-2"><i class="fa fa-check"></i><b>4.1</b> Reading</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>4.2</b> Probability Models</a></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>4.3</b> Probability Axioms</a></li>
<li class="chapter" data-level="4.4" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>4.4</b> Probability Rules</a></li>
<li class="chapter" data-level="4.5" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>4.5</b> Random Variables</a><ul>
<li class="chapter" data-level="4.5.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>4.5.1</b> Distributions</a></li>
<li class="chapter" data-level="4.5.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>4.5.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="4.5.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>4.5.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="4.5.4" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>4.5.4</b> Distributions in R</a></li>
<li class="chapter" data-level="4.5.5" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>4.5.5</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>4.6</b> Expectations</a></li>
<li class="chapter" data-level="4.7" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>4.7</b> Likelihood</a></li>
<li class="chapter" data-level="4.8" data-path="probability.html"><a href="probability.html#references"><i class="fa fa-check"></i><b>4.8</b> References</a><ul>
<li class="chapter" data-level="4.8.1" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>4.8.1</b> Videos</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="probability.html"><a href="probability.html#source-3"><i class="fa fa-check"></i><b>4.9</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>5</b> Statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="statistics.html"><a href="statistics.html#reading-3"><i class="fa fa-check"></i><b>5.1</b> Reading</a></li>
<li class="chapter" data-level="5.2" data-path="statistics.html"><a href="statistics.html#statistics-1"><i class="fa fa-check"></i><b>5.2</b> Statistics</a></li>
<li class="chapter" data-level="5.3" data-path="statistics.html"><a href="statistics.html#estimators"><i class="fa fa-check"></i><b>5.3</b> Estimators</a><ul>
<li class="chapter" data-level="5.3.1" data-path="statistics.html"><a href="statistics.html#properties"><i class="fa fa-check"></i><b>5.3.1</b> Properties</a></li>
<li class="chapter" data-level="5.3.2" data-path="statistics.html"><a href="statistics.html#example-mse-of-an-estimator"><i class="fa fa-check"></i><b>5.3.2</b> Example: MSE of an Estimator</a></li>
<li class="chapter" data-level="5.3.3" data-path="statistics.html"><a href="statistics.html#estimation-methods"><i class="fa fa-check"></i><b>5.3.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="5.3.4" data-path="statistics.html"><a href="statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.3.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="5.3.5" data-path="statistics.html"><a href="statistics.html#method-of-moments"><i class="fa fa-check"></i><b>5.3.5</b> Method of Moments</a></li>
<li class="chapter" data-level="5.3.6" data-path="statistics.html"><a href="statistics.html#empirical-distribution-function"><i class="fa fa-check"></i><b>5.3.6</b> Empirical Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="statistics.html"><a href="statistics.html#source-4"><i class="fa fa-check"></i><b>5.4</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>6</b> Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-regression.html"><a href="linear-regression.html#reading-4"><i class="fa fa-check"></i><b>6.1</b> Reading</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression.html"><a href="linear-regression.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>6.2</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression.html"><a href="linear-regression.html#setup"><i class="fa fa-check"></i><b>6.3</b> Setup</a></li>
<li class="chapter" data-level="6.4" data-path="linear-regression.html"><a href="linear-regression.html#mathematical-setup"><i class="fa fa-check"></i><b>6.4</b> Mathematical Setup</a></li>
<li class="chapter" data-level="6.5" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>6.5</b> Linear Regression Models</a></li>
<li class="chapter" data-level="6.6" data-path="linear-regression.html"><a href="linear-regression.html#using-lm"><i class="fa fa-check"></i><b>6.6</b> Using <code>lm()</code></a></li>
<li class="chapter" data-level="6.7" data-path="linear-regression.html"><a href="linear-regression.html#the-predict-function"><i class="fa fa-check"></i><b>6.7</b> The <code>predict()</code> Function</a></li>
<li class="chapter" data-level="6.8" data-path="linear-regression.html"><a href="linear-regression.html#data-splitting"><i class="fa fa-check"></i><b>6.8</b> Data Splitting</a></li>
<li class="chapter" data-level="6.9" data-path="linear-regression.html"><a href="linear-regression.html#regression-metrics"><i class="fa fa-check"></i><b>6.9</b> Regression Metrics</a><ul>
<li class="chapter" data-level="6.9.1" data-path="linear-regression.html"><a href="linear-regression.html#graphical-evaluation"><i class="fa fa-check"></i><b>6.9.1</b> Graphical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="linear-regression.html"><a href="linear-regression.html#example-simple-simulated-data"><i class="fa fa-check"></i><b>6.10</b> Example: “Simple” Simulated Data</a></li>
<li class="chapter" data-level="6.11" data-path="linear-regression.html"><a href="linear-regression.html#example-diamonds-data"><i class="fa fa-check"></i><b>6.11</b> Example: Diamonds Data</a></li>
<li class="chapter" data-level="6.12" data-path="linear-regression.html"><a href="linear-regression.html#example-credit-card-data"><i class="fa fa-check"></i><b>6.12</b> Example: Credit Card Data</a></li>
<li class="chapter" data-level="6.13" data-path="linear-regression.html"><a href="linear-regression.html#source-5"><i class="fa fa-check"></i><b>6.13</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>7</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#reading-5"><i class="fa fa-check"></i><b>7.1</b> Reading</a></li>
<li class="chapter" data-level="7.2" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#mathematical-setup-1"><i class="fa fa-check"></i><b>7.2</b> Mathematical Setup</a></li>
<li class="chapter" data-level="7.3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>7.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="7.4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#decision-trees"><i class="fa fa-check"></i><b>7.4</b> Decision Trees</a></li>
<li class="chapter" data-level="7.5" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#example-credit-card-data-1"><i class="fa fa-check"></i><b>7.5</b> Example: Credit Card Data</a></li>
<li class="chapter" data-level="7.6" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#source-6"><i class="fa fa-check"></i><b>7.6</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html"><i class="fa fa-check"></i><b>8</b> Bias–Variance Tradeoff</a><ul>
<li class="chapter" data-level="8.1" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>8.1</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="8.2" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>8.2</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="8.3" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#simulation"><i class="fa fa-check"></i><b>8.3</b> Simulation</a></li>
<li class="chapter" data-level="8.4" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>8.4</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="8.5" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#source-7"><i class="fa fa-check"></i><b>8.5</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-overview.html"><a href="regression-overview.html"><i class="fa fa-check"></i><b>9</b> Regression Overview</a><ul>
<li class="chapter" data-level="9.1" data-path="regression-overview.html"><a href="regression-overview.html#goal"><i class="fa fa-check"></i><b>9.1</b> Goal</a></li>
<li class="chapter" data-level="9.2" data-path="regression-overview.html"><a href="regression-overview.html#strategy"><i class="fa fa-check"></i><b>9.2</b> Strategy</a></li>
<li class="chapter" data-level="9.3" data-path="regression-overview.html"><a href="regression-overview.html#models"><i class="fa fa-check"></i><b>9.3</b> Models</a></li>
<li class="chapter" data-level="9.4" data-path="regression-overview.html"><a href="regression-overview.html#model-flexibility"><i class="fa fa-check"></i><b>9.4</b> Model Flexibility</a></li>
<li class="chapter" data-level="9.5" data-path="regression-overview.html"><a href="regression-overview.html#overfitting"><i class="fa fa-check"></i><b>9.5</b> Overfitting</a></li>
<li class="chapter" data-level="9.6" data-path="regression-overview.html"><a href="regression-overview.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>9.6</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="9.7" data-path="regression-overview.html"><a href="regression-overview.html#source-8"><i class="fa fa-check"></i><b>9.7</b> Source</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="the-backlog.html"><a href="the-backlog.html"><i class="fa fa-check"></i><b>A</b> The Backlog</a><ul>
<li class="chapter" data-level="A.1" data-path="the-backlog.html"><a href="the-backlog.html#sorted"><i class="fa fa-check"></i><b>A.1</b> Sorted</a></li>
<li class="chapter" data-level="A.2" data-path="the-backlog.html"><a href="the-backlog.html#unsorted"><i class="fa fa-check"></i><b>A.2</b> Unsorted</a></li>
<li class="chapter" data-level="A.3" data-path="the-backlog.html"><a href="the-backlog.html#source-9"><i class="fa fa-check"></i><b>A.3</b> Source</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://daviddalpiaz.org" target="blank">&copy; 2020 David Dalpiaz</a></li>
<li><a href="https://spring-2020.stat432.org" target="blank">[STAT 432] Spring 2020</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-overview" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Regression Overview</h1>
<hr />
<p>This chapter will provide an overview of the <strong>regression</strong> concepts that we have learned thus far. It will also serve to outline the general concepts of <strong>supervised learning</strong> which will also apply to our next task, <em>classification.</em></p>
<p>The information here may eventually be merged into the previous chapter, but for now it is separated for clarify. (Also, much of this information already appears there.)</p>
<hr />
<div id="goal" class="section level2">
<h2><span class="header-section-number">9.1</span> Goal</h2>
<p>What is the goal of regression models in the context of machine learning? We can discuss it in two ways:</p>
<ul>
<li>Make <strong>predictions</strong> on <em>unseen data</em>.</li>
<li>Estimate the <strong>regression function</strong>, which under squared error loss is the <em>conditional mean</em> of <span class="math inline">\(Y\)</span>, the response, given <span class="math inline">\(X\)</span>, the features.</li>
</ul>
<p>These goal are essentially the same. We want to fit a model that “generalizes” well, that is, works well on unseen data. To do this, we want to use a model of appropriate <strong>flexibility</strong> so as not to <strong>overfit</strong> to the train data. In other words, we want to train a models that learns the <strong>signal</strong>, the regression function, and not the <strong>noise</strong>, the random variation in the training data.</p>
<p>In previous chapters we have formalized this goal a bit more mathematically, but for a general recap, we stick to more casual language.</p>
<hr />
</div>
<div id="strategy" class="section level2">
<h2><span class="header-section-number">9.2</span> Strategy</h2>
<p>How do we find and train models that generalize well to unseen data? We generally follow these steps.</p>
<ol style="list-style-type: decimal">
<li><em>Split</em> the data in to <strong>training data</strong> and <strong>testing data</strong>.
<ul>
<li>Within the training data, we will do whatever we would.</li>
<li>The testing data will never be used to make any decision that lead to the selection of a model.</li>
<li>We often use 80% of the available data for training.</li>
</ul></li>
<li><em>Split</em> the training data into <strong>estimation data</strong> and <strong>validation data</strong>.
<ul>
<li>We often use 80% of the available data for estimation.</li>
</ul></li>
<li><em>Decide</em> on a set of <strong>candidate models</strong>.<br />
</li>
<li><em>Fit</em> (train) each candidate model to the <strong>estimation data</strong>.</li>
<li><em>Evaluate</em> all candidate models fit to the estimation data based on their performance on the <strong>validation data</strong>.
<ul>
<li>Performance here is based on the ability of the model to predict on the validation data which was <strong>not</strong> used to train the models.</li>
</ul></li>
<li><em>Select</em> one of the candidate models based on the <strong>validation performance</strong>.</li>
<li><em>Fit</em> the chosen model to the entire <strong>training dataset</strong>.</li>
<li><em>Estimate</em> model performance using the <strong>test data</strong>.</li>
</ol>
<p>Note that we are using the <strong>validation data</strong> to <em>select</em> a model, while the <strong>test data</strong> is used to <em>estimate model performance</em>.</p>
<hr />
</div>
<div id="models" class="section level2">
<h2><span class="header-section-number">9.3</span> Models</h2>
<p>While there are many, many models that can be used for regression, we have focused on three <strong>families</strong> of models. We saw how each can be used to estimate the regression function. Importantly, each model family can be made more or less flexible to accommodate different datasets in order to find a model that predicts well.</p>
<div id="linear-models" class="section level4">
<h4><span class="header-section-number">9.3.0.1</span> Linear Models</h4>
<p><strong>Linear models</strong> are a family of <strong>parametric</strong> models which assume that the regression function is a linear combination of the features. For example, with a single feature <span class="math inline">\(x\)</span>, we could assume</p>
<p><span class="math display">\[
\mu(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \ldots + \beta_9 x^9
\]</span></p>
<p>Here, the <span class="math inline">\(\beta\)</span> coefficients are <em>model parameters</em> that are learned from the data via least squares or maximum likelihood. (Additionally, we could assume a conditional normal distribution with a constant variance, which would require estimating the <span class="math inline">\(\sigma\)</span> parameters. This is not necessary to estimate the mean.)</p>
</div>
<div id="k-nearest-neighbors-1" class="section level4">
<h4><span class="header-section-number">9.3.0.2</span> k-Nearest Neighbors</h4>
<p><strong>k-Nearest Neighbors</strong> models are a family of <strong>nonparametric models</strong> with a single <em>tuning parameter</em>, <span class="math inline">\(k\)</span>, the number of neighbors to use when estimating the regression function. We can also control which features are used when measuring distances.</p>
</div>
<div id="decision-trees-1" class="section level4">
<h4><span class="header-section-number">9.3.0.3</span> Decision Trees</h4>
<p><strong>Decision Tree</strong> models are a family of <strong>nonparametric models</strong> with a number of <em>tuning parameters</em>, but most notably, <code>cp</code>, the “complexity parameter” which indirectly controls the number of splits used to create neighborhoods of observations. We can also control which features are used when considering splits.</p>
<hr />
</div>
</div>
<div id="model-flexibility" class="section level2">
<h2><span class="header-section-number">9.4</span> Model Flexibility</h2>
<p>While we may not have explicitly stated this relationship, the plot below shows how train and validation “error” change as a function of model flexibility.</p>
<p><img src="09-regression-overview_files/figure-html/unnamed-chunk-1-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The “error” in this plot could be any reasonable error metric used, for example, RMSE. While here we are specifically discussing estimation and validation error, we more generally are discussing an error metric calculated on the same data used to train the model (estimation) and an error metric calculated on data <strong>not</strong> used to train the model, for example the validation data.</p>
<p>The “line” and “curve” seen above are highly idealized, that is, you won’t see nice linear and quadratic trends in practice. However, you will see that <em>training error</em> <strong>decreases</strong> as <em>model flexibility</em> <strong>increases</strong>. (This is essentially one of the few use cases for actually calculating training error, to verify this relationship as a sanity check.) On the other hand, often we will see that <em>validation error</em> first <strong>decreases</strong>, then <strong>increases</strong> as <em>model flexibility</em> <strong>increases</strong>. (Sometimes you might see only an increase or decrease which would suggest you need to also try additional models with more or less flexibility.) While the validation “curve” is idealized as a “curve” that decrease then increases, in practice, it might be a bit more “wiggly” just due to the random nature of the validation split.</p>
<p>How can we modify the flexibility of the models we have considered?</p>
<div id="linear-models-1" class="section level4">
<h4><span class="header-section-number">9.4.0.1</span> Linear Models</h4>
<p>To increase the flexibility of linear models, add additional transformed features or simply add features.</p>
<p>For example a model that assumes a quadratic mean function</p>
<p><span class="math display">\[
\mu_1(x) = \beta_0 + \beta_1 x + \beta_2 x ^ 2
\]</span></p>
<p>is more flexible than a model that assumes a linear mean function</p>
<p><span class="math display">\[
\mu_2(x) = \beta_0 + \beta_1 x
\]</span></p>
<p>The model that assumes a quadratic mean function <strong>can</strong> learn a linear mean function, but this added flexibility comes with a price. (Possible overfitting.)</p>
<p>Similarly, a model that assumes the mean function is a function of two features</p>
<p><span class="math display">\[
\mu_1(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>is more flexible than a model that assumes the mean function is only a function of one of these features.</p>
<p><span class="math display">\[
\mu_2(x) = \beta_0 + \beta_1 x
\]</span></p>
</div>
<div id="k-nearest-neighbors-2" class="section level4">
<h4><span class="header-section-number">9.4.0.2</span> k-Nearest Neighbors</h4>
<p>Given a set of feature variables, as <span class="math inline">\(k\)</span> increases, model flexibility <strong>decreases.</strong> (Note that adding and removing features does have an effect on model flexibility, generally adding flexibility with additional features, there are situations where adding features will decrease training error.)</p>
</div>
<div id="decision" class="section level4">
<h4><span class="header-section-number">9.4.0.3</span> Decision</h4>
<p>Given a set of feature variables, as <code>cp</code> increases, model flexibility <strong>decreases.</strong> (Note that adding and removing features does have an effect on model flexibility, generally adding flexibility with additional features, there are situations where adding features will decrease training error.)</p>
<hr />
</div>
</div>
<div id="overfitting" class="section level2">
<h2><span class="header-section-number">9.5</span> Overfitting</h2>
<p>Overfitting occurs when we have fit to not just the <strong>signal</strong> but also the <strong>noise.</strong> That is, a model performs too well on the training data.</p>
<p>Let’s take a look at this visually.</p>
<p><img src="09-regression-overview_files/figure-html/unnamed-chunk-5-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>In each of the plots above, the dashed black curve represents the true mean function of interest, in this case,</p>
<p><span class="math display">\[
\mu(x) = \sin(x)
\]</span></p>
<p>with points simulated about this mean according to a standard normal. (That is, the noise is standard normal.)</p>
<p>We see that the model with <span class="math inline">\(k = 1\)</span> has fit far too well to the training data. The estimated mean function, seen in green, goes through each training point. That is, there is no training error. This model is <strong>too flexible</strong> and is <strong>overfitting</strong>. We have learned <strong>both</strong> the signal and the noise, thus this model will predict poorly on new data.</p>
<p>The model with <span class="math inline">\(k = 25\)</span> has fits the training data poorly. The estimated mean function, seen in red, does not match the true mean function well. The points are far from the estimated mean function. his model is <strong>too inflexible</strong> and is <strong>underfitting</strong>. It has learned neither the signal not the noise.</p>
<p>The model with <span class="math inline">\(k = 5\)</span> seems like a reasonable in-between. Doesn’t seem to be chasing noise. Seems to reasonably approximate the true mean function.</p>
<p>How do we assess over and underfitting in practice, when we don’t know the true mean function? We have to look at train and validation errors.</p>
<ul>
<li>Model that are probably <strong>underfitting</strong>: “Large” Train RMSE and a Validation RMSE larger than the smallest. The less flexible, the more probable the underfitting.</li>
<li>Model that are probably <strong>overfitting</strong>: “Small” Train RMSE and a Validation RMSE larger than the smallest. The more flexible, the more probable the overfitting.</li>
</ul>
<p><img src="09-regression-overview_files/figure-html/unnamed-chunk-6-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The further a model is to the <em>left</em> of this plot, the greater the chance it is <strong>underfit</strong>. The further a model is to the <em>right</em> of this plot, the greater the chance it is <strong>overfit</strong>.</p>
<hr />
</div>
<div id="bias-variance-tradeoff" class="section level2">
<h2><span class="header-section-number">9.6</span> Bias-Variance Tradeoff</h2>
<p><strong>Why</strong> does changing the model flexibility influence the predictive performance of these models? The bias-variance tradeoff.</p>
<p>As models <em>increase</em> in <strong>flexibility</strong>,</p>
<ul>
<li><strong>bias</strong> is <em>decreased</em></li>
<li><strong>variance</strong> is <em>increased</em></li>
</ul>
<p>And together, the MSE is equal to the bias squared plus the variance.</p>
<p>However, the rate at which the variance increases can be and generally is different than the rate at which the bias deceases. This is why we must validate our models. Essentially, by modifying the model complexity and validating the results, we are trying to find the right balance between bias and variance.</p>
<hr />
</div>
<div id="source-8" class="section level2">
<h2><span class="header-section-number">9.7</span> Source</h2>
<ul>
<li><code>R</code> Markdown: <a href="09-regression-overview.Rmd"><code>09-regression-overview.Rmd</code></a></li>
</ul>
<hr />

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="biasvariance-tradeoff.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-backlog.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/09-regression-overview.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
