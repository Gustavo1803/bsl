<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Probability | Basics of Statistical Learning</title>
  <meta name="description" content="Chapter 6 Probability | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.14.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Probability | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://daviddalpiaz.github.io/bsl/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Probability | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />


<meta name="date" content="2019-10-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-1.html"/>
<link rel="next" href="estimation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#caveat-emptor"><i class="fa fa-check"></i>Caveat Emptor</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#organization"><i class="fa fa-check"></i>Organization</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I A Machine Learning Preview</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="regression-powerlifting.html"><a href="regression-powerlifting.html"><i class="fa fa-check"></i><b>2</b> Regression: Powerlifting</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-powerlifting.html"><a href="regression-powerlifting.html#background"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="regression-powerlifting.html"><a href="regression-powerlifting.html#data"><i class="fa fa-check"></i><b>2.2</b> Data</a></li>
<li class="chapter" data-level="2.3" data-path="regression-powerlifting.html"><a href="regression-powerlifting.html#eda"><i class="fa fa-check"></i><b>2.3</b> EDA</a></li>
<li class="chapter" data-level="2.4" data-path="regression-powerlifting.html"><a href="regression-powerlifting.html#modeling"><i class="fa fa-check"></i><b>2.4</b> Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="regression-powerlifting.html"><a href="regression-powerlifting.html#model-evaluation"><i class="fa fa-check"></i><b>2.5</b> Model Evaluation</a></li>
<li class="chapter" data-level="2.6" data-path="regression-powerlifting.html"><a href="regression-powerlifting.html#discussion"><i class="fa fa-check"></i><b>2.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-handwriting.html"><a href="classification-handwriting.html"><i class="fa fa-check"></i><b>3</b> Classification: Handwriting</a><ul>
<li class="chapter" data-level="3.1" data-path="classification-handwriting.html"><a href="classification-handwriting.html#background-1"><i class="fa fa-check"></i><b>3.1</b> Background</a></li>
<li class="chapter" data-level="3.2" data-path="classification-handwriting.html"><a href="classification-handwriting.html#data-1"><i class="fa fa-check"></i><b>3.2</b> Data</a></li>
<li class="chapter" data-level="3.3" data-path="classification-handwriting.html"><a href="classification-handwriting.html#eda-1"><i class="fa fa-check"></i><b>3.3</b> EDA</a></li>
<li class="chapter" data-level="3.4" data-path="classification-handwriting.html"><a href="classification-handwriting.html#modeling-1"><i class="fa fa-check"></i><b>3.4</b> Modeling</a></li>
<li class="chapter" data-level="3.5" data-path="classification-handwriting.html"><a href="classification-handwriting.html#model-evaluation-1"><i class="fa fa-check"></i><b>3.5</b> Model Evaluation</a></li>
<li class="chapter" data-level="3.6" data-path="classification-handwriting.html"><a href="classification-handwriting.html#discussion-1"><i class="fa fa-check"></i><b>3.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="clustering-basketball-players.html"><a href="clustering-basketball-players.html"><i class="fa fa-check"></i><b>4</b> Clustering: Basketball Players</a><ul>
<li class="chapter" data-level="4.1" data-path="clustering-basketball-players.html"><a href="clustering-basketball-players.html#background-2"><i class="fa fa-check"></i><b>4.1</b> Background</a></li>
<li class="chapter" data-level="4.2" data-path="clustering-basketball-players.html"><a href="clustering-basketball-players.html#data-2"><i class="fa fa-check"></i><b>4.2</b> Data</a></li>
<li class="chapter" data-level="4.3" data-path="clustering-basketball-players.html"><a href="clustering-basketball-players.html#eda-2"><i class="fa fa-check"></i><b>4.3</b> EDA</a></li>
<li class="chapter" data-level="4.4" data-path="clustering-basketball-players.html"><a href="clustering-basketball-players.html#modeling-2"><i class="fa fa-check"></i><b>4.4</b> Modeling</a></li>
<li class="chapter" data-level="4.5" data-path="clustering-basketball-players.html"><a href="clustering-basketball-players.html#model-evaluation-2"><i class="fa fa-check"></i><b>4.5</b> Model Evaluation</a></li>
<li class="chapter" data-level="4.6" data-path="clustering-basketball-players.html"><a href="clustering-basketball-players.html#discussion-2"><i class="fa fa-check"></i><b>4.6</b> Discussion</a></li>
</ul></li>
<li class="part"><span><b>II Some Machine Learning Foundations</b></span></li>
<li class="chapter" data-level="5" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>5</b> Introduction</a></li>
<li class="chapter" data-level="6" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>6</b> Probability</a><ul>
<li class="chapter" data-level="6.1" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>6.1</b> Probability Models</a></li>
<li class="chapter" data-level="6.2" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>6.2</b> Probability Axioms</a></li>
<li class="chapter" data-level="6.3" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>6.3</b> Probability Rules</a></li>
<li class="chapter" data-level="6.4" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>6.4</b> Random Variables</a><ul>
<li class="chapter" data-level="6.4.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>6.4.1</b> Distributions</a></li>
<li class="chapter" data-level="6.4.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>6.4.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="6.4.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>6.4.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="6.4.4" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>6.4.4</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>6.5</b> Expectations</a></li>
<li class="chapter" data-level="6.6" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>6.6</b> Likelihood</a></li>
<li class="chapter" data-level="6.7" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>6.7</b> Videos</a></li>
<li class="chapter" data-level="6.8" data-path="probability.html"><a href="probability.html#references"><i class="fa fa-check"></i><b>6.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>7</b> Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="estimation.html"><a href="estimation.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability</a></li>
<li class="chapter" data-level="7.2" data-path="estimation.html"><a href="estimation.html#statistics"><i class="fa fa-check"></i><b>7.2</b> Statistics</a></li>
<li class="chapter" data-level="7.3" data-path="estimation.html"><a href="estimation.html#estimators"><i class="fa fa-check"></i><b>7.3</b> Estimators</a><ul>
<li class="chapter" data-level="7.3.1" data-path="estimation.html"><a href="estimation.html#properties"><i class="fa fa-check"></i><b>7.3.1</b> Properties</a></li>
<li class="chapter" data-level="7.3.2" data-path="estimation.html"><a href="estimation.html#methods"><i class="fa fa-check"></i><b>7.3.2</b> Methods</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III A Tour of Machine Learning</b></span></li>
<li class="chapter" data-level="8" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>8</b> Introduction</a></li>
<li class="chapter" data-level="9" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>9</b> Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression.html"><a href="regression.html#setup"><i class="fa fa-check"></i><b>9.1</b> Setup</a></li>
<li class="chapter" data-level="9.2" data-path="regression.html"><a href="regression.html#modeling-3"><i class="fa fa-check"></i><b>9.2</b> Modeling</a><ul>
<li class="chapter" data-level="9.2.1" data-path="regression.html"><a href="regression.html#linear-models"><i class="fa fa-check"></i><b>9.2.1</b> Linear Models</a></li>
<li class="chapter" data-level="9.2.2" data-path="regression.html"><a href="regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>9.2.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="9.2.3" data-path="regression.html"><a href="regression.html#decision-trees"><i class="fa fa-check"></i><b>9.2.3</b> Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regression.html"><a href="regression.html#procedure"><i class="fa fa-check"></i><b>9.3</b> Procedure</a></li>
<li class="chapter" data-level="9.4" data-path="regression.html"><a href="regression.html#data-splitting"><i class="fa fa-check"></i><b>9.4</b> Data Splitting</a></li>
<li class="chapter" data-level="9.5" data-path="regression.html"><a href="regression.html#metrics"><i class="fa fa-check"></i><b>9.5</b> Metrics</a></li>
<li class="chapter" data-level="9.6" data-path="regression.html"><a href="regression.html#model-complexity"><i class="fa fa-check"></i><b>9.6</b> Model Complexity</a></li>
<li class="chapter" data-level="9.7" data-path="regression.html"><a href="regression.html#overfitting"><i class="fa fa-check"></i><b>9.7</b> Overfitting</a></li>
<li class="chapter" data-level="9.8" data-path="regression.html"><a href="regression.html#multiple-features"><i class="fa fa-check"></i><b>9.8</b> Multiple Features</a></li>
<li class="chapter" data-level="9.9" data-path="regression.html"><a href="regression.html#example-analysis"><i class="fa fa-check"></i><b>9.9</b> Example Analysis</a></li>
<li class="chapter" data-level="9.10" data-path="regression.html"><a href="regression.html#misc-todos"><i class="fa fa-check"></i><b>9.10</b> MISC TODOS</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html"><i class="fa fa-check"></i><b>10</b> Bias–Variance Tradeoff</a><ul>
<li class="chapter" data-level="10.1" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>10.1</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="10.2" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>10.2</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="10.3" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#simulation"><i class="fa fa-check"></i><b>10.3</b> Simulation</a></li>
<li class="chapter" data-level="10.4" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>10.4</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="10.5" data-path="biasvariance-tradeoff.html"><a href="biasvariance-tradeoff.html#reproducibility"><i class="fa fa-check"></i><b>10.5</b> Reproducibility</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>11</b> Classification</a><ul>
<li class="chapter" data-level="11.1" data-path="classification.html"><a href="classification.html#stat-432-materials"><i class="fa fa-check"></i><b>11.1</b> STAT 432 Materials</a></li>
<li class="chapter" data-level="11.2" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>11.2</b> Bayes Classifier</a><ul>
<li class="chapter" data-level="11.2.1" data-path="classification.html"><a href="classification.html#bayes-error-rate"><i class="fa fa-check"></i><b>11.2.1</b> Bayes Error Rate</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="classification.html"><a href="classification.html#building-a-classifier"><i class="fa fa-check"></i><b>11.3</b> Building a Classifier</a></li>
<li class="chapter" data-level="11.4" data-path="classification.html"><a href="classification.html#modeling-4"><i class="fa fa-check"></i><b>11.4</b> Modeling</a><ul>
<li class="chapter" data-level="11.4.1" data-path="classification.html"><a href="classification.html#linear-models-1"><i class="fa fa-check"></i><b>11.4.1</b> Linear Models</a></li>
<li class="chapter" data-level="11.4.2" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>11.4.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="11.4.3" data-path="classification.html"><a href="classification.html#decision-trees-1"><i class="fa fa-check"></i><b>11.4.3</b> Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="classification.html"><a href="classification.html#misc-todo-stuff"><i class="fa fa-check"></i><b>11.5</b> MISC TODO STUFF</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>12</b> Resampling</a><ul>
<li class="chapter" data-level="12.1" data-path="resampling.html"><a href="resampling.html#stat-432-materials-1"><i class="fa fa-check"></i><b>12.1</b> STAT 432 Materials</a></li>
<li class="chapter" data-level="12.2" data-path="resampling.html"><a href="resampling.html#validation-set-approach"><i class="fa fa-check"></i><b>12.2</b> Validation-Set Approach</a></li>
<li class="chapter" data-level="12.3" data-path="resampling.html"><a href="resampling.html#cross-validation"><i class="fa fa-check"></i><b>12.3</b> Cross-Validation</a></li>
<li class="chapter" data-level="12.4" data-path="resampling.html"><a href="resampling.html#test-data"><i class="fa fa-check"></i><b>12.4</b> Test Data</a></li>
<li class="chapter" data-level="12.5" data-path="resampling.html"><a href="resampling.html#misc-todos-1"><i class="fa fa-check"></i><b>12.5</b> MISC TODOS</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>13</b> Supervised Learning</a></li>
<li class="chapter" data-level="14" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>14</b> Regularization</a><ul>
<li class="chapter" data-level="14.1" data-path="regularization.html"><a href="regularization.html#stat-432-materials-2"><i class="fa fa-check"></i><b>14.1</b> STAT 432 Materials</a></li>
<li class="chapter" data-level="14.2" data-path="regularization.html"><a href="regularization.html#adding-bias-to-reduce-variance"><i class="fa fa-check"></i><b>14.2</b> adding bias to reduce variance</a></li>
<li class="chapter" data-level="14.3" data-path="regularization.html"><a href="regularization.html#scaling-matters"><i class="fa fa-check"></i><b>14.3</b> scaling matters?</a></li>
<li class="chapter" data-level="14.4" data-path="regularization.html"><a href="regularization.html#moving-to-two-dimensions"><i class="fa fa-check"></i><b>14.4</b> moving to two dimensions</a></li>
<li class="chapter" data-level="14.5" data-path="regularization.html"><a href="regularization.html#boston-is-boring"><i class="fa fa-check"></i><b>14.5</b> boston is boring</a></li>
<li class="chapter" data-level="14.6" data-path="regularization.html"><a href="regularization.html#some-more-simulation"><i class="fa fa-check"></i><b>14.6</b> some more simulation</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ensembles.html"><a href="ensembles.html"><i class="fa fa-check"></i><b>15</b> Ensembles</a><ul>
<li class="chapter" data-level="15.1" data-path="ensembles.html"><a href="ensembles.html#stat-432-materials-3"><i class="fa fa-check"></i><b>15.1</b> STAT 432 Materials</a></li>
<li class="chapter" data-level="15.2" data-path="ensembles.html"><a href="ensembles.html#bagging"><i class="fa fa-check"></i><b>15.2</b> Bagging</a><ul>
<li class="chapter" data-level="15.2.1" data-path="ensembles.html"><a href="ensembles.html#simultation-study"><i class="fa fa-check"></i><b>15.2.1</b> Simultation Study</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ensembles.html"><a href="ensembles.html#random-forest"><i class="fa fa-check"></i><b>15.3</b> Random Forest</a></li>
<li class="chapter" data-level="15.4" data-path="ensembles.html"><a href="ensembles.html#boosting"><i class="fa fa-check"></i><b>15.4</b> Boosting</a></li>
</ul></li>
<li class="part"><span><b>IV Mathematics</b></span></li>
<li class="chapter" data-level="16" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>16</b> Introduction</a></li>
<li class="part"><span><b>V Computing</b></span></li>
<li class="chapter" data-level="17" data-path="introduction-4.html"><a href="introduction-4.html"><i class="fa fa-check"></i><b>17</b> Introduction</a></li>
<li class="chapter" data-level="18" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i><b>18</b> Resources</a><ul>
<li class="chapter" data-level="18.1" data-path="resources.html"><a href="resources.html#resources-1"><i class="fa fa-check"></i><b>18.1</b> Resources</a><ul>
<li class="chapter" data-level="18.1.1" data-path="resources.html"><a href="resources.html#r"><i class="fa fa-check"></i><b>18.1.1</b> R</a></li>
<li class="chapter" data-level="18.1.2" data-path="resources.html"><a href="resources.html#rstudio"><i class="fa fa-check"></i><b>18.1.2</b> RStudio</a></li>
<li class="chapter" data-level="18.1.3" data-path="resources.html"><a href="resources.html#r-markdown"><i class="fa fa-check"></i><b>18.1.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="resources.html"><a href="resources.html#bsl-idioms"><i class="fa fa-check"></i><b>18.2</b> BSL Idioms</a><ul>
<li class="chapter" data-level="18.2.1" data-path="resources.html"><a href="resources.html#reference-style"><i class="fa fa-check"></i><b>18.2.1</b> Reference Style</a></li>
<li class="chapter" data-level="18.2.2" data-path="resources.html"><a href="resources.html#bsl-style-overrides"><i class="fa fa-check"></i><b>18.2.2</b> BSL Style Overrides</a></li>
<li class="chapter" data-level="18.2.3" data-path="resources.html"><a href="resources.html#objects-and-functions"><i class="fa fa-check"></i><b>18.2.3</b> Objects and Functions</a></li>
<li class="chapter" data-level="18.2.4" data-path="resources.html"><a href="resources.html#print-versus-return"><i class="fa fa-check"></i><b>18.2.4</b> Print versus Return</a></li>
<li class="chapter" data-level="18.2.5" data-path="resources.html"><a href="resources.html#help"><i class="fa fa-check"></i><b>18.2.5</b> Help</a></li>
<li class="chapter" data-level="18.2.6" data-path="resources.html"><a href="resources.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>18.2.6</b> Keyboard Shortcuts</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="resources.html"><a href="resources.html#common-issues"><i class="fa fa-check"></i><b>18.3</b> Common Issues</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="simulation-and-bootstrap.html"><a href="simulation-and-bootstrap.html"><i class="fa fa-check"></i><b>19</b> Simulation and Bootstrap</a><ul>
<li class="chapter" data-level="19.1" data-path="simulation-and-bootstrap.html"><a href="simulation-and-bootstrap.html#stat-432-materials-4"><i class="fa fa-check"></i><b>19.1</b> STAT 432 Materials</a></li>
<li class="chapter" data-level="19.2" data-path="simulation-and-bootstrap.html"><a href="simulation-and-bootstrap.html#misc-notes"><i class="fa fa-check"></i><b>19.2</b> Misc Notes</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>20</b> Data Manipulation</a><ul>
<li class="chapter" data-level="20.1" data-path="data-manipulation.html"><a href="data-manipulation.html#dplyr"><i class="fa fa-check"></i><b>20.1</b> <code>dplyr</code></a></li>
<li class="chapter" data-level="20.2" data-path="data-manipulation.html"><a href="data-manipulation.html#data.table"><i class="fa fa-check"></i><b>20.2</b> <code>data.table</code></a></li>
<li class="chapter" data-level="20.3" data-path="data-manipulation.html"><a href="data-manipulation.html#data-splitting-with-dplyranti_join"><i class="fa fa-check"></i><b>20.3</b> Data Splitting with <code>dplyr::anti_join</code></a></li>
</ul></li>
<li class="part"><span><b>VI Analysis</b></span></li>
<li class="chapter" data-level="21" data-path="introduction-5.html"><a href="introduction-5.html"><i class="fa fa-check"></i><b>21</b> Introduction</a></li>
<li class="part"><span><b>VII Appendix</b></span></li>
<li class="chapter" data-level="22" data-path="introduction-6.html"><a href="introduction-6.html"><i class="fa fa-check"></i><b>22</b> Introduction</a></li>
<li class="chapter" data-level="23" data-path="additional-reading.html"><a href="additional-reading.html"><i class="fa fa-check"></i><b>23</b> Additional Reading</a><ul>
<li class="chapter" data-level="23.1" data-path="additional-reading.html"><a href="additional-reading.html#books"><i class="fa fa-check"></i><b>23.1</b> Books</a></li>
<li class="chapter" data-level="23.2" data-path="additional-reading.html"><a href="additional-reading.html#papers"><i class="fa fa-check"></i><b>23.2</b> Papers</a></li>
<li class="chapter" data-level="23.3" data-path="additional-reading.html"><a href="additional-reading.html#blog-posts"><i class="fa fa-check"></i><b>23.3</b> Blog Posts</a></li>
<li class="chapter" data-level="23.4" data-path="additional-reading.html"><a href="additional-reading.html#miscellaneous"><i class="fa fa-check"></i><b>23.4</b> Miscellaneous</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/daviddalpiaz/bsl" target="blank">&copy; 2019 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Probability</h1>
<ul>
<li>TODO: Note! This is copy-pasted from R4SL.</li>
</ul>
<p>We give a very brief review of some necessary probability concepts. As the treatment is less than complete, a list of references is given at the end of the chapter. For example, we ignore the usual recap of basic set theory and omit proofs and examples.</p>
<div id="probability-models" class="section level2">
<h2><span class="header-section-number">6.1</span> Probability Models</h2>
<p>When discussing probability models, we speak of random <strong>experiments</strong> that produce one of a number of possible <strong>outcomes</strong>.</p>
<p>A <strong>probability model</strong> that describes the uncertainty of an experiment consists of two elements:</p>
<ul>
<li>The <strong>sample space</strong>, often denoted as <span class="math inline">\(\Omega\)</span>, which is a set that contains all possible outcomes.</li>
<li>A <strong>probability function</strong> that assigns to an event <span class="math inline">\(A\)</span> a nonnegative number, <span class="math inline">\(P[A]\)</span>, that represents how likely it is that event <span class="math inline">\(A\)</span> occurs as a result of the experiment.</li>
</ul>
<p>We call <span class="math inline">\(P[A]\)</span> the <strong>probability</strong> of event <span class="math inline">\(A\)</span>. An <strong>event</strong> <span class="math inline">\(A\)</span> could be any subset of the sample space, not necessarily a single possible outcome. The probability law must follow a number of rules, which are the result of a set of axioms that we introduce now.</p>
</div>
<div id="probability-axioms" class="section level2">
<h2><span class="header-section-number">6.2</span> Probability Axioms</h2>
<p>Given a sample space <span class="math inline">\(\Omega\)</span> for a particular experiment, the <strong>probability function</strong> associated with the experiment must satisfy the following axioms.</p>
<ol style="list-style-type: decimal">
<li><em>Nonnegativity</em>: <span class="math inline">\(P[A] \geq 0\)</span> for any event <span class="math inline">\(A \subset \Omega\)</span>.</li>
<li><em>Normalization</em>: <span class="math inline">\(P[\Omega] = 1\)</span>. That is, the probability of the entire space is 1.</li>
<li><em>Additivity</em>: For mutually exclusive events <span class="math inline">\(E_1, E_2, \ldots\)</span>
<span class="math display">\[
P\left[\bigcup_{i = 1}^{\infty} E_i\right] = \sum_{i = 1}^{\infty} P[E_i]
\]</span></li>
</ol>
<p>Using these axioms, many additional probability rules can easily be derived.</p>
</div>
<div id="probability-rules" class="section level2">
<h2><span class="header-section-number">6.3</span> Probability Rules</h2>
<p>Given an event <span class="math inline">\(A\)</span>, and its complement, <span class="math inline">\(A^c\)</span>, that is, the outcomes in <span class="math inline">\(\Omega\)</span> which are not in <span class="math inline">\(A\)</span>, we have the <strong>complement rule</strong>:</p>
<p><span class="math display">\[
P[A^c] = 1 - P[A]
\]</span></p>
<p>In general, for two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we have the <strong>addition rule</strong>:</p>
<p><span class="math display">\[
P[A \cup B] = P[A] + P[B] - P[A \cap B]
\]</span></p>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are also <em>disjoint</em>, then we have:</p>
<p><span class="math display">\[
P[A \cup B] = P[A] + P[B]
\]</span></p>
<p>If we have <span class="math inline">\(n\)</span> mutually exclusive events, <span class="math inline">\(E_1, E_2, \ldots E_n\)</span>, then we have:</p>
<p><span class="math display">\[
P\left[\textstyle\bigcup_{i = 1}^{n} E_i\right] = \sum_{i = 1}^{n} P[E_i]
\]</span></p>
<p>Often, we would like to understand the probability of an event <span class="math inline">\(A\)</span>, given some information about the outcome of event <span class="math inline">\(B\)</span>. In that case, we have the <strong>conditional probability rule</strong> provided <span class="math inline">\(P[B] &gt; 0\)</span>.</p>
<p><span class="math display">\[
P[A \mid B] = \frac{P[A \cap B]}{P[B]}
\]</span></p>
<p>Rearranging the conditional probability rule, we obtain the <strong>multiplication rule</strong>:</p>
<p><span class="math display">\[
P[A \cap B] = P[B] \cdot P[A \mid B] \cdot 
\]</span></p>
<p>For a number of events <span class="math inline">\(E_1, E_2, \ldots E_n\)</span>, the multiplication rule can be expanded into the <strong>chain rule</strong>:</p>
<p><span class="math display">\[
P\left[\textstyle\bigcap_{i = 1}^{n} E_i\right] = P[E_1] \cdot P[E_2 \mid E_1] \cdot P[E_3 \mid E_1 \cap E_2] \cdots P\left[E_n \mid \textstyle\bigcap_{i = 1}^{n - 1} E_i\right] 
\]</span></p>
<p>Define a <strong>partition</strong> of a sample space <span class="math inline">\(\Omega\)</span> to be a set of disjoint events <span class="math inline">\(A_1, A_2, \ldots, A_n\)</span> whose union is the sample space <span class="math inline">\(\Omega\)</span>. That is</p>
<p><span class="math display">\[
A_i \cap A_j = \emptyset
\]</span></p>
<p>for all <span class="math inline">\(i \neq j\)</span>, and</p>
<p><span class="math display">\[
\bigcup_{i = 1}^{n} A_i = \Omega.
\]</span></p>
<p>Now, let <span class="math inline">\(A_1, A_2, \ldots, A_n\)</span> form a partition of the sample space where <span class="math inline">\(P[A_i] &gt; 0\)</span> for all <span class="math inline">\(i\)</span>. Then for any event <span class="math inline">\(B\)</span> with <span class="math inline">\(P[B] &gt; 0\)</span> we have <strong>Bayes’ Rule</strong>:</p>
<p><span class="math display">\[
P[A_i | B] = \frac{P[A_i]P[B | A_i]}{P[B]} = \frac{P[A_i]P[B | A_i]}{\sum_{i = 1}^{n}P[A_i]P[B | A_i]}
\]</span></p>
<p>The denominator of the latter equality is often called the <strong>law of total probability</strong>:</p>
<p><span class="math display">\[
P[B] = \sum_{i = 1}^{n}P[A_i]P[B | A_i]
\]</span></p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>independent</strong> if they satisfy</p>
<p><span class="math display">\[
P[A \cap B] = P[A] \cdot P[B]
\]</span></p>
<p>This becomes the new multiplication rule for independent events.</p>
<p>A collection of events <span class="math inline">\(E_1, E_2, \ldots E_n\)</span> is said to be independent if</p>
<p><span class="math display">\[
P\left[\bigcap_{i \in S} E_i \right] = \prod_{i \in S}P[E_i]
\]</span></p>
<p>for every subset <span class="math inline">\(S\)</span> of <span class="math inline">\(\{1, 2, \ldots n\}\)</span>.</p>
<p>If this is the case, then the chain rule is greatly simplified to:</p>
<p><span class="math display">\[
P\left[\textstyle\bigcap_{i = 1}^{n} E_i\right] = \prod_{i=1}^{n}P[E_i]
\]</span></p>
</div>
<div id="random-variables" class="section level2">
<h2><span class="header-section-number">6.4</span> Random Variables</h2>
<p>A <strong>random variable</strong> is simply a <em>function</em> which maps outcomes in the sample space to real numbers.</p>
<div id="distributions" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Distributions</h3>
<p>We often talk about the <strong>distribution</strong> of a random variable, which can be thought of as:</p>
<p><span class="math display">\[
\text{distribution} = \text{list of possible} \textbf{ values} + \text{associated} \textbf{ probabilities}
\]</span></p>
<p>This is not a strict mathematical definition, but is useful for conveying the idea.</p>
<p>If the possible values of a random variables are <em>discrete</em>, it is called a <em>discrete random variable</em>. If the possible values of a random variables are <em>continuous</em>, it is called a <em>continuous random variable</em>.</p>
</div>
<div id="discrete-random-variables" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Discrete Random Variables</h3>
<p>The distribution of a discrete random variable <span class="math inline">\(X\)</span> is most often specified by a list of possible values and a probability <strong>mass</strong> function, <span class="math inline">\(p(x)\)</span>. The mass function directly gives probabilities, that is,</p>
<p><span class="math display">\[
p(x) = p_X(x) = P[X = x].
\]</span></p>
<p>Note we almost always drop the subscript from the more correct <span class="math inline">\(p_X(x)\)</span> and simply refer to <span class="math inline">\(p(x)\)</span>. The relevant random variable is discerned from context</p>
<p>The most common example of a discrete random variable is a <strong>binomial</strong> random variable. The mass function of a binomial random variable <span class="math inline">\(X\)</span>, is given by</p>
<p><span class="math display">\[
p(x | n, p) = {n \choose x} p^x(1 - p)^{n - x}, \ \ \ x = 0, 1, \ldots, n, \ n \in \mathbb{N}, \ 0 &lt; p &lt; 1.
\]</span></p>
<p>This line conveys a large amount of information.</p>
<ul>
<li>The function <span class="math inline">\(p(x | n, p)\)</span> is the mass function. It is a function of <span class="math inline">\(x\)</span>, the possible values of the random variable <span class="math inline">\(X\)</span>. It is conditional on the <strong>parameters</strong> <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. Different values of these parameters specify different binomial distributions.</li>
<li><span class="math inline">\(x = 0, 1, \ldots, n\)</span> indicates the <strong>sample space</strong>, that is, the possible values of the random variable.</li>
<li><span class="math inline">\(n \in \mathbb{N}\)</span> and <span class="math inline">\(0 &lt; p &lt; 1\)</span> specify the <strong>parameter spaces</strong>. These are the possible values of the parameters that give a valid binomial distribution.</li>
</ul>
<p>Often all of this information is simply encoded by writing</p>
<p><span class="math display">\[
X \sim \text{bin}(n, p).
\]</span></p>
</div>
<div id="continuous-random-variables" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Continuous Random Variables</h3>
<p>The distribution of a continuous random variable <span class="math inline">\(X\)</span> is most often specified by a set of possible values and a probability <strong>density</strong> function, <span class="math inline">\(f(x)\)</span>. (A cumulative density or moment generating function would also suffice.)</p>
<p>The probability of the event <span class="math inline">\(a &lt; X &lt; b\)</span> is calculated as</p>
<p><span class="math display">\[
P[a &lt; X &lt; b] = \int_{a}^{b} f(x)dx.
\]</span></p>
<p>Note that densities are <strong>not</strong> probabilities.</p>
<p>The most common example of a continuous random variable is a <strong>normal</strong> random variable. The density of a normal random variable <span class="math inline">\(X\)</span>, is given by</p>
<p><span class="math display">\[
f(x | \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left[\frac{-1}{2} \left(\frac{x - \mu}{\sigma}\right)^2 \right],  \ \ \ -\infty &lt; x &lt; \infty, \ -\infty &lt; \mu &lt; \infty, \ \sigma &gt; 0.
\]</span></p>
<ul>
<li>The function <span class="math inline">\(f(x | \mu, \sigma^2)\)</span> is the density function. It is a function of <span class="math inline">\(x\)</span>, the possible values of the random variable <span class="math inline">\(X\)</span>. It is conditional on the <strong>paramters</strong> <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. Different values of these parameters specify different normal distributions.</li>
<li><span class="math inline">\(-\infty &lt; x &lt; \infty\)</span> indicates the sample space. In this case, the random variable may take any value on the real line.</li>
<li><span class="math inline">\(-\infty &lt; \mu &lt; \infty\)</span> and <span class="math inline">\(\sigma &gt; 0\)</span> specify the parameter space. These are the possible values of the parameters that give a valid normal distribution.</li>
</ul>
<p>Often all of this information is simply encoded by writing</p>
<p><span class="math display">\[
X \sim N(\mu, \sigma^2)
\]</span></p>
</div>
<div id="several-random-variables" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Several Random Variables</h3>
<p>Consider two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We say they are independent if</p>
<p><span class="math display">\[
f(x, y) = f(x) \cdot f(y)
\]</span></p>
<p>for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Here <span class="math inline">\(f(x, y)\)</span> is the <strong>joint</strong> density (mass) function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We call <span class="math inline">\(f(x)\)</span> the <strong>marginal</strong> density (mass) function of <span class="math inline">\(X\)</span>. Then <span class="math inline">\(f(y)\)</span> the marginal density (mass) function of <span class="math inline">\(Y\)</span>. The joint density (mass) function <span class="math inline">\(f(x, y)\)</span> together with the possible <span class="math inline">\((x, y)\)</span> values specify the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Similar notions exist for more than two variables.</p>
</div>
</div>
<div id="expectations" class="section level2">
<h2><span class="header-section-number">6.5</span> Expectations</h2>
<p>For discrete random variables, we define the <strong>expectation</strong> of the function of a random variable <span class="math inline">\(X\)</span> as follows.</p>
<p><span class="math display">\[
\mathbb{E}[g(X)] \triangleq \sum_{x} g(x)p(x)
\]</span></p>
<p>For continuous random variables we have a similar definition.</p>
<p><span class="math display">\[
\mathbb{E}[g(X)] \triangleq \int g(x)f(x) dx
\]</span></p>
<p>For specific functions <span class="math inline">\(g\)</span>, expectations are given names.</p>
<p>The <strong>mean</strong> of a random variable <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
\mu_{X} = \text{mean}[X] \triangleq \mathbb{E}[X].
\]</span></p>
<p>So for a discrete random variable, we would have</p>
<p><span class="math display">\[
\text{mean}[X] = \sum_{x} x \cdot p(x)
\]</span></p>
<p>For a continuous random variable we would simply replace the sum by an integral.</p>
<p>The <strong>variance</strong> of a random variable <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
\sigma^2_{X} = \text{var}[X] \triangleq \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
\]</span></p>
<p>The <strong>standard deviation</strong> of a random variable <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
\sigma_{X} = \text{sd}[X] \triangleq \sqrt{\sigma^2_{X}} = \sqrt{\text{var}[X]}.
\]</span></p>
<p>The <strong>covariance</strong> of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[
\text{cov}[X, Y] \triangleq \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X] \cdot \mathbb{E}[Y].
\]</span></p>
</div>
<div id="likelihood" class="section level2">
<h2><span class="header-section-number">6.6</span> Likelihood</h2>
<p>Consider <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(X_1, X_2, \ldots X_n\)</span>. We can then write their <strong>likelihood</strong> as</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid x_1, x_2, \ldots x_n) = \prod_{i = i}^n f(x_i; \theta)
\]</span></p>
<p>where <span class="math inline">\(f(x_i; \theta)\)</span> is the density (or mass) function of random variable <span class="math inline">\(X_i\)</span> evaluated at <span class="math inline">\(x_i\)</span> with parameter <span class="math inline">\(\theta\)</span>.</p>
<p>Whereas a probability is a function of a possible observed value given a particular parameter value, a likelihood is the opposite. It is a function of a possible parameter value given observed data.</p>
<p>Maximumizing likelihood is a common techinque for fitting a model to data.</p>
</div>
<div id="videos" class="section level2">
<h2><span class="header-section-number">6.7</span> Videos</h2>
<p>The YouTube channel <a href="https://www.youtube.com/user/mathematicalmonk">mathematicalmonk</a> has a great <a href="https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4">Probability Primer playlist</a> containing lectures on many fundamental probability concepts. Some of the more important concepts are covered in the following videos:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=5BWk5qe5EJ8&amp;index=11&amp;list=PL17567A1A3F5DB5E4">Conditional Probability</a></li>
<li><a href="https://www.youtube.com/watch?v=KK9jvGl9FY0&amp;index=12&amp;list=PL17567A1A3F5DB5E4">Independence</a></li>
<li><a href="https://www.youtube.com/watch?v=RMS-WglZP-c&amp;index=13&amp;list=PL17567A1A3F5DB5E4">More Independence</a></li>
<li><a href="https://www.youtube.com/watch?v=cM1BqBv11U8&amp;index=14&amp;list=PL17567A1A3F5DB5E4">Bayes Rule</a></li>
</ul>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">6.8</span> References</h2>
<p>Any of the following are either dedicated to, or contain a good coverage of the details of the topics above.</p>
<ul>
<li>Probability Texts
<ul>
<li><a href="http://athenasc.com/probbook.html">Introduction to Probability</a> by Dimitri P. Bertsekas and John N. Tsitsiklis</li>
<li><a href="https://www.pearsonhighered.com/program/Ross-First-Course-in-Probability-A-9th-Edition/PGM110742.html">A First Course in Probability</a> by Sheldon Ross</li>
</ul></li>
<li>Machine Learning Texts with Probability Focus
<ul>
<li><a href="http://www.springer.com/us/book/9781441996336">Probability for Statistics and Machine Learning</a> by Anirban DasGupta</li>
<li><a href="https://mitpress.mit.edu/books/machine-learning-0">Machine Learning: A Probabilistic Perspective</a> by Kevin P. Murphy</li>
</ul></li>
<li>Statistics Texts with Introduction to Probability
<ul>
<li><a href="https://www.pearsonhighered.com/program/Hogg-Probability-and-Statistical-Inference-9th-Edition/PGM91556.html">Probability and Statistical Inference</a> by Robert V. Hogg, Elliot Tanis, and Dale Zimmerman</li>
<li><a href="https://www.pearsonhighered.com/program/Hogg-Introduction-to-Mathematical-Statistics-7th-Edition/PGM49624.html">Introduction to Mathematical Statistics</a> by Robert V. Hogg, Joseph McKean, and Allen T. Craig</li>
</ul></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="estimation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/02-02-probability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bsl.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
